---
title: [[QUE]] - (How) do fields of innovation vary in their legibility of quality?
url: https://roamresearch.com/#/app/megacoglab/page/HDOo5p1t3
author: Joel Chan
date: Fri Sep 10 2021 21:40:23 GMT-0400 (Eastern Daylight Time)
---

- #Question

    - # Synthesis

        - After looking at both [[@coleHierarchySciences1983]] and [[@fanelliBibliometricEvidenceHierarchy2013]] in some detail, I think the following claim is reasonably likely: there is some systematic variation between disciplines (mostly physical sciences, biological sciences, and social sciences, in that order), but the difference is small.

        - It was noisy in Cole's time, with limited methodological ability to do large-scale bibliometric analyses. Lots of null effects, but this was with small datasets; strongest effect was looking at ["core" of knowledge]([[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]), with textbook content as a proxy (we see, roughly, variations in [number]([[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]) and [age]([[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]) of references cited in physics/chemistry vs. sociology textbooks of that era), but this was a pilot analysis.

        - After bibliometrics became a thing, in Fanelli's era, we start to see more systematic evidence. Fanelli reviews some, and offers [some of their own]([[[[EVD]] - statistically significant but small differences in bibliometric measures of consensus across papers in physical, hard and soft biological, and social sciences  - [[@fanelliBibliometricEvidenceHierarchy2013]]]]). But in general the differences do not seem to be super large; the fact that they were noisier in Cole's era, and statistically reliable but small in absolute effect size in Fanelli's era, makes me think the effect sizes are small, at least at the frontier, as Cole argued.

        - Throughout this thread, there is a consistent hypothesized hierarchy of domains, from physical sciences to biological sciences to social sciences. Fanelli thinks that math and humanities are weird beasts because the fundamental dynamic has to do with physical constraints on what is studied and how it can be studied. Not obvious that this applies for math and humanities, although the patterns sort of line up ok, with math above physics, and humanities below social sciences.

        - There is more still to read, with some evidence around consensus around the meaning of terms (see [[@hjorlandEpistemologySociocognitivePerspective2002]] on [[paradigmatic relevance]] and [[@mcmahanAmbiguityEngagement2018]] on [[ambiguity]] in language). There's also a fun study on the degree to which scientists consult their peers before submitting, as a proxy for a priori confidence in the quality of work [[@sulsSocialComparisonSocial1983]]. Lots more in references of [[@coleHierarchySciences1983]] and [[@fanelliBibliometricEvidenceHierarchy2013]]. Important note: I haven't directly processed these references yet, except for [[@mcmahanAmbiguityEngagement2018]].

        - I think it would be reasonable to use the [orderings](((CaQWGh1pG))) in [[@fanelliBibliometricEvidenceHierarchy2013]], which are broadly consistent across studies of "hierarchy of sciences", as a first pass.

        - ## Themes and claims

            - [[[[CLM]] - Scientific fields vary substantially in terms of ease of consensus on judging what constitutes a valuable new contribution]]

                - [[@coleHierarchySciences1983]] basically clarifies and sharpens this claim because they don't find any evidence for this, and instead have reviewed a bunch of papers and conclude the [opposite]([[Opposed By]]) in terms of the research frontier:

                    - [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]

                - But [[@coleHierarchySciences1983]] also find some limited/scoped evidence in [support]([[SupportedBy]]):

                    - [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]

                    - [[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]

                - #SupportedBy

                    - [[[[EVD]] - statistically significant but small differences in bibliometric measures of consensus across papers in physical, hard and soft biological, and social sciences  - [[@fanelliBibliometricEvidenceHierarchy2013]]]]

            - [[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]

                - This is a more qualified version of the [general positive claim]([[[[CLM]] - Scientific fields vary substantially in terms of ease of consensus on judging what constitutes a valuable new contribution]])

                - And this is [[SupportedBy]]

                    - [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]

                    - [[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]

    - ---

    - # #annotatedbib

        - ## #references

            - [[@simontonVarietiesScientificCreativity2009]] (secondary source, can dig into the primary sources)

                - Simonton 2002 and Simonton 2004

                    - Simonton, D.K. (2004). Psychology’s status as a scientific discipline: Its empirical placement within an implicit hierarchy of the sciences. Review of General Psychology, 8, 59–67

                - {{[[DONE]]}} Cole 1983 - Citation concentration to specific articles and peer evaluation consensu

                    - [[@coleHierarchySciences1983]]

                - Suls and Fletcher 1983 - peer consultation rate [[@sulsSocialComparisonSocial1983]]

                    - Suls, J., & Fletcher, B. (1983). Social comparison in the social and physical sciences: An archival study. Journal of Personality and Social Psychology, 44, 575–580.

            - {{[[DONE]]}} [[@fanelliBibliometricEvidenceHierarchy2013]]

            - [[@kuhnStructureScientificRevolutions1996]] paradigmatic science

            - [[@hjorlandEpistemologySociocognitivePerspective2002]] [[paradigmatic relevance]]

            - does degree of [[Scatter]] matter?

        - ## #[[Claims and Evidence]]

            - [[@coleHierarchySciences1983]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] |  review of empirical studies that test whether fields assumed to be in hierarchy vary in terms of cognitive consensus, by various measures; failed to find differences at the frontier (e.g., grant reviews), but found some potentially strong differences in the core (proxy of textbooks)

        - ## #Integrated

        - ## #NotRelevant

    - ---

    - # Meta

        - Tags:

###### References

[[September 10th, 2021]]

- [[@coleHierarchySciences1983]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] |  review of empirical studies that test whether fields assumed to be in hierarchy vary in terms of cognitive consensus, by various measures; failed to find differences at the frontier (e.g., grant reviews), but found some potentially strong differences in the core (proxy of textbooks)

    - intellectual lineage

        - [[hierarchy of the sciences]] idea from [[Auguste Comte]] in history of sciences

        - [[Thomas Kuhn]] paradigms - some are "preparadigmatic", so less consensus about shared theoretical structures and methodological approaches

        - availability of shared and mature theory is a proxy for legibility, probably, because it can provide a shared framework against which to judge contributions

        - here, these authors talk about "cognitive consensus"

        - distinction between "core and "frontier"

            - maybe there is a sense of the "size" and stability of the core? by proxy, level of agreement (over time) across syllabi?

                - cc [[[[QUE]] - How can we measure quality legibility of a field?]]

            - seems related, but not identical to the idea of [[Core and Scatter]]

        - summary of different axes: development of theory, quantification, cognitive consensus, predictability, obsolescence, rate of growth

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1BbLOXLK8j.png?alt=media&token=af5d5ab3-926e-4d81-be77-3c55e3cca183) (p. 113)

    - other thoughts

    - second half follows distinctino between core and frontier, and is an attempt to explain this null finding; hypothesize that the data so far was at the frontier; what about the core? this is the primary study here

        - # methods notes

            - sample of 8 undergraduate textbooks in chemistry, physics, and sociology

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FG0CAw_-Dzs.png?alt=media&token=c7b2090d-ee08-466d-9bdf-a1b9ff76e20f) (p. 133)

            - measure: age distribution of references

            - measure: total number of references

        - # results

            - [[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]

            - [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]

                - take this and [previous result]([[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]) with a big grain of salt! no justification for textbook selection, and it's basically ~2-3 texts per discipline; they look like intro texts

        - conclusion from this is: [[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]

    - core of first half of paper is review of a bunch of prior studies, which basically provide no/negative evidence that there's systematic variation in consensus across fields that are thought to be at different levels of the [[hierarchy of the sciences]]: [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]

        - see Zuckerman and Merton idea of codification

            - they come right out and say that legibility/consensus is a potential indicator of degree of codification

            - lower rejection rates as a proxy. huh. that's... not clean.

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FiDdkKglTtx.png?alt=media&token=d80e3283-0072-4b6b-81ad-f7653b23c9df)

                - cc [[[[QUE]] - How can we measure quality legibility of a field?]]

        - rubin 1975 - chemists blame themselves, sociologists dispute validity of criteria - proxy for paradigm development and consensus?

        - hargens 1975 idea of normative integration (maps to cognitive consensus here) - political science < chemistry and math

        - cole 1979 -

            - no significant differences in relationship between citations and full professor promotion decisions

            - no significant differences in high citation performers x age (no more young high performers in "high consensus" fields)

                - this makes me think of [[@jonesAgeGreatInvention2010]] - IIRC they had different results? or maybe it was just increasing over time, but no big differences across fields? or maybe it was one of [[Dean Simonton]]'s papers?

            - note, though: i think the hierarchy was taken as given, not measured directly.

        - cole, cole and dietrich 1978 -

            - ask to eval peers and "greatest scientists" - small and barely statistically significant differences in expected pattern

                - this makes me think if there's any published data on IRR for peer review; taht could be proxy for legibility - maybe average time to publication? publication history? rate of "sleeping beauties"?

            - another proxy that i hate: distribution of citations (more concentration = higher consensus). i gues, like it or not, if there is high concentration, then there is high consensus in a sense. even if htat consensus is off base. here they used [[Gini coefficient]]

                - both cc [[[[QUE]] - How can we measure quality legibility of a field?]]

        - pilot study in this paper: does citation obsolescence vary across english lit vs. science?

            - control for growth rate of field (number of pubs), which Zuckerman and Merton and Price do not

        - cole and cole 1980; cole and cole 1981; cole, rubin and cole 1978 - no evidence of systematic variation across fields in terms of IRR (roughly) for grant peer review

        - small 1975 - weak correlation between reviewer IRR and eventual citation rate

        - carter 1974 - NIH peer review, week correlation between priority scores and later citations of work (r = .40; hmmm actually not that weak; real signal, seems like, but far from perfectly predictive)

        - cole and cole 1973 -

            - similar to [[@morganPrestigeDrivesEpistemic2018]] - if equal quant / citations, person from higher prestige institution rated more highly than person from lower prestige institution

                - cc [[[[CLM]] - Prestige substantially controls how scientific ideas spread]]

        - these are the ones to dig into that have the actual evidence #[[➰ breadcrumbs]]

    - ratio of core to frontier is proxy for "cognitive consensus" hierarchy?
[[September 10th, 2021]]

- Create [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - Seems like [[hierarchy of the sciences]] is a concept, based on google scholar hits
[[Week of October 4th, 2021]]

- [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - [[[[EVD]] - statistically significant but small differences in bibliometric measures of consensus across papers in physical, hard and soft biological, and social sciences  - [[@fanelliBibliometricEvidenceHierarchy2013]]]]

    - [[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]
[[NowReading]]

- [[@zuckermanAgeAgingAge1972]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - #inspectionalread

        - key concepts:

            - codification (this is what was referenced in [[@coleHierarchySciences1983]])

            - gerontocracy (presumably this refers to the dominance of science by the older)

            - obsolesence

                - related to "half-life of facts" [[@arbesmanHalfLifeFactsWhy2012]]?

        - section 2 on age stratification and the codification of scientific knowledge is the most relevant, so i'll focus on that

    - #lit-context

        - coming from [[Sociology]]

    - core focus of chapter is on age-related dynamics in science - science as a "young man's game". cross-check with:

        - [[@simontonVarietiesScientificCreativity2009]] for age-related dynamics (there might be a more precise cite)

        - [[@jonesAgeGreatInvention2010]] seems to have contradictory results: that the balance of power has shifted from the young to the old in comparison to Zuckerman's time

        - connection to age structure suggests a hypothesis too for a convergent/alt measure of [[Legibility]]: age structure (where, all things being equal, fields where great achievements occur later have lower rates of legibility; takes longer to get to the frontier)

    - generally what they seem to be arguing

    - core definitions of [[codification]] on pp. 303-305

        - connects to [[obsolescence]]

        - family resemblance

        - connects to degree of [[degree of empiricism, after Conant]] (more codified = less empiricist)
[[September 22nd, 2021]]

- possible leads from [[Misha Teplitskiy]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - https://link.springer.com/article/10.1007%2FBF02130467

    - this might be useful for literature it mentions: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0014331

    - https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0140525X00065675

    - Lamont 2009 book How Professors Think, I think also addresses this

    - https://www.jstor.org/stable/2095739?seq=1#metadata_info_tab_contents
[[September 11th, 2021]]

- Thinking again about [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] while reading [[@lakatosMethod1999]]

    - This idea of whether it's possible to cleanly predict/judge the quality of an idea without a long time horizon of seeing it in action, seems very closely related to [[Dean Simonton]]'s idea of blindness

    - [[Imre Lakatos]]'s idea of research programmes as the unit of analysis - being able to somehow judge in a stable way (after a certain point) whether a programme is progressive or degenerative

        - and the general [[demarcation problem]] is a version of our legibility question: how to judge science from pseudoscience (i.e., looks like science, but isn't as good as "the real thing") is sort of asking how to distinguish "good" theories (science) from "worthless" theories (pseudoscience)

            - i am very skeptical that there is a general solution to this (see, for example [this proof by van Rooij]([[[[EVD]] - assuming that there is a polynomial-time procedure for choosing solutions for an intractable function implies that P=NP - [[@vanrooijIntractabilityUseHeuristics2012]]]]), and [this claim]([[[[CLM]] - intractable problems cannot be solved by a fixed number of parallel procedures - [[@vanrooijIntractabilityUseHeuristics2012]]]])), but curious about local solutions to this, per [[meta-rationality]]: are there relatively stable heuristics, and are there variations in stability/maturity across settings?

                - even if there aren't "rational" and useful general solutions to the demarcation problem

    - sleeping beauties - do they vary by field? if so, that might be a useful indicator
[[September 17th, 2021]]

- Ok so here is where I'm at in terms of the [question of legibility]([[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]).

    - After bibliometrics became a thing, in Fanelli's era, we start to see more systematic evidence. Fanelli reviews some, and offers [some of their own]([[[[EVD]] - statistically significant but small differences in bibliometric measures of consensus across papers in physical, hard and soft biological, and social sciences  - [[@fanelliBibliometricEvidenceHierarchy2013]]]]). But in general the differences do not seem to be super large; the fact that they were noisier in Cole's era, and statistically reliable but small in absolute effect size in Fanelli's era, makes me think the effect sizes are small, at least at the frontier, as Cole argued.

    - After looking at both [[@coleHierarchySciences1983]] and [[@fanelliBibliometricEvidenceHierarchy2013]] in some detail, I think the following claim is reasonably likely: there is some systematic variation between disciplines (mostly physical sciences, biological sciences, and social sciences, in that order), but the difference is small.

    - It was noisy in Cole's time, with limited methodological ability to do large-scale bibliometric analyses. Lots of null effects, but this was with small datasets; strongest effect was looking at ["core" of knowledge]([[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]), with textbook content as a proxy (we see, roughly, variations in [number]([[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]) and [age]([[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]) of references cited in physics/chemistry vs. sociology textbooks of that era), but this was a pilot analysis.

    - Throughout this thread, there is a consistent hypothesized hierarchy of domains, from physical sciences to biological sciences to social sciences. Fanelli thinks that math and humanities are weird beasts because the fundamental dynamic has to do with physical constraints on what is studied and how it can be studied. Not obvious that this applies for math and humanities, although the patterns sort of line up ok, with math above physics, and humanities below social sciences.

    - I think it would be reasonable to use the [orderings](![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjWysvSGtjC.png?alt=media&token=167979b3-c1bc-46ac-9b1a-bea2418ff71f) (p. 2)) in [[@fanelliBibliometricEvidenceHierarchy2013]], which are broadly consistent across studies of "hierarchy of sciences", as a first pass.

    - There is more still to read, with some evidence around consensus around the meaning of terms (see [[@hjorlandEpistemologySociocognitivePerspective2002]] on [[paradigmatic relevance]] and [[@mcmahanAmbiguityEngagement2018]] on [[ambiguity]] in language). There's also a fun study on the degree to which scientists consult their peers before submitting, as a proxy for a priori confidence in the quality of work [[@sulsSocialComparisonSocial1983]]. Lots more in references of [[@coleHierarchySciences1983]] and [[@fanelliBibliometricEvidenceHierarchy2013]]. Important note: I haven't directly processed these references yet, except for [[@mcmahanAmbiguityEngagement2018]].
[[September 15th, 2021]]

- #[[➰ breadcrumbs]] Thread on [[hierarchy of the sciences]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] and [[[[QUE]] - How can we measure quality legibility of a field?]]

    - Cole: The hierarchy of the sciences? - Google Scholar: https://scholar.google.com/scholar?start=10&hl=en&as_sdt=20000005&sciodt=0,21&as_ylo=2017&cites=12177003916617353377&scipsc=

    - Vocabulary sharing among subjects belonging to the hierarchy of sciences | SpringerLink: https://link.springer.com/article/10.1007/s11192-020-03671-7

    - Co-existing Notions of Research Quality: A Framework to Study Context-specific Understandings of Good Research | SpringerLink: https://link.springer.com/article/10.1007/s11024-019-09385-2

    - Psychology as a Science within Comte's Hypothesized Hierarchy: Empirical Investigations and Conceptual Implications - Dean Keith Simonton, 2015: https://journals.sagepub.com/doi/10.1037/gpr0000039

    - Bibliometric Evidence for a Hierarchy of the Sciences - [[[scite report]]] for [[@fanelliBibliometricEvidenceHierarchy2013]]: https://scite.ai/reports/bibliometric-evidence-for-a-hierarchy-xAYp43?contradicting=false&mentioning=false&page=1&unclassified=false&utm_campaign=plugin&utm_medium=plugin&utm_source=generic

    - Three+one bonus Google Scholar improvements that I wish Google would do. | Aaron Tay's Musings about librarianship: http://musingsaboutlibrarianship.blogspot.com/2021/09/threeone-bonus-google-scholar.html

    - The Temporal Structure of Scientific Consensus Formation - [[[scite report]]]: https://scite.ai/reports/the-temporal-structure-of-scientific-m5MZej?contradicting=false&mentioning=false&page=1&unclassified=false&utm_campaign=badge&utm_medium=badge&utm_source=roamresearch.com

    - Quantifying the consensus on anthropogenic global warming in the scientific literature - [[[scite report]]]: https://scite.ai/reports/quantifying-the-consensus-on-anthropogenic-4GdDg3?contradicting=false&mentioning=false&page=1&unclassified=false
[[September 17th, 2021]]

- [[@fanelliBibliometricEvidenceHierarchy2013]] for [[[[QUE]] - How can we measure quality legibility of a field?]] and [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - thread back from [[Auguste Comte]], ties back to ideas of "hard science" and "soft science". i do like that they acknowledge here the move in post-positivism as well as alternative views.

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FFRxRBrAVx0.png?alt=media&token=c3ff1b37-e259-493e-92d9-635f4acaf3ec) (p. 1)

    - possible reason for "softness" - increased "complexity". this reminds me of [[@howardFaceMasksCOVID192020]], and makes me wonder about the ratio of lit reviews to primary papers as one noisy proxy for the softness of a field (more = softer)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FONI-qIoAF_.png?alt=media&token=773e292c-35a2-4288-81ce-9de3c758b986) 
![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F7jaWWXyd10.png?alt=media&token=44f016bb-0a24-49bc-b401-3ba5949d10b3)(pp. 1-2)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1Rc_IWtZEz.png?alt=media&token=c44a4484-2bc1-459f-9dea-f3f1524851d2) (p. 2)

    - methods notes

        - same hypothesized ordering here as in [[@coleHierarchySciences1983]] and [[@simontonVarietiesScientificCreativity2009]], from physics to biology to social science

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjWysvSGtjC.png?alt=media&token=167979b3-c1bc-46ac-9b1a-bea2418ff71f) (p. 2)

        - sample: ~29k papers published in first issue of 2012 across 12 disciplines, ranging from physical sciences (space science, physics, chemistry) to hard (molecular biology, biology and biochemistry) and soft biological sciences (plant and animal sciences, environment/ecology), to social sciences (psychiatry/psychology, econ & business, social sciences general)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FKKYGlx4Jbp.png?alt=media&token=ad6bbcd1-8a2a-4db5-a20f-7b89f2fe1718) (p. 3)

        - measures included number of authors (+), length of article (-), number of references (-), references to monographs (-), age of references (aka [[Price's index]]) (-), diversity of sources (-), relative title length (+), first-person (-), sharing of references degree (-), sharing of references intensity (+)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F562g7HBU7C.png?alt=media&token=14d06539-8276-4eec-93bf-b0ed91b7ac47) (p. 3)

        - analytic approach: multivariate analysis with an ordinal regression in a generalized linear model

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FhlHEmoqLZn.png?alt=media&token=96d7b3e9-37d0-478b-b24c-24de2433403f) (p. 8)

    - results

        - [[[[EVD]] - statistically significant but small differences in bibliometric measures of consensus across papers in physical, hard and soft biological, and social sciences  - [[@fanelliBibliometricEvidenceHierarchy2013]]]]
[[September 13th, 2021]]

- More on [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] and [[[[QUE]] - How can we measure quality legibility of a field?]]

    - [[Thomas Kuhn]] has the idea of a paradigm and "normal science". But according to Crotty's reading of Kuhn in Ch. 2, the presence of a paradigm is actually associated with *lower* tolerance for novelty!

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FAQjJiiQ4Kv.png?alt=media&token=86d4fe36-357d-4c46-a9b2-87fd00c3f02b)
