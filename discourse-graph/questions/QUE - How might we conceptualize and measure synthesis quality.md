---
title: [[QUE]] - How might we conceptualize and measure synthesis quality?
url: https://roamresearch.com/#/app/megacoglab/page/oK6BmDyst
author: Joel Chan
date: Thu Jun 10 2021 09:55:33 GMT-0400 (Eastern Daylight Time)
---

- The central open question we want to answer in this work is whether/how [[[[PTN]] - discourse graph]] can improve the quality of individual synthesis, given equivalent effort.
- There are two ways to frame the question. The first is to simply ask whether the probability that an effective synthesis is produced for a given project (and associated literature collection) increases when a researcher creates and uses [[[[PTN]] - discourse graph]], and if so, by how much. The second is to ask whether the best level of synthesis that can be achieved on a fixed budget increases when [[[[PTN]] - discourse graph]] are used, and if so, by how much.
- In both cases, we need a firm conceptualization of synthesis quality. After previous work on synthesis in scholarly work [[@strikeTypesSynthesisTheir1983]][[@booteScholarsResearchersCentrality2005]], we can conceptualize synthesis quality along two major axes:

    - 1) The resulting synthesis includes **critical engagement** with/across sources. Sources are not taken at face value, but are instead weighed and put in conversation with each other, bringing out key points of strength and weakness in individual studies, and articulating points of conflict and convergence across sources. Oten this entails articulating commonalities/differences in contextual details across sources that may not be commonly discussed. Examples of this might be an articulation of a key methodological gap in a body of evidence; or discussing how a few critical studies might outweigh a larger collection of weaker evidence, based on differences in internal and/or external validity; or identifying a possible mediator/moderator for a causal pathway based on systematic variations in effects across a contextual dimension.

    - 2) The resulting synthesis contributes a **new, useful conceptual whole** that is not clearly seen in the individual or aggregated source materials. Examples of this might be a new understanding of a key concept that clarifies connections and differences across disciplinary boundaries; or a novel mechanism (and associated hypotheses to test) that might account for contradictory observations in a body of findings; or a novel relationship to examine between concepts not previously thought to be closely related; or a novel theory/model of a phenomenon.
- These axes of quality have emerged from conceptual and critical reflection on the nature of evidence/conceptual synthesis [[@strikeTypesSynthesisTheir1983]], as well as efforts in doctoral education to create rubrics for assessing the level of synthesis in dissertation literature reviews [[@granelloPromotingCognitiveComplexity2001]][[@booteScholarsResearchersCentrality2005]] and empirical analyses of dissertation examiners' comments on dissertation literature reviews [[@lovittsMakingImplicitExplicit2007]][[@holbrookInvestigatingPhDThesis2004]].
- These aspects of quality are hard to measure in the laboratory. It is hard to leave room for significant conceptual innovation in a controlled task, and harder still to measure conceptual innovation in a consistent way that is amenable to valid statistical analysis. And it is also very hard to leave enough time for a significant conceptual innovation to emerge within the usual time constraints (on the order of an hour or so) of a typical laboratory study. For these reasons, systematic study of [[[[PTN]] - discourse graph]] (and their relationship to synthesis) in authentic scientific practice is essential to really understand whether/how [[[[PTN]] - discourse graph]] yield real benefits for synthesis.
- That is not to say that experiments could not uncover part of the picture: while it might be difficult or impossible to observe (and therefore assess) a new conceptual whole in a laboratory session, we might be able to observe and assess patterns of engagement with sources (e.g., whether/how there is critical engagement with sources and their contextual details). And in limited cases, it might be informative to observe if participants in a study can, given a starting [[[[PTN]] - discourse graph]] and one or more known conceptual innovations that are latent in the source set, reproduce or approximate the conceptual innovations.
- In any case, neither type of study has really been done for [[[[PTN]] - discourse graph]].
- A fundamental uncertainty is whether the relatively formalized nature of [[[[PTN]] - discourse graph]] can be integrated with the open-ended, seemingly unstructured, backtrack-ridden, speculative and "half-baked" nature of thinking that underlies real synthesis (see [[@gruberDarwinManPsychological1974]] for vivid descriptions of this). Some past work on whether/how formality can be integrated into systems for supporting thinking suggests there is a real danger that imposing such formalisms may yield more harm (e.g., by cutting off speculative lines of inquiry, imposing too much overhead) than benefits in settings characterized by context-dependent, evolving requirements for formal structure, such as creative knowledge work in uncharted domains (see [[@shipmanFormalityConsideredHarmful1999]] for an excellent introduction to this). A related danger is that the kinds of knowledge that is necessary for
- A counterpoint to this might come from studies of sensemaking, decision-support systems, and also programming practice (daur), where the forcing function of explicit modeling (e.g., through abstractions and functions) yields significant benefits for constructing better mental models of a domain.

###### References

[[January 31st, 2021]]

- Subsection articulating research [question](#[[❔ questions]]): ^^**Can this new medium improve the quality of individual synthesis, given equivalent effort? If so, how?**^^ - also relevant for [[[[QUE]] - How might we conceptualize and measure synthesis quality?]]

    - There are two ways to frame the question. The first is to simply ask whether the probability that an effective synthesis is produced for a given project (and associated literature collection) increases when a researcher creates and uses [[[[PTN]] - discourse graph]], and if so, by how much. The second is to ask whether the best level of synthesis that can be achieved on a fixed budget increases when [[[[PTN]] - discourse graph]] are used, and if so, by how much.

    - In both cases, we need a firm conceptualization of synthesis quality. After previous work on synthesis in scholarly work [[@strikeTypesSynthesisTheir1983]][[@booteScholarsResearchersCentrality2005]], we can conceptualize synthesis quality along two major axes:

        - 1) The resulting synthesis includes **critical engagement** with/across sources. Sources are not taken at face value, but are instead weighed and put in conversation with each other, bringing out key points of strength and weakness in individual studies, and articulating points of conflict and convergence across sources. Oten this entails articulating commonalities/differences in contextual details across sources that may not be commonly discussed. Examples of this might be an articulation of a key methodological gap in a body of evidence; or discussing how a few critical studies might outweigh a larger collection of weaker evidence, based on differences in internal and/or external validity; or identifying a possible mediator/moderator for a causal pathway based on systematic variations in effects across a contextual dimension.

        - 2) The resulting synthesis contributes a **new, useful conceptual whole** that is not clearly seen in the individual or aggregated source materials. Examples of this might be a new understanding of a key concept that clarifies connections and differences across disciplinary boundaries; or a novel mechanism (and associated hypotheses to test) that might account for contradictory observations in a body of findings; or a novel relationship to examine between concepts not previously thought to be closely related; or a novel theory/model of a phenomenon.

    - These axes of quality have emerged from conceptual and critical reflection on the nature of evidence/conceptual synthesis [[@strikeTypesSynthesisTheir1983]], as well as efforts in doctoral education to create rubrics for assessing the level of synthesis in dissertation literature reviews [[@granelloPromotingCognitiveComplexity2001]][[@booteScholarsResearchersCentrality2005]] and empirical analyses of dissertation examiners' comments on dissertation literature reviews [[@lovittsMakingImplicitExplicit2007]][[@holbrookInvestigatingPhDThesis2004]].

    - These aspects of quality are hard to measure in the laboratory. It is hard to leave room for significant conceptual innovation in a controlled task, and harder still to measure conceptual innovation in a consistent way that is amenable to valid statistical analysis. And it is also very hard to leave enough time for a significant conceptual innovation to emerge within the usual time constraints (on the order of an hour or so) of a typical laboratory study. For these reasons, systematic study of [[[[PTN]] - discourse graph]] (and their relationship to synthesis) in authentic scientific practice is essential to really understand whether/how [[[[PTN]] - discourse graph]] yield real benefits for synthesis.

    - That is not to say that experiments could not uncover part of the picture: while it might be difficult or impossible to observe (and therefore assess) a new conceptual whole in a laboratory session, we might be able to observe and assess patterns of engagement with sources (e.g., whether/how there is critical engagement with sources and their contextual details). And in limited cases, it might be informative to observe if participants in a study can, given a starting [[[[PTN]] - discourse graph]] and one or more known conceptual innovations that are latent in the source set, reproduce or approximate the conceptual innovations.

    - In any case, neither type of study has really been done for [[[[PTN]] - discourse graph]].

    - A fundamental uncertainty is whether the relatively formalized nature of [[[[PTN]] - discourse graph]] can be integrated with the open-ended, seemingly unstructured, backtrack-ridden, speculative and "half-baked" nature of thinking that underlies real synthesis (see [[@gruberDarwinManPsychological1974]] for vivid descriptions of this). Some past work on whether/how formality can be integrated into systems for supporting thinking suggests there is a real danger that imposing such formalisms may yield more harm (e.g., by cutting off speculative lines of inquiry, imposing too much overhead) than benefits in settings characterized by context-dependent, evolving requirements for formal structure, such as creative knowledge work in uncharted domains (see [[@shipmanFormalityConsideredHarmful1999]] for an excellent introduction to this). A related danger is that the kinds of knowledge that is necessary for

    - A counterpoint to this might come from studies of sensemaking, decision-support systems, and also programming practice (daur), where the forcing function of explicit modeling (e.g., through abstractions and functions) yields significant benefits for constructing better mental models of a domain.

    - The central open question we want to answer in this work is whether/how [[[[PTN]] - discourse graph]] can improve the quality of individual synthesis, given equivalent effort.
[[June 10th, 2021]]

- {{[[TODO]]}} [[Joel Chan]] Write up and collate something around [[[[QUE]] - How might we conceptualize and measure synthesis quality?]] #Backlog

    - This is where the stuff from [[@lovittsMakingImplicitExplicit2007]], [[@granelloPromotingCognitiveComplexity2001]], and others will come in, as well as some examples like [[@howardFaceMasksCOVID192020]]

    - And we can talk about [[systematic review]]s being narrower subsets of synthesis (see [[@greenhalghTimeChallengeSpurious2018]]: #CLlaim [[[[CLM]] - [[systematic review]]s are typically narrowly focused, and provide less insight]])
[[June 29th, 2021]]

- #WorkingOn [[[[QUE]] - How might we conceptualize and measure synthesis quality?]] for [[outline/2021-06-29 talk on knowledge management with [[org/Protocol Labs]] summer research associates]]

    - [[@booteScholarsResearchersCentrality2005]] has a rubric

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyPK1uMfJyb?alt=media&token=4a2ebef2-85f1-48e7-b5a2-8a5ad89d5c4d)

    - [[@lovittsMakingImplicitExplicit2007]] has rubrics from a bunch of fields

        - intuition quote:

            - > Students who write acceptable literature reviews often take the literature they have read at face value and do not (or cannot) discriminate between good papers and bad ones. Their literature reviews are descriptive summaries, “so-and-so and so-and-so said,”, that make obvious points. Similarly, unacceptable literature reviews lack an organizing intelligence. (p. 45)

    - [[@strikeTypesSynthesisTheir1983]]

        - > A quality [[synthesis]] will clarify and resolve, rather than obscure inconsistencies or tensions between material synthesized

        - > A quality [[synthesis]] will result in a progressive problem shift

        - > A successful [[synthesis]] will satisfy the formal criteria for good theories
[[June 14th, 2022]]

- [[[[QUE]] - How might we conceptualize and measure synthesis quality?]]

    - coming up with plausible confounds as a measure

    - ruling out possible confounds
