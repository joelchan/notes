---
title: [[QUE]] - How can we measure quality legibility of a field?
url: https://roamresearch.com/#/app/megacoglab/page/9x3HF8Ce0
author: Joel Chan
date: Fri Sep 10 2021 22:04:54 GMT-0400 (Eastern Daylight Time)
---

- #Question

    - Related to: [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - # Synthesis

        - [[@coleHierarchySciences1983]] argues that basically frontier-level measures (citation concentration, obsolescence, peer review IRR, peer evals) don't give us much signal, but ["core knowledge" measures (e.g., textbooks) probably will]([[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]).

            - Really need to trace this forward, see whether there are any better signals today with better methods. To their credit, Cole et al were quite upfront about methodological limitations.

                - One pointer here is [[@fanelliBibliometricEvidenceHierarchy2013]]

    - ---

    - # #annotatedbib

        - ## #references

            - [[@coleHierarchySciences1983]]

                - bunch more from: core of first half of paper is review of a bunch of prior studies, which basically provide no/negative evidence that there's systematic variation in consensus across fields that are thought to be at different levels of the [[hierarchy of the sciences]]: [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]

        - ## #[[Claims and Evidence]]

        - ## #Integrated

        - ## #NotRelevant

    - ---

    - # Meta

        - Tags:

###### References

[[September 10th, 2021]]

- Also create [[[[QUE]] - How can we measure quality legibility of a field?]]

    - [[@coleHierarchySciences1983]] [[@coleHierarchySciences1983]] argues that basically frontier-level measures (citation concentration, obsolescence, peer review IRR, peer evals) don't give us much signal, but ["core knowledge" measures (e.g., textbooks) probably will]([[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]).

        - I think a concrete idea might be to use data from [[OpenSyllabus.org]] to sort of replicate the measures of age distribution of references and total number of references

        - #[[➰ breadcrumbs]] need to follow up on this though. this was work done in the early days; follow citation trails forward.

            - frustrating because the paper has like ~600 citations on Google Scholar, but only 2 "meaningful" ones on [Scite](https://scite.ai/reports/the-hierarchy-of-the-sciences-XpQYXw?contradicting=false&mentioning=false&page=1&unclassified=false&utm_campaign=badge&utm_medium=badge&utm_source=scholar.google.com), both supportive, but not really granular, and not really true to the claims of Cole :/

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FOtCoGzA6uN.png?alt=media&token=6a772c92-4c07-4bbd-8bf7-61d35daf4f98)
[[September 15th, 2021]]

- #[[➰ breadcrumbs]] Thread on [[hierarchy of the sciences]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] and [[[[QUE]] - How can we measure quality legibility of a field?]]

    - Cole: The hierarchy of the sciences? - Google Scholar: https://scholar.google.com/scholar?start=10&hl=en&as_sdt=20000005&sciodt=0,21&as_ylo=2017&cites=12177003916617353377&scipsc=

    - Vocabulary sharing among subjects belonging to the hierarchy of sciences | SpringerLink: https://link.springer.com/article/10.1007/s11192-020-03671-7

    - Co-existing Notions of Research Quality: A Framework to Study Context-specific Understandings of Good Research | SpringerLink: https://link.springer.com/article/10.1007/s11024-019-09385-2

    - Psychology as a Science within Comte's Hypothesized Hierarchy: Empirical Investigations and Conceptual Implications - Dean Keith Simonton, 2015: https://journals.sagepub.com/doi/10.1037/gpr0000039

    - Bibliometric Evidence for a Hierarchy of the Sciences - [[[scite report]]] for [[@fanelliBibliometricEvidenceHierarchy2013]]: https://scite.ai/reports/bibliometric-evidence-for-a-hierarchy-xAYp43?contradicting=false&mentioning=false&page=1&unclassified=false&utm_campaign=plugin&utm_medium=plugin&utm_source=generic

    - Three+one bonus Google Scholar improvements that I wish Google would do. | Aaron Tay's Musings about librarianship: http://musingsaboutlibrarianship.blogspot.com/2021/09/threeone-bonus-google-scholar.html

    - The Temporal Structure of Scientific Consensus Formation - [[[scite report]]]: https://scite.ai/reports/the-temporal-structure-of-scientific-m5MZej?contradicting=false&mentioning=false&page=1&unclassified=false&utm_campaign=badge&utm_medium=badge&utm_source=roamresearch.com

    - Quantifying the consensus on anthropogenic global warming in the scientific literature - [[[scite report]]]: https://scite.ai/reports/quantifying-the-consensus-on-anthropogenic-4GdDg3?contradicting=false&mentioning=false&page=1&unclassified=false
[[September 17th, 2021]]

- [[@fanelliBibliometricEvidenceHierarchy2013]] for [[[[QUE]] - How can we measure quality legibility of a field?]] and [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]]

    - thread back from [[Auguste Comte]], ties back to ideas of "hard science" and "soft science". i do like that they acknowledge here the move in post-positivism as well as alternative views.

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FFRxRBrAVx0.png?alt=media&token=c3ff1b37-e259-493e-92d9-635f4acaf3ec) (p. 1)

    - possible reason for "softness" - increased "complexity". this reminds me of [[@howardFaceMasksCOVID192020]], and makes me wonder about the ratio of lit reviews to primary papers as one noisy proxy for the softness of a field (more = softer)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FONI-qIoAF_.png?alt=media&token=773e292c-35a2-4288-81ce-9de3c758b986) 
![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F7jaWWXyd10.png?alt=media&token=44f016bb-0a24-49bc-b401-3ba5949d10b3)(pp. 1-2)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1Rc_IWtZEz.png?alt=media&token=c44a4484-2bc1-459f-9dea-f3f1524851d2) (p. 2)

    - methods notes

        - same hypothesized ordering here as in [[@coleHierarchySciences1983]] and [[@simontonVarietiesScientificCreativity2009]], from physics to biology to social science

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjWysvSGtjC.png?alt=media&token=167979b3-c1bc-46ac-9b1a-bea2418ff71f) (p. 2)

        - sample: ~29k papers published in first issue of 2012 across 12 disciplines, ranging from physical sciences (space science, physics, chemistry) to hard (molecular biology, biology and biochemistry) and soft biological sciences (plant and animal sciences, environment/ecology), to social sciences (psychiatry/psychology, econ & business, social sciences general)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FKKYGlx4Jbp.png?alt=media&token=ad6bbcd1-8a2a-4db5-a20f-7b89f2fe1718) (p. 3)

        - measures included number of authors (+), length of article (-), number of references (-), references to monographs (-), age of references (aka [[Price's index]]) (-), diversity of sources (-), relative title length (+), first-person (-), sharing of references degree (-), sharing of references intensity (+)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F562g7HBU7C.png?alt=media&token=14d06539-8276-4eec-93bf-b0ed91b7ac47) (p. 3)

        - analytic approach: multivariate analysis with an ordinal regression in a generalized linear model

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FhlHEmoqLZn.png?alt=media&token=96d7b3e9-37d0-478b-b24c-24de2433403f) (p. 8)

    - results

        - [[[[EVD]] - statistically significant but small differences in bibliometric measures of consensus across papers in physical, hard and soft biological, and social sciences  - [[@fanelliBibliometricEvidenceHierarchy2013]]]]
[[September 13th, 2021]]

- More on [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] and [[[[QUE]] - How can we measure quality legibility of a field?]]

    - [[Thomas Kuhn]] has the idea of a paradigm and "normal science". But according to Crotty's reading of Kuhn in Ch. 2, the presence of a paradigm is actually associated with *lower* tolerance for novelty!

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FAQjJiiQ4Kv.png?alt=media&token=86d4fe36-357d-4c46-a9b2-87fd00c3f02b)
