---
title: [[QUE]] - What is context for the purposes of scholarly synthesis?
url: https://roamresearch.com/#/app/megacoglab/page/fFCAtZVW5
author: Joel Chan
date: Fri Jan 08 2021 00:51:02 GMT+0800 (Malaysia Time)
---

- [[question]]

    - Tags: [[DSynthesis Infrastructure]]

    - Description

        - When interpreting and reusing ideas from past work, questions of applicability, validity, and significance are of central concern. New research questions are frequently framed against important "gaps" in these areas. Consider the following example. Suppose an HCI researcher is interested in moderation and online harassment. She is interested specifically in understanding the costs and benefits of strong moderation actions such as banning "bad faith" interlocutors (aka trolls) on the quality of discourse in a politics-focused subreddit. Regarding **applicability**, she needs to judge which studies and findings/theories are "actually applicable" (e.g.,, studying similar populations, interventions, settings, or outcome measures?) to her setting. Should she care that two studies found that temporary (~1 day) bans on bad faith actors had no effect on the degree to which participants used negative sentiments in their posts during the ban? How applicable is a theory of bad faith and discourse proposed by a philosopher from the early 2000's (before the rise of modern social media)? Regarding **validity**, she needs to know what findings have been established with sufficient certainty. Would it matter if the majority of past studies had relied on self-report measures (but no behavioral measures) of discourse quality? What if the bulk of studies with positive effects for bans were coming from authors in a tight interrelated network of researchers who came from the same lab? Finally, regarding **significance**, she needs to judge which findings and concepts have the highest potential for knowledge gain. She might want to avoid studying effects of interventions that had been established across many studies with high precision to have small effects on discourse quality. She might also want to focus on ban intervention-outcome pairs that have mixed effects in past studies. Or she might want to focus on hypotheses that have opposing predictions from multiple prominent theories, or a critical prediction from a single theory.

        - These complex questions require reasoning over far more than the claims from past papers: for example, to judge applicability, details of ^^methodology^^ (participants, setting, measures) and ^^metadata^^ (field of the author/journal, year of publication) are needed. Similarly, for validity, details of methodology are required. Judgments of applicability might also be necessary to reason about the degree of concordance across studies (which studies that have measured "the same thing" came to "the same" conclusions?). Finally, judgments of significance requires complex reasoning over the ^^discourse context^^ assumed by a given study, degree of concordance with other studies, details of results such as effect sizes, and more.

        - Under some conditions, there can be relatively standardized sets of contextual information that will be broadly useful for judgments, such as the [PICO]([[PICO frames]]

        - However, as theories of context and [[reuse]]

            - [[CLM - Compression and contextualizability are in tension]]

            - [[CLM - Specifying context for future reuse requires predicting trajectories of future reuse]]

        - We consider the information required to make judgments of applicability, validity, and significance to be [[context]]

    - R-Sources

###### Discourse Context

- **Informed By::** [[CLM - Specifying context for future reuse requires predicting trajectories of future reuse]]
- **Informed By::** [[CLM - Predicting trajectories of future reuse of information objects is hard]]
- **Informed By::** [[CLM - Compression and contextualizability are in tension]]
- **Informed By::** [[@blakeCollaborativeInformationSynthesis2006]]
- **Informed By::** [[@tongConsolidatedCriteriaReporting2007]]
