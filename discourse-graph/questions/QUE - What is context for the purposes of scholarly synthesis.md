---
title: [[QUE]] - What is context for the purposes of scholarly synthesis?
url: https://roamresearch.com/#/app/megacoglab/page/fFCAtZVW5
author: Joel Chan
date: Thu Jan 07 2021 11:51:02 GMT-0500 (Eastern Standard Time)
---

- #[[Question]]

    - Tags: #[[D/Synthesis Infrastructure]]

    - Description

        - When interpreting and reusing ideas from past work, questions of applicability, validity, and significance are of central concern. New research questions are frequently framed against important "gaps" in these areas. Consider the following example. Suppose an HCI researcher is interested in moderation and online harassment. She is interested specifically in understanding the costs and benefits of strong moderation actions such as banning "bad faith" interlocutors (aka trolls) on the quality of discourse in a politics-focused subreddit. Regarding **applicability**, she needs to judge which studies and findings/theories are "actually applicable" (e.g.,, studying similar populations, interventions, settings, or outcome measures?) to her setting. Should she care that two studies found that temporary (~1 day) bans on bad faith actors had no effect on the degree to which participants used negative sentiments in their posts during the ban? How applicable is a theory of bad faith and discourse proposed by a philosopher from the early 2000's (before the rise of modern social media)? Regarding **validity**, she needs to know what findings have been established with sufficient certainty. Would it matter if the majority of past studies had relied on self-report measures (but no behavioral measures) of discourse quality? What if the bulk of studies with positive effects for bans were coming from authors in a tight interrelated network of researchers who came from the same lab? Finally, regarding **significance**, she needs to judge which findings and concepts have the highest potential for knowledge gain. She might want to avoid studying effects of interventions that had been established across many studies with high precision to have small effects on discourse quality. She might also want to focus on ban intervention-outcome pairs that have mixed effects in past studies. Or she might want to focus on hypotheses that have opposing predictions from multiple prominent theories, or a critical prediction from a single theory.

        - These complex questions require reasoning over far more than the claims from past papers: for example, to judge applicability, details of ^^methodology^^ (participants, setting, measures) and ^^metadata^^ (field of the author/journal, year of publication) are needed. Similarly, for validity, details of methodology are required. Judgments of applicability might also be necessary to reason about the degree of concordance across studies (which studies that have measured "the same thing" came to "the same" conclusions?). Finally, judgments of significance requires complex reasoning over the ^^discourse context^^ assumed by a given study, degree of concordance with other studies, details of results such as effect sizes, and more.

        - Under some conditions, there can be relatively standardized sets of contextual information that will be broadly useful for judgments, such as the [PICO]([[PICO frames]]) (Population, Intervention, Comparator, Outcome) and worksheets like [[Risk of Bias scores]] for precisely focused [[systematic review]]s of [[RCT]]s. The [[org/Oxford Center for Evidence-Based Medicine]] has a [list of useful worksheets](https://www.cebm.ox.ac.uk/resources/ebm-tools/critical-appraisal-tools) for [[critical appraisal]] of various genres of research, including qualitative and case studies. For the latter, more focused checklists like [[Consolidated Criteria for Reporting Qualitative Studies]] ([[@tongConsolidatedCriteriaReporting2007]]) are also useful

        - However, as theories of context and [[reuse]] in CSCW would predict, what specific kinds of contextual information are required for a literature review can often vary substantially across research projects. For instance, [[@blakeCollaborativeInformationSynthesis2006]] discovered in their cognitive work analysis of a team of medical researchers conducting systematic reviews that [the particulars of information needed can vary substantially by the nature of the research question](some types of [[context]] information were more contextual, depending on the particular "hypothesis projection" of the review, which varied across the lifecycle of the project studied (e.g., location of medical condition, amount of exposure, confounding risk factors), while others were more constant regardless of hypothesis (e.g., study- and population-context information)). In interdisciplinary fields such as HCI, the nature of contextual information needed to critically assess and synthesize past work can be found in handbooks like the popular "Ways of Knowing in HCI" book [[Ways of Knowing in HCI]] that includes discussions, and to a lesser extent in research methods texts, where questions of internal and external validity (which map to validity and applicability) are discussed, and the information required to judge these questions, are enumerated.

            - [[[[CL]] - Compression and contextualizability are in tension]]

            - [[[[CLM]] - Specifying context for future reuse requires predicting trajectories of future reuse]], but [[[[CL]] - Predicting trajectories of future reuse of information objects is hard]]

        - We consider the information required to make judgments of applicability, validity, and significance to be [[context]] for the information being reused (i.e., sources and claims) in [literature reviewing]([[synthesis]]).

    - R-Sources

###### References

[[July 18th, 2020]]

- Where might we get an enumeration of the space of possible [[context]]-relevant bits? [*]()[[[[QUE]] - What is context for the purposes of scholarly synthesis?]]

    - One source is studies of [[metaknowledge]] [[@evansMetaknowledge2011]]

    - Another clue is to think more about the **goals** of [[reuse]] for [[synthesis]], which we might phrase as questions. Each of these questions may call to mind different kinds of [[context]].

        - See list of [[context queries]]

    - Can also go bottom up from [[Query: Examples of context]]

    - Another clue is to think of "research methods" - what would convince X person that Y claim is trustworthy?

        - Ex: [[Cochrane systematic reviews]] produces standards of evidence (such as [[Risk of Bias scores]]), which rely on sets of features from papers.

        - Ex: [[@hayesKnowingDoingAction2014]] What makes for good [[m/Participatory Action Research]]? (lots more in [[Ways of Knowing in HCI]])

    - Going deeper, you can think of the consequences of [[social construction]], to get at the [[subtext]] of what is going on.
[[January 7th, 2021]]

- Starting a new question note for [[D/Synthesis Infrastructure]] about [[context]]: [[[[QUE]] - What is context for the purposes of scholarly synthesis?]]

    - Starting points for this question come from the reviews for our [[WP: Context Capture Process Patterns and Tool Affordances]] submission

        - {{embed: Issue: conceptual clarity around the concept of context for lit reviewing specifically}}

    - Also related:

        - [[[[CLM]] - Context is necessary for reasoning under uncertainty]]

        - [[[[CLM]] - Contextualizability is necessary for synthesis]]

        - [[[[QUE]] - How can we support explicit contention with evidence when synthesizing knowledge claims?]]

    - There is a nice distinction that R1 makes about the differences between metadata and methods as types of context. I think this is insightful.

        - There is a connection here to two general buckets of "context":

            - The lit on [[critical appraisal]], that goes nicely with standards for [[systematic review]]s, evidence quality, and evidence appraisal for translational use of research and [[evidence-based medicine]]

                - cc some great resources that [[Brandon Toner]] sent me from his doctoral course on [[critical appraisal]]

                    - Critical Appraisal tools — Centre for Evidence-Based Medicine (CEBM), University of Oxford: https://www.cebm.ox.ac.uk/resources/ebm-tools/critical-appraisal-tools

                    - from chat:

                        - Brandon Toner  9:24 AM

                            - Hey Joel!

                            - Heard back from my critical appraisal professor — he offered some initial thoughts which are quite helpful!

                            - “I am happy to have a quick chat and maybe share what I think would be important things to consider. I think for me the initial important things in the types of studies you are reading that I would look at are the following:

                            - 1. Data sources

                            - a. Quality

                            - b. Previous use

                            - c. validation

                            - 2. Outcome measures

                            - a. Validated?

                            - b. How did they define them

                            - 3. Control of time, bias and confounding

                            - a. Selection bias in testing (lots of bias)

                            - b. Collider bias.

                            - c. Timing in the overall pandemic

                            - d. Other factors we know impacting this such as SES, urban/rural, race, age etc.

                            - 4. Impact of policies in the background”

                            - 9:24

                            - He was quite interested in the project and offered to chat about it with me — so I’ll followup after I discuss with him.

                            - 9:25

                            - This should be useful for guiding some commentary on the included studies.  (edited)

                        - Joel Chan  2:24 PM

                            - this is a wonderful, thoughtful list that's very useful!

                            - 2:26

                            - could see each of these becoming attributes, maybe? one question might be how to structure the observation notes to maximize the usability of these attributes (ideally we can use them to reason about individual findings, not just whole papers). one issue with roam atm is that block refs don't "bring along" the page refs with them, which makes them not queryable. but i wonder if we could write a smartblock solutino to this.

                            - 2:26

                            - btw, @mattbrockwell @Brandon would love to connect soon (in the next week if possible?) to discuss best practices for multiplayer and how to set up conventions/etc. for this project.

                            - I'll also be testing out what i learn from this in my own lab :slightly_smiling_face:

                        - Brandon Toner  2:34 PM

                            - Perhaps the main points could become attributes:

                            - Data Sources::

                            - Outcome Measures::

                            - Control of time, bias and confounding:: (alt. Wording?)

                            - Impact of policies in the background:: (alt. Wording?)

                            - 2:35

                            - I agree, the observation notes should be structured in such a way as to make attributes easy to nest.

                            - 2:38

                            - Main considerations for multiplayer:

                            - data structures to promote clarity and search ability

                            - Ability to easily determine authorship

                            - Ability to support multiple versions of something / conflicting data from multiple users

                            - Processes for approval, adoption of data into database. (Post-review observation notes for example)

                            - History/versions preservation if necessary to see how something was changed.

                            - Collaboration/chat

                        - Joel Chan  2:39 PM

                            - yup was thinking main points could be attributes, maybe in addition to the more standard PICO categories?

                        - Brandon Toner  2:40 PM

                            - PICO is good for framing CONTEXT (what clinical question was the research attempting to address?)

                            - 2:41

                            - The above is the APPRAISAL (evaluation of the QUALITY of the evidence)

                        - Joel Chan  2:41 PM

                            - P in PICO at least is context to me, though

                            - 2:41

                            - in the sense that, independent of quality concerns, we also want to reason about generalizability

                            - :+1:

                            - 1

                            - 2:42

                            - so population comes into play

                            - 2:42

                            - as does intervention and outcome, in the sense that operationalizations of constructs might vary across contexts and studies

                            - 2:43

                            - ex: some researchers i've been working with noted that for studies of vitamin b6 as a potential booster of immune function, the manner of intervention (e.g., dosage, mixture, concentration, etc.) matters a LOT

                        - Brandon Toner  2:43 PM

                            - Yup, the main aspects of appraisal ask:

                                - is it valid? (Were the methods appropriate? Internal validity)

                                - Is it important? (What were the results? Significant?)

                                - Is it applicable? (Generalizable)

                        - Joel Chan  2:43 PM

                            - ah, ok appraisal covers generalizability then

                            - :+1:

                            - 1

                            - 2:44

                            - would you be willing to share your syllabus for the critical appraisal course with me?

                            - :+1:

                            - 1

                            - 2:44

                            - i'm actually kind of shocked this isn't standard issue in phd programs generally

                            - 2:44

                            - like, it's not really taught explicitly in most places i can think of (certainly not in my phd, or current program, or anywhere else i've had experience)

                        - Brandon Toner  2:45 PM

                            - Yea, it’s mixed for inclusion in medical profession curriculums as well. I’ve been lucky enough to have two courses with excellent profs. Both with different approaches/backgrounds.

                            - 2:46

                            - I consider it a VERY important skill set in medicine — it is the backbone of evidence-based medicine

                        - Joel Chan  2:46 PM

                            - i mean, some of this comes into play in research methods courses, but these are more for the generation of evidence, which i guess students are supposed to extrapolate to appraisal of evidence, so it's not... terrible?

                            - :+1:

                            - 1

                        - Brandon Toner  2:47 PM

                            - That’s the kind of training I’d like more of. I’m more trained in the APPLICATION of evidence, as opposed to the generation of evidence

                            - 1 reply

                            - 21 hours agoView thread

                        - Brandon Toner  2:55 PM

                            - PDF

                            - CourseOutlineSyllabus_student2020F (1).pdf

                            - 227 kB PDF227 kB — Click to view

                        - Joel Chan  3:04 PM

                        - awesome thanks!

                        - :+1:

                        - 1

                        - New

                        - Brandon Toner  3:38 PM

                        - Some additional resources:

                        - https://casp-uk.net/casp-tools-checklists/

                        - https://ebm-tools.knowledgetranslation.net/card

                        - https://www.cebm.ox.ac.uk/resources/ebm-tools/critical-appraisal-tools

                        - CASP - Critical Appraisal Skills ProgrammeCASP - Critical Appraisal Skills Programme

                        - CASP CHECKLISTS - CASP - Critical Appraisal Skills Programme

                        - This set of eight critical appraisal tools are designed to be used when reading research. CASP has appraisal checklists designed for use with Systematic Reviews, Randomised Controlled Trials, Cohort Studies, Case Control Studies, Economic Evaluations, Diagnostic Studies, Qualitative studies and Clinical Prediction Rule. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0

                        - Est. reading time

                        - 3 minutes

                        - ebm-tools.knowledgetranslation.net

                        - Pocket Cards

                        - Pocket Cards

                        - cebm.ox.ac.ukcebm.ox.ac.uk

                        - Critical Appraisal tools

                        - Critical appraisal worksheets to help you appraise the reliability, importance and applicability of clinical evidence.

                        - :heart:

                        - 1

                - This is where stuff like [[PICO frames]] come in, as well as the stuff from [[Cochrane systematic reviews]]'s [[Risk of Bias scores]]

    - I like the way that Brandon phrases the key questions to ask from his [[critical appraisal]] class:

        - {{embed: Yup, the main aspects of appraisal ask:}}

        - Reminds me very much of [[context queries]]

        - To draw the analogy back to [[context]] in the the [[reuse]] literature, in studies of reuse in various settings, a key aspect of reuse was the ability to trust/interpret/understand a piece of information from someone else in the past, in order to *use* it for some purpose.

            - Let's go back and consider a concrete example from [[@ackermanOrganizationalMemoryObjects2004]]

                - To properly deal with a [discrepancy in benefits from the caller](((aJyU_lBdO))), a [HLG employee](((95SLsSLsN))) needed to [[reuse]] information from the [CARL database](((OLechJ6p6))), among other sources of information, to create an escalation to the benefits group. To do this, she needed important [[context]] that was missing from the CARL record itself, such as details of the record's creation or maintenance (was it authoritative?), and any circumstances surrounding the caller's employment. The HLG employee dealt with this missing context by consulting an expert (a senior agent) and her own memory, rather than searching databases for additional information, even though that information could in principle be in there.

                - In this case, the notion of "correctness"/completeness was central, since the job to be done was to resolve a discrepancy. Was there an error to fix? One way to figure that out is to determine whether there were any errors in the employee record in CARL, and whether there was a need to correct it. To make this judgment, the employee needed to know whether the record had been completed, whether there were any issues that might hold up or explain the discrepancy (to be able to properly escalate to the benefits group).

            - To map this to our setting of literature reviewing and knowledge synthesis, the idea of ["authoritativeness"/correctness/completeness](In this case, the notion of "correctness"/completeness was central, since the job to be done was to resolve a discrepancy. Was there an error to fix? One way to figure that out is to determine whether there were any errors in the employee record in CARL, and whether there was a need to correct it. To make this judgment, the employee needed to know whether the record had been completed, whether there were any issues that might hold up or explain the discrepancy (to be able to properly escalate to the benefits group).) (and the necessary bits of contextual information that enable this judgement) corresponds to the notion of "[[internal validity]]", which is of central concern when "reusing" past work: we need to rely on past work to justify our research questions - how much do we know that X is true? what is our best guess of the value of X? how much uncertainty remains in our estimate of X?

                - A lot of the work in [[Risk of Bias scores]] [emphasizes this sort of judgment](> **Risk of bias assessment (sometimes called "quality assessment" or "critical appraisal") helps to establish transparency of evidence synthesis results and findings. A risk of bias assessment is often performed for each included study in your review. ** Evidence syntheses strive to eliminate bias in their findings.  Individual studies that are included in a synthesis may include biases in their results or conclusions, for example design flaws that raise questions about validity of findings or an overestimate of intervention effect.  Risk of bias assessment generally is not required with evidence synthesis methods outside of systematic reviews. However, this may depend on the evidence synthesis method that you are utilizing.).

            - [[critical appraisal]] also has notions of generalizability ([[external validity]]), especially for aggregating results across different settings/studies (do they actually go together, and in what ways), and when judging whether/how to apply insights to inform decision-making in particular settings of use (does this apply to me here now?)

            - This brief report by [[@greifenederApplesVsOranges2017]] reported that In a [sample of 15 research papers](((A7EM9MCGm))), [methods details](((6F_i67WIF))) about papers cited in a literature review were included/discussed only about half the time, even when they were directly comparing their findings

                - Their lit review has more citations that give more color to the twofold notion of [[internal validity]] (correctness) vs. [[external validity]] (comparability)

                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FCvMAkWxLmB.png?alt=media&token=97428aa3-172d-405c-b112-d8d831bedc1e)

            - Two concrete cases that come immediately to mind where we run afoul of [[external validity]] (also doubling as [examples of]([[example-of]]) [[ambiguity]]):

                - [[@leighstarThisNotBoundary2010]] complains about the undue "stretching" of the term [[[[PTN]] - boundary object]]

                - [[@coxWhatAreCommunities2005]] argues that There are very distinct "varieties" of the concept of [[communities of practice]], with very different theoretical and pragmatic implications - it's crucial to be clear about which variety you build on - papering over the distinctions will lead to confusion and watered-down impact of the concept on your work (p. 536)

            - Stretching on external validity (sort of like connecting [[far analogies]]) isn't necessarily bad on its own.

                - But it should probably be done with some degree of visibility/control.

                - cf. the notion of [potentially helpful ambiguity](Ambiguity in language can be helpful for integrating work: even [[Charles Darwin]] and [[Thomas Kuhn]] were examples of work where language was ambiguous, and this ended up having positive effects on the science) from [[James A. Evans]]

                - even [[@leighstarThisNotBoundary2010]] might say that the stretching has been productive

            - I've also talked about this twofold distinction, adding my own, from [[context queries]]:

                - [[internal validity]] --> How much do I ^^trust^^ this claim (or set of claims)?

                - [[external validity]] --> How might I ^^generalize^^ from these claims? For example, do these claims apply to my context?

                - significance --> How might I ^^explain^^ what is going on in this set of claims? For example, how might I explain contradictions or tensions amongst these ideas?

    - For [[John Morabito]], create observation notes from the key papers we cite about [[context]] and [[reuse]] in CSCW generally in intro of [[WP: Context Capture Process Patterns and Tool Affordances]]

        - {{[[DONE]]}} [[@hinrichsContextGrabbingAssigning2005]]

            - words from chi submission:

                - > Hinrichs and colleagues documented how workers in a steel mill and sewerage plant faced challenges reusing documents which were outdated or inappropriate. The use of physical documents and decentralized information management meant that multiple copies of the same document could appear in different locations, each with varying levels of accuracy or completeness. In one extreme case, this “incomplete or inaccessible documentation” could lead to “costly exploratory ‘digging by hand’ to avoid damaging power lines” (p.375). - from chi submission

        - {{[[DONE]]}} [[@dourishInformationContextLessons1993]]

            - words from chi submission:

                - > Similarly, Dourish and colleagues [11] found in a case study of calendar systems that having metadata for event information such as the title of the event or arrival time of the speaker, in addition to who the author of the information is, are critical for the interpretation of the events.

        - {{[[DONE]]}} [[@luttersBoundaryObjectsCollaborative2007]]

            - words from chi submission:

                - > Information may lose context over time which can make reusing it difficult. For example, in a field study of collaborative information reuse in aircraft technical support, engineers lamented reusing old records because information was missing, outdated, or not appropriate anymore because of procedural changes. Over the years if any changes to the records were not tagged, the context of those changes were lost.

        - {{[[DONE]]}} [[@andersonDataBaseMent2008]]

            - > However, manually adding context, to support information reuse and retrieval, can be a challenge. Anderson and colleagues [5] explore this in a field study of the information management systems in the healthcare setting. Healthcare workers used a system which featured a drop-down menu that would allow them to select relevant contextual information to attach to a file where they recorded patient interactions. However, the options in the drop-down menu were frequently not specific enough, or too many options were available, which led to the employees choosing the closest option, or a catch all “Not Specified” option. This all led to ambiguous contextual information capture which proved problematic down the line.

        - {{[[DONE]]}} [[@ackermanOrganizationalMemoryObjects2004]]
[[January 18th, 2021]]

- New papers to add to [[Z: Theories are core examples of synthesis]] and [[[[QUE]] - What is synthesis?]] and also [[[[QUE]] - What is context for the purposes of scholarly synthesis?]] #[[➰ breadcrumbs]]

    - [[@oberauerAddressingTheoryCrisis2019]] Title: Addressing the theory crisis in psychology

    - [[@wieserPsychologyCrisisNeed2016]] Title: Psychology's "Crisis" and the Need for Reflection. A Plea for Modesty in Psychological Theorizing

    - [[@scheelWhyHypothesisTesters2020]] Title: Why Hypothesis Testers Should Spend Less Time Testing Hypotheses

        - this one in particular has some really nice reflections on the notion of a [[deductive chain]] that precedes hypothesis testing - a lot of the core activities that help strengthen the deductive chain can be thought of as [[synthesis]]

            - this pairs nicely with [[@vanrooijTheoryTestHow2021]] in specifying heuristics and moves for theory-building, which... I think is a nice fit for [[[[PTN]] - discourse graph]] and [[context queries]]

                - how nice if we had tools and systems (with data!) that allowed us to explore these, in a somewhat analogous way to photoshop (exploring variants), simulations, statistical software, and other tools for thought.

                - what are the tools for synthesis?

                    - as I've said in the past [[[[CLM]] - Effective individual synthesis systems (seem to mostly) exist (for a select few)]]

                        - the critical problem, though is this: because they're so individual, everyone is effectively an island. what if you don't have?

                        - a small exception, again, is sofware for [[systematic review]]s, but again, this doesn't solve our problem, bc so much [[synthesis]] that we want to do goes far beyond [[RCT]]s

                            - plus, they're often, afaict, "use once", not really supporting [[interoperability]] [[@oconnorStillMovingAutomation2019]] and [[reuse]] [[@blakeCollaborativeInformationSynthesis2006a]]
[[February 6th, 2021]]

- note their strong grounding in argument theory, building on [[@toulminUsesArgument2003]], but going beyond it to think more deeply about [[context]], which, like what we [have been saying]([[[[QUE]] - What is context for the purposes of scholarly synthesis?]]), can be roughly divided into "data" (the thing we nest under the `observation note`: a figure/table/quote, etc.), the `context snippets` (typically methodological details that help us understand how the data was produced), and the discourse context of the claim

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F__UTLi_BlT.png?alt=media&token=f4559a05-ab2a-473d-b9a7-5f8d3dfdc48e)
