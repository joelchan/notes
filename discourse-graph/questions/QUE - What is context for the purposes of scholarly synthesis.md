---
title: [[QUE]] - What is context for the purposes of scholarly synthesis?
url: https://roamresearch.com/#/app/megacoglab/page/fFCAtZVW5
author: Joel Chan
date: Thu Jan 07 2021 11:51:02 GMT-0500 (Eastern Standard Time)
---

- #[[Question]]

    - Tags: #[[D/Synthesis Infrastructure]]

    - Description

        - When interpreting and reusing ideas from past work, questions of applicability, validity, and significance are of central concern. New research questions are frequently framed against important "gaps" in these areas. Consider the following example. Suppose an HCI researcher is interested in moderation and online harassment. She is interested specifically in understanding the costs and benefits of strong moderation actions such as banning "bad faith" interlocutors (aka trolls) on the quality of discourse in a politics-focused subreddit. Regarding **applicability**, she needs to judge which studies and findings/theories are "actually applicable" (e.g.,, studying similar populations, interventions, settings, or outcome measures?) to her setting. Should she care that two studies found that temporary (~1 day) bans on bad faith actors had no effect on the degree to which participants used negative sentiments in their posts during the ban? How applicable is a theory of bad faith and discourse proposed by a philosopher from the early 2000's (before the rise of modern social media)? Regarding **validity**, she needs to know what findings have been established with sufficient certainty. Would it matter if the majority of past studies had relied on self-report measures (but no behavioral measures) of discourse quality? What if the bulk of studies with positive effects for bans were coming from authors in a tight interrelated network of researchers who came from the same lab? Finally, regarding **significance**, she needs to judge which findings and concepts have the highest potential for knowledge gain. She might want to avoid studying effects of interventions that had been established across many studies with high precision to have small effects on discourse quality. She might also want to focus on ban intervention-outcome pairs that have mixed effects in past studies. Or she might want to focus on hypotheses that have opposing predictions from multiple prominent theories, or a critical prediction from a single theory.

        - These complex questions require reasoning over far more than the claims from past papers: for example, to judge applicability, details of ^^methodology^^ (participants, setting, measures) and ^^metadata^^ (field of the author/journal, year of publication) are needed. Similarly, for validity, details of methodology are required. Judgments of applicability might also be necessary to reason about the degree of concordance across studies (which studies that have measured "the same thing" came to "the same" conclusions?). Finally, judgments of significance requires complex reasoning over the ^^discourse context^^ assumed by a given study, degree of concordance with other studies, details of results such as effect sizes, and more.

        - Under some conditions, there can be relatively standardized sets of contextual information that will be broadly useful for judgments, such as the [PICO]([[PICO frames]]) (Population, Intervention, Comparator, Outcome) and worksheets like [[Risk of Bias scores]] for precisely focused [[systematic review]]s of [[RCT]]s. The [[org/Oxford Center for Evidence-Based Medicine]] has a [list of useful worksheets](https://www.cebm.ox.ac.uk/resources/ebm-tools/critical-appraisal-tools) for [[critical appraisal]] of various genres of research, including qualitative and case studies. For the latter, more focused checklists like [[Consolidated Criteria for Reporting Qualitative Studies]] ([[@tongConsolidatedCriteriaReporting2007]]) are also useful

        - However, as theories of context and [[reuse]] in CSCW would predict, what specific kinds of contextual information are required for a literature review can often vary substantially across research projects. For instance, [[@blakeCollaborativeInformationSynthesis2006]] discovered in their cognitive work analysis of a team of medical researchers conducting systematic reviews that [the particulars of information needed can vary substantially by the nature of the research question](some types of [[context]] information were more contextual, depending on the particular "hypothesis projection" of the review, which varied across the lifecycle of the project studied (e.g., location of medical condition, amount of exposure, confounding risk factors), while others were more constant regardless of hypothesis (e.g., study- and population-context information)). In interdisciplinary fields such as HCI, the nature of contextual information needed to critically assess and synthesize past work can be found in handbooks like the popular "Ways of Knowing in HCI" book [[Ways of Knowing in HCI]] that includes discussions, and to a lesser extent in research methods texts, where questions of internal and external validity (which map to validity and applicability) are discussed, and the information required to judge these questions, are enumerated.

            - [[[[CL]] - Compression and contextualizability are in tension]]

            - [[[[CLM]] - Specifying context for future reuse requires predicting trajectories of future reuse]], but [[[[CL]] - Predicting trajectories of future reuse of information objects is hard]]

        - We consider the information required to make judgments of applicability, validity, and significance to be [[context]] for the information being reused (i.e., sources and claims) in [literature reviewing]([[synthesis]]).

    - R-Sources

###### Discourse Context

- **Informed By::** [[CLM - Specifying context for future reuse requires predicting trajectories of future reuse]]
- **Informed By::** [[@blakeCollaborativeInformationSynthesis2006]]
- **Informed By::** [[@tongConsolidatedCriteriaReporting2007]]
