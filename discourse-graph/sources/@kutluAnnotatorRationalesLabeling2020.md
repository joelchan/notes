---
title: @kutluAnnotatorRationalesLabeling2020
url: https://roamresearch.com/#/app/megacoglab/page/KvnkmAqV8
author: Joel Chan
date: Fri Sep 25 2020 13:43:43 GMT-0400 (Eastern Daylight Time)
---

- #references

    - Title: Annotator Rationales for Labeling Tasks in Crowdsourcing

    - Meta:

        - Tags: #ref/Paper

        - Authored by::  Mucahid Kutlu ,  Tyler McDonnell ,  [[Matthew Lease]] ,  Tamer Elsayed

        - Year: [[2020]]

        - Publication: Journal of Artificial Intelligence Research

        - URL: https://jair.org/index.php/jair/article/view/12012

        - Citekey: kutluAnnotatorRationalesLabeling2020

    - Content

        - Placeholder

        - Abstract

            - When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon’s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.

###### Discourse Context



###### References

[[September 25th, 2020]]

- new paper from [[Matthew Lease]]'s group showing that / how collecting rationales along with annotations when [[crowdsourcing]] improves a host of things for minimal additional cost [[@kutluAnnotatorRationalesLabeling2020]]

    - When collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. To address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. We evaluate this approach on an information retrieval task in which human judges rate the relevance of Web pages for different search topics. Cost-benefit analysis over 10,000 judgments collected on Amazon’s Mechanical Turk suggests a win-win. Firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. Secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. Consequently, we can realize the above benefits with minimal additional cost.

    - Relevant for the #idea pushing on the [attribute problem](((U1Bb05b-y))) feels like one of the most impactful things to push on, specifically how to efficiently __discover__ these attributes

        - In part because of the idea that the rationales could provide "double supervision"
