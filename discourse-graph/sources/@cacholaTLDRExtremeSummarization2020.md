---
title: @cacholaTLDRExtremeSummarization2020
url: https://roamresearch.com/#/app/megacoglab/page/TAfYNDoZq
author: Joel Chan
date: Sun May 03 2020 22:45:55 GMT-0400 (Eastern Daylight Time)
---

- Metadata::

    - Title: TLDR: Extreme Summarization of Scientific Documents

        - Tags:: #references#ref/Paper

    - Authored by::  Isabel Cachola ,  Kyle Lo ,  Arman Cohan ,  Daniel S. Weld

    - Year: 2020

    - Publication: arXiv:2004.15011 [cs]

    - URL: http://arxiv.org/abs/2004.15011

    - PDF

        - Placeholder
- #lit-context

    - Still a #preprint
- #[[üìù lit-notes]]

    - Key figure:

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FoSZaq6ZuL0.png?alt=media&token=54648dfb-06d8-47ee-8dbe-b9738be9dba7)

    - #appraisal Summarization in this way is **really hard** - best models perform at less than 50% on the special summarization version of F1

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FJQxRSGRQYd.png?alt=media&token=c2c06ee2-4a07-4896-b912-925b789bf7f5)

    - The trick here seems to be partly to leverage an open dataset of diverse TL;DRs written by humans, on openreviews.net - training for the multi-objective instead of single gold standard

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FVCxJUhUKke.png?alt=media&token=7f371604-4081-4be0-8a49-2498defbe2a9)

###### Discourse Context



###### References

[[May 3rd, 2020]]

- Another relevant paper for [[D/Computational Analogy]] (possibly also technical inspiration for [[D/Synthesis Infrastructure]]): [[@cacholaTLDRExtremeSummarization2020]] Title: TLDR: Extreme Summarization of Scientific Documents

    - Another relevant dataset: https://www.shortscience.org/about (already 1506 summaries!) - could use as training data to help train methods of extracting purpose/mechanism (or similar) from papers
