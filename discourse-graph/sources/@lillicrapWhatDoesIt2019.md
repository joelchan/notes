---
title: @lillicrapWhatDoesIt2019
url: https://roamresearch.com/#/app/megacoglab/page/k0jKIsb4a
author: Joel Chan
date: Tue Oct 20 2020 22:37:35 GMT-0400 (Eastern Daylight Time)
---

- #[[references]] for [[P/Material Knowledge of ML for Design]]

    - Title: What does it mean to understand a neural network?

    - Meta:

        - Authored by:: [[Timothy P. Lillicrap]] [[Konrad P. Kording]]

        - Year: [[2019]]

        - Zotero link:: [Zotero Link](zotero://select/items/7_42NFJLCR)

        - Abstract

            - We can define a neural network that can learn to recognize objects in less than 100 lines of code. However, after training, it is characterized by millions of weights that contain the knowledge about many object types across visual scenes. Such networks are thus dramatically easier to understand in terms of the code that makes them than the resulting properties, such as tuning or connections. In analogy, we conjecture that rules for development and learning in brains may be far easier to understand than their resulting properties. The analogy suggests that neuroscience would benefit from a focus on learning and development.

    - #lit-context

        - #preprint

###### Discourse Context



###### References

[[October 20th, 2020]]

- [[@lillicrapWhatDoesIt2019]] makes a version of this point, arguing that there is a limit to how much we can [compress]([[compression]]) an intelligent system of meaningful complexity, not unlike the human brain!

    - striking #example-of this: networks that do well on [[dataset/ImageNet]] cannot really be compressed to fewer than 100k parameters

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fl79QNgnnC_.png?alt=media&token=c47295a7-1382-4fbd-8101-21b4904ba60b)
