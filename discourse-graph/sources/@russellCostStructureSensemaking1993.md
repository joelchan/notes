---
title: @russellCostStructureSensemaking1993
url: https://roamresearch.com/#/app/megacoglab/page/-1zsayc1D
author: Joel Chan
date: Wed Feb 12 2020 17:25:06 GMT-0500 (Eastern Standard Time)
---

- #references

    - Title: The Cost Structure of Sensemaking

    - Meta

        - Tags:: #[[references]] #[[D/Synthesis Infrastructure]] #[[üß± sensemaking]]

        - Authored by:: [[Daniel M. Russell]], [[Mark J. Stefik]], [[Peter Pirolli]], and [[Stuart K. Card]]

        - Year: 1993

        - Publication: [[conf/CHI]]

        - Links:

            - [PDF](https://drive.google.com/file/d/131ppP9hcxLxLy3ZC0cPfKa_XqzpSWQlg/view)

        - Bibtex key: russellCostStructureSensemaking1993

        - Content

            - PDF

                - {{iframe: https://drive.google.com/file/d/1qQCp-xopjVXYWQqd-2y6mIPjn4ZaSdQA/preview}}

    - #lit-context

        - #canonical paper for [[üß± sensemaking]] in [[HCI]] stream

    - #[[üìù lit-notes]]

        - Introduces the [[Learning Loop Complex]] and idea of [[encodon]] as bridge between data and representations ([[Schema]]s)that are then used for downstream tasks during [[üß± sensemaking]] (p. 271)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FLdc6dvd3SX?alt=media&token=daba4a52-0c6f-4ab8-b781-ede3d49e4558)

        - Also analyzes the [[üß± cost structure]] of [[üß± sensemaking]] and uses that to identify the most impactful ways to ease the process of sensemaking (in their case, it was data extraction)

            - They find that, for the cases they study, extracting data is the most costly step for [[üß± sensemaking]] (p.273).

                - This is very similar to the idea of the challenges to sastifying #[[[[CLM]] - Composability is necessary for synthesis]], and evokes the same feeling as what happens in [[systematic review]]s, as [reported](#> The Data Extraction Process seemed particularly demanding to those involved in the corresponding tasks. The Coordinating Editor (P1) described [[systematic review]]ers as being "enslaved to the trapped data", with reviewers "chiseling the mine" to get at the data they needed. (p.208) #synthesis) by [[@knightEnslavedTrappedData2019]]

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FEn4rDIQNGn.png?alt=media&token=d5e18cdb-710c-490d-8138-7b086de05026)

                - cf. #[[[[CLM]] - Gracefully integrating formalism into interactive systems is hard]]

                - also #[[[[CLM]] - Specifying context for future reuse is costly]]

###### Discourse Context

- **Informs::** [[CLM - Sensemaking requires iterative loops of (re)interpreting data in light of evolving schemas]]
- **Informs::** [[CLM - Composability is necessary for synthesis]]
- **Informs::** [[QUE - What (existing) systems facilitate individual synthesis]]

###### References

[[January 26th, 2021]]

- Significant work across formal conceptual models of scholarly argumentation [[@deWaardProteinsFairytalesDirections2010]], [[@clarkMicropublicationsSemanticModel2014]], sensemaking [[@russellCostStructureSensemaking1993]], and knowledge management [[@ackermanSharingKnowledgeExpertise2013]] suggest that an optimal information model for supporting synthesis is the epistemic model: shareable collections of knowledge __claims__ (e.g., __children are less susceptible to COVID-19 infection, given equivalent exposure__) and their associated __context__ (e.g., \textit{was sampling symptom-based? Where was the study conducted? At what point in the locality's epidemic time course?}), \textit{connected} in a discourse graph (e.g., supporting other hypotheses/claims, corroborated/disputed by other claims).

    - Key hypothesized affordances of [[[[PTN]] - discourse graph]] include the ability to compress complex ideas into granular statements, which affords quicker comparison, comprehension, and composition of ideas, while also retaining links to contextual details to allow for "unpacking" and critical interrogation of ideas as they are interpreted and recombined into more complex arguments/models.

    - These affordances are hypothesized to enable key process-level interactions with scholarly knowledge that are crucial for synthesis, such as a principled dialectic between context-rich observations (data) and creative and formal construction of models and hypotheses (theory).

    - These affordances may also enable more effective reuse and remixing of ideas across boundaries of time, projects, and even collaborators.
[[January 27th, 2021]]

- Significant work across formal conceptual models of scholarly argumentation [[@deWaardProteinsFairytalesDirections2010]], [[@clarkMicropublicationsSemanticModel2014]], sensemaking [[@russellCostStructureSensemaking1993]], and knowledge management [[@ackermanSharingKnowledgeExpertise2013]] suggest that an optimal information model for supporting synthesis is a collection of knowledge **claims** and their associated __context__, __connected__ in a discourse graph.

    - The answers to these questions cannot be found simply in the titles of research papers, in groupings of papers by area, or even in citation or authorship networks (effective though they might be at revealing macro structures in scientific communities). These information structures are not at the desired level of granularity, which is at the level of theoretical concepts/claims and empirical **claims** and evidence: for example, "__game theory predicts that discourse can devolve to an escalating tit-for-tat spiral under X conditions__", or "__banning bad actors from a subreddit in 2012 was somewhat effective at mitigating spread of misinformation on the subreddit__". This level of granularity is crucial not just for finding relevant claims to inform the synthesis, but also for constructing more complex arguments and theories (by connecting statements in logical and discursive relationships).

    - To understand why this information model is hypothesized to augment synthesis, let us return to our motivating example of the researcher who wants to design new interventions to mitigate online harassment. To synthesize a formulation of this problem that can advance the state of the art, she needs material that can help her work through detailed answers to a range of granular questions. For example, what theories of online behavior and antisocial behavior might be most relevant for understanding what is going on here? Which theories have the most empirical support in this particular setting? Are there conflicting theoretical predictions that might signal fruitful areas of inquiry? What are the key phenomena to keep in mind when designing an intervention (e.g., perceptions of human vs. automated action, procedural considerations, noise in judgments of wrongdoing, scale considerations for spread of harm)? What intervention patterns have been proposed that are both a) judged on theoretical and circumstantial grounds as likely to be effective in this setting, and b) lacking in direct evidence for efficacy?

    - Beyond operating at the claim level, however, our researcher will need to work through a range of **contextual details**. For example, to judge which studies and findings/theories are "actually applicable" (e.g.,, studying similar populations, interventions, settings, or outcome measures?) to her setting, she might need to reason over the fact that two studies that concluded limited efficacy of bans had ban interventions that were quite short, on a forum with no identity verification. Or she might reason through the fact that a prominent theory of bad faith and discourse was proposed by a philosopher from the early 2000's (before the rise of modern social media). To judge the validity of past findings (e.g., what has been established with sufficient certainty, where the frontier might be), she would need to know, for example, which findings came from which measures (e.g., self-report, behavioral measures), and the extent to which findings have been replicated cross authors from different labs, and across a variety of settings.

    - Also, contextual details, such as methodology or metadata, are explicitly included in the discourse graph. This should support direct analysis of claims with their evidentiary context. Figure X shows how this might be supported in the specific worked example above.

    - Beyond these direct hypothesized benefits to synthesis, such models could also be used and repurposed over time, across projects, and potentially even across people. For example, imagine collaborators sharing these collections of epistemic models with each other, to speed up the process of working towards shared mental models and identify productive areas of divergence; or PIs and senior students onboarding newer students not with long reading lists, but subsets of epistemic models that they can build on and add to over time. How much time and overhead could be saved if this were a reality?

    - An epistemic model has key affordances that are hypothesized to enable just these sorts of operations. Information is represented primarily at the claim/statement level, and are embedded in a discourse graph. Thus, claims should be able to have many-to-many relationships to support composition of more complex arguments and theories, or "decompression" into component supporting/opposing claims.
[[May 28th, 2020]]

- Although efforts like #[[@russellBeingLiterateLarge2006]] and #[[@russellCostStructureSensemaking1993]] and #[[@pirolliSensemakingProcessLeverage2005]] seem to target more **__process__**-level frictions, and not as much the data model / artifacts themselves.

    - Slight exception from #[[@russellBeingLiterateLarge2006]]: Good example on p5: if "moving" a paper is costly, people do it a lot less
[[#AnnotatedBib for Synthesis Infrastructures]]

- [[@russellCostStructureSensemaking1993]]

    - Introduces the [[Learning Loop Complex]] and idea of [[encodon]] as bridge between data and representations ([[Schema]]s)that are then used for downstream tasks during [[üß± sensemaking]] (p. 271)
[[Z Sensemaking models partially model scholarly synthesis as of April 21st, 2020]]

- [[R: Sensemaker]] and [[R: SketchTrieve]] apply the sensemaking model from [[@russellCostStructureSensemaking1993]] to build tools **that support literature foraging (information seeking), specifically support users to construct a representation.** Different from synthesis, literature foraging is one of the few initial steps for literature review.

    - [[R: baldonadoSenseMakerInformationexplorationInterface1997]] support users to construct a representation for the current information context and user interest, **whose end goal is discovering new information**. The new information is not necessarily gap-identifying, creative and academic. It could be everyday information, vocational, for personal growth, e.g. developing an IS  in Java language. User actions are limited to manipulating a document collection, instead of creating something new from it.

        - URL: http://ilpubs.stanford.edu:8090/217/1/1996-85.pdf

        - Title: SenseMaker: An Information-Exploration Interface Supporting the Contextual Evolution of a User‚Äôs Interests

        - Has a good scenario for our ASIST paper though

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FsWIpiUgoLz?alt=media&token=a3281344-d8cc-4cfb-8b55-c7d83fd046f5)

    - [[R: SketchTrieve]]

        - Title: An informal information‚Äêseeking environment

        - URL: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.507.8491&rep=rep1&type=pdf

        - It also focuses on **information-seeking** and **search without the determinacy**

        - However, this paper lists inspirational design considerations that are useful to cite: side-by-side visibility
[[October 27th, 2021]]

- inspired by sensemaking measures of loops between "schema level" and "data level", e.g., [[Learning Loop Complex]] [[@russellCostStructureSensemaking1993]] and protocol studies like [[@zhangpengyiComprehensiveModelCognitive2014]]

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FLdc6dvd3SX?alt=media&token=daba4a52-0c6f-4ab8-b781-ede3d49e4558)
