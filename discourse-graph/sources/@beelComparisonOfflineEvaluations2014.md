---
title: @beelComparisonOfflineEvaluations2014
url: https://roamresearch.com/#/app/megacoglab/page/yZrDWAhw7
author: Joel Chan
date: Wed Jan 13 2021 12:31:42 GMT-0500 (Eastern Standard Time)
---



###### Discourse Context



###### References

[[January 13th, 2021]]

- [[@beelComparisonOfflineEvaluations2014]]

    - #[[references]]

        - Title: A comparison of Offline Evaluations, Online Evaluations, and User Studies

        - Meta:

            - Authored by:: [[Joeran Beel]] [[Stefan Langer]]

            - Year: [[2014]]

            - Publication: undefined

            - Zotero link: [Zotero Link](zotero://select/items/1_FGCM9B8R)

            - URL: [Beel & Langer (2014). A comparison of Offline Evaluations, Online Evaluations, and User Studies. undefined](undefined)

        - Content

            - Abstract

                - The evaluation of recommender systems is key to the successful application of recommender systems in

                - practice. To evaluate recommender systems, there are three evaluation methods: offline evaluations, online

                - evaluations, and user studies. In this paper, we examine and discuss the appropriateness of the three

                - methods. Among others, we use the three methods to evaluate a number of recommendations approaches in

                - the field of research paper recommendations. The evaluations show that results from offline evaluations

                - sometimes contradict results from online evaluations and user studies. We discuss potential reasons for this,

                - namely the impact of human factors, the potential inherent value of offline evaluations, and the imperfection

                - of offline-datasets, and we conclude that offline evaluations probably are not suitable to evaluate

                - recommender systems, particularly in the domain of research paper recommender systems. We further

                - analyze and discuss the appropriateness of some online evaluation metrics such as click-through rate, link-

                - through rate, and cite-through rate. We conclude that click-through is the most favorable metric, at least in

                - our research, since metrics such as cite-through rate did correlate less well with user satisfaction than click-

                - through rate.
