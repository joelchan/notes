---
title: @dasguptaMemoryComputationalResource2021
url: https://roamresearch.com/#/app/megacoglab/page/joq3kbwLD
author: Joel Chan
date: Thu Jan 14 2021 11:51:28 GMT-0500 (Eastern Standard Time)
---

- #[[references]]

    - Title: Memory as a Computational Resource

    - Meta:

        - Authored by:: [[Ishita Dasgupta]] [[Samuel J. Gershman]]

        - Year: [[2021]]

        - Publication: Trends in Cognitive Sciences

        - Zotero link: [Zotero Link](zotero://select/items/1_SBN25NC6)

        - URL: [Dasgupta & Gershman (2021). Memory as a Computational Resource. Trends in Cognitive Sciences](http://www.sciencedirect.com/science/article/pii/S1364661320303053)

    - Content

        - Abstract

            - Computer scientists have long recognized that naive implementations of algorithms often result in a paralyzing degree of redundant computation. More sophisticated implementations harness the power of memory by storing computational results and reusing them later. We review the application of these ideas to cognitive science, in four case studies (mental arithmetic, mental imagery, planning, and probabilistic inference). Despite their superficial differences, these cognitive processes share a common reliance on memory that enables efficient computation.

###### Discourse Context



###### References

[[January 14th, 2021]]

- There is another angle on scale now that I'm thinking about, sparked by this paper on "memoization" as a strategy for more efficient computation [[@dasguptaMemoryComputationalResource2021]] Title: Memory as a Computational Resource

    - Why might efficiency matter? I think there is something to the fact that the human brain operates on temporal dynamics (rising and falling of spikes, oscillation, synchronization), such that it might really matter whether or not we can get ideas "on the table" at the same timescale

        - Something about [[Hebbian learning]] here, [[cells that fire together, wire together]]

            - And back to [[[[CL]] - Complex compositionality can arise from surprisingly simple units and connection dynamics]]

                - This could be a reason to be bullish on the answer "yes" to [[[[QUE]] - Can deep learning discover analogical representations?]], to the extent that connectivity could give rise to meaningful forms of [[compositionality]]

    - This is separate from ability to iterate: faster cycles = more iteration = more learning, better ideas
