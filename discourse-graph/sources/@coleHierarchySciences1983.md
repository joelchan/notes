---
title: @coleHierarchySciences1983
url: https://roamresearch.com/#/app/megacoglab/page/efSuMAHEy
author: Joel Chan
date: Fri Sep 10 2021 21:50:18 GMT-0400 (Eastern Daylight Time)
---

- #[[references]]

    - Title: The Hierarchy of the Sciences?

    - Meta:

        - Authored by:: [[Stephen Cole]]

        - Year: [[1983]]

        - Publication: American Journal of Sociology

        - Zotero link: [Zotero Link](zotero://select/items/7_DLS6WTUC)

        - URL: [Cole (1983). The Hierarchy of the Sciences?. American Journal of Sociology](https://www.jstor.org/stable/2779049)

    - Content

        - Abstract

            - For 200 years it has been assumed that the sciences are arranged in a hierarchy, with developed natural sciences like physics at the top and social sciences like sociology at the bottom. Sciences at the top of the hierarchy presumably display higher levels of consensus and more rapid rates of advancement than those at the bottom. A distinction is made between two classes of knowledge: the core, or fully evaluated and universally accepted ideas which serve as the starting points for graduate education, and the research frontier, or all research currently being conducted. Data are presented from a set of empirical studies which show that, at the top and at the bottom of the hierarchy in either cognitive consensus or the rate at which new ideas are incorporated. It is concluded that in all sciences knowledge at the research frontier is a loosely woven web characterized by substantial levels of disagreement and difficulty in determining which contributions will turn out to be significant. Even at the research frontier, however, minimal levels of consensus are a necessary condition for the accumulation of knowledge. Consensus in all sciences is maintained by sociological processes such as the evaluation and reward systems.

###### Discourse Context

- **Informs::** [[CLM - Scientific fields vary substantially in terms of ease of consensus on judging what constitutes a valuable new contribution]]
- **Informs::** [[QUE - (How) do fields of innovation vary in their legibility of quality]]
- **Informs::** [[QUE - How can we measure quality legibility of a field]]
- **SourceFor::** [[EVD - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - @coleHierarchySciences1983]]
- **SourceFor::** [[EVD - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - @coleHierarchySciences1983]]

###### References

[[September 10th, 2021]]

- [[@coleHierarchySciences1983]] for [[[[QUE]] - (How) do fields of innovation vary in their legibility of quality?]] |  review of empirical studies that test whether fields assumed to be in hierarchy vary in terms of cognitive consensus, by various measures; failed to find differences at the frontier (e.g., grant reviews), but found some potentially strong differences in the core (proxy of textbooks)

    - intellectual lineage

        - [[hierarchy of the sciences]] idea from [[Auguste Comte]] in history of sciences

        - [[Thomas Kuhn]] paradigms - some are "preparadigmatic", so less consensus about shared theoretical structures and methodological approaches

        - availability of shared and mature theory is a proxy for legibility, probably, because it can provide a shared framework against which to judge contributions

        - here, these authors talk about "cognitive consensus"

        - distinction between "core and "frontier"

            - maybe there is a sense of the "size" and stability of the core? by proxy, level of agreement (over time) across syllabi?

                - cc [[[[QUE]] - How can we measure quality legibility of a field?]]

            - seems related, but not identical to the idea of [[Core and Scatter]]

        - summary of different axes: development of theory, quantification, cognitive consensus, predictability, obsolescence, rate of growth

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1BbLOXLK8j.png?alt=media&token=af5d5ab3-926e-4d81-be77-3c55e3cca183) (p. 113)

    - other thoughts

    - second half follows distinctino between core and frontier, and is an attempt to explain this null finding; hypothesize that the data so far was at the frontier; what about the core? this is the primary study here

        - # methods notes

            - sample of 8 undergraduate textbooks in chemistry, physics, and sociology

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FG0CAw_-Dzs.png?alt=media&token=c7b2090d-ee08-466d-9bdf-a1b9ff76e20f) (p. 133)

            - measure: age distribution of references

            - measure: total number of references

        - # results

            - [[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]

            - [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]

                - take this and [previous result]([[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]) with a big grain of salt! no justification for textbook selection, and it's basically ~2-3 texts per discipline; they look like intro texts

        - conclusion from this is: [[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]

    - core of first half of paper is review of a bunch of prior studies, which basically provide no/negative evidence that there's systematic variation in consensus across fields that are thought to be at different levels of the [[hierarchy of the sciences]]: [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]

        - see Zuckerman and Merton idea of codification

            - they come right out and say that legibility/consensus is a potential indicator of degree of codification

            - lower rejection rates as a proxy. huh. that's... not clean.

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FiDdkKglTtx.png?alt=media&token=d80e3283-0072-4b6b-81ad-f7653b23c9df)

                - cc [[[[QUE]] - How can we measure quality legibility of a field?]]

        - rubin 1975 - chemists blame themselves, sociologists dispute validity of criteria - proxy for paradigm development and consensus?

        - hargens 1975 idea of normative integration (maps to cognitive consensus here) - political science < chemistry and math

        - cole 1979 -

            - no significant differences in relationship between citations and full professor promotion decisions

            - no significant differences in high citation performers x age (no more young high performers in "high consensus" fields)

                - this makes me think of [[@jonesAgeGreatInvention2010]] - IIRC they had different results? or maybe it was just increasing over time, but no big differences across fields? or maybe it was one of [[Dean Simonton]]'s papers?

            - note, though: i think the hierarchy was taken as given, not measured directly.

        - cole, cole and dietrich 1978 -

            - ask to eval peers and "greatest scientists" - small and barely statistically significant differences in expected pattern

                - this makes me think if there's any published data on IRR for peer review; taht could be proxy for legibility - maybe average time to publication? publication history? rate of "sleeping beauties"?

            - another proxy that i hate: distribution of citations (more concentration = higher consensus). i gues, like it or not, if there is high concentration, then there is high consensus in a sense. even if htat consensus is off base. here they used [[Gini coefficient]]

                - both cc [[[[QUE]] - How can we measure quality legibility of a field?]]

        - pilot study in this paper: does citation obsolescence vary across english lit vs. science?

            - control for growth rate of field (number of pubs), which Zuckerman and Merton and Price do not

        - cole and cole 1980; cole and cole 1981; cole, rubin and cole 1978 - no evidence of systematic variation across fields in terms of IRR (roughly) for grant peer review

        - small 1975 - weak correlation between reviewer IRR and eventual citation rate

        - carter 1974 - NIH peer review, week correlation between priority scores and later citations of work (r = .40; hmmm actually not that weak; real signal, seems like, but far from perfectly predictive)

        - cole and cole 1973 -

            - similar to [[@morganPrestigeDrivesEpistemic2018]] - if equal quant / citations, person from higher prestige institution rated more highly than person from lower prestige institution

                - cc [[[[CLM]] - Prestige substantially controls how scientific ideas spread]]

        - these are the ones to dig into that have the actual evidence #[[➰ breadcrumbs]]

    - ratio of core to frontier is proxy for "cognitive consensus" hierarchy?
[[QUE - (How) do fields of innovation vary in their legibility of quality]]

- [[@coleHierarchySciences1983]] basically clarifies and sharpens this claim because they don't find any evidence for this, and instead have reviewed a bunch of papers and conclude the [opposite]([[Opposed By]]) in terms of the research frontier:

    - [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]
[[September 10th, 2021]]

- [[@coleHierarchySciences1983]] [[@coleHierarchySciences1983]] argues that basically frontier-level measures (citation concentration, obsolescence, peer review IRR, peer evals) don't give us much signal, but ["core knowledge" measures (e.g., textbooks) probably will]([[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]).

    - I think a concrete idea might be to use data from [[OpenSyllabus.org]] to sort of replicate the measures of age distribution of references and total number of references

    - #[[➰ breadcrumbs]] need to follow up on this though. this was work done in the early days; follow citation trails forward.

        - frustrating because the paper has like ~600 citations on Google Scholar, but only 2 "meaningful" ones on [Scite](https://scite.ai/reports/the-hierarchy-of-the-sciences-XpQYXw?contradicting=false&mentioning=false&page=1&unclassified=false&utm_campaign=badge&utm_medium=badge&utm_source=scholar.google.com), both supportive, but not really granular, and not really true to the claims of Cole :/

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FOtCoGzA6uN.png?alt=media&token=6a772c92-4c07-4bbd-8bf7-61d35daf4f98)
[[September 22nd, 2021]]

- The [thread on legibility](core of first half of paper is review of a bunch of prior studies, which basically provide no/negative evidence that there's systematic variation in consensus across fields that are thought to be at different levels of the [[hierarchy of the sciences]]: [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]) from [[@coleHierarchySciences1983]] seems fundamental. Many threads run through the question of how to assess ideas.

    - And it connects to the synthesis stuff insofar as legibility is helped by (effective, malleable vs. calcified, shallow) synthesis

    - Project on problem formulation (and how to properly specify an objective function for a possible language model)

    - Also the stuff with Matt.

    - And of course AJ's stuff with bridging analogies (which has an idea of ranking).
[[September 17th, 2021]]

- same hypothesized ordering here as in [[@coleHierarchySciences1983]] and [[@simontonVarietiesScientificCreativity2009]], from physics to biology to social science

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjWysvSGtjC.png?alt=media&token=167979b3-c1bc-46ac-9b1a-bea2418ff71f) (p. 2)
[[QUE - (How) do fields of innovation vary in their legibility of quality]]

- But [[@coleHierarchySciences1983]] also find some limited/scoped evidence in [support]([[SupportedBy]]):

    - [[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]

    - [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]
[[QUE - How can we measure quality legibility of a field]]

- [[@coleHierarchySciences1983]]

    - bunch more from: core of first half of paper is review of a bunch of prior studies, which basically provide no/negative evidence that there's systematic variation in consensus across fields that are thought to be at different levels of the [[hierarchy of the sciences]]: [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]
[[September 10th, 2021]]

- [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]

    - take this and [previous result]([[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]) with a big grain of salt! no justification for textbook selection, and it's basically ~2-3 texts per discipline; they look like intro texts
[[QUE - (How) do fields of innovation vary in their legibility of quality]]

- [[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]

    - And this is [[SupportedBy]]

        - [[[[EVD]] - undergrad 1980s textbooks in physics and chemistry cited far fewer texts compare to sociology textbooks - [[@coleHierarchySciences1983]]]]

        - [[[[EVD]] - most undergrad 1980s textbooks in physics and chemistry cited work published before 1960; sociology texts mostly cited work published after 1960 - [[@coleHierarchySciences1983]]]]

    - This is a more qualified version of the [general positive claim]([[[[CLM]] - Scientific fields vary substantially in terms of ease of consensus on judging what constitutes a valuable new contribution]])
[[QUE - How can we measure quality legibility of a field]]

- [[@coleHierarchySciences1983]] argues that basically frontier-level measures (citation concentration, obsolescence, peer review IRR, peer evals) don't give us much signal, but ["core knowledge" measures (e.g., textbooks) probably will]([[[[CLM]] - There is systematic variation in consensus of the core knowledge of fields - [[@coleHierarchySciences1983]]]]).

    - Really need to trace this forward, see whether there are any better signals today with better methods. To their credit, Cole et al were quite upfront about methodological limitations.

        - One pointer here is [[@fanelliBibliometricEvidenceHierarchy2013]]
[[September 10th, 2021]]

- core of first half of paper is review of a bunch of prior studies, which basically provide no/negative evidence that there's systematic variation in consensus across fields that are thought to be at different levels of the [[hierarchy of the sciences]]: [[[[CLM]] - There is no systematic variation in consensus across fields on judging quality of contributions to research frontiers - [[@coleHierarchySciences1983]]]]

    - see Zuckerman and Merton idea of codification

        - they come right out and say that legibility/consensus is a potential indicator of degree of codification

        - lower rejection rates as a proxy. huh. that's... not clean.

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FiDdkKglTtx.png?alt=media&token=d80e3283-0072-4b6b-81ad-f7653b23c9df)

            - cc [[[[QUE]] - How can we measure quality legibility of a field?]]

    - rubin 1975 - chemists blame themselves, sociologists dispute validity of criteria - proxy for paradigm development and consensus?

    - hargens 1975 idea of normative integration (maps to cognitive consensus here) - political science < chemistry and math

    - cole 1979 -

        - no significant differences in relationship between citations and full professor promotion decisions

        - no significant differences in high citation performers x age (no more young high performers in "high consensus" fields)

            - this makes me think of [[@jonesAgeGreatInvention2010]] - IIRC they had different results? or maybe it was just increasing over time, but no big differences across fields? or maybe it was one of [[Dean Simonton]]'s papers?

        - note, though: i think the hierarchy was taken as given, not measured directly.

    - cole, cole and dietrich 1978 -

        - ask to eval peers and "greatest scientists" - small and barely statistically significant differences in expected pattern

            - this makes me think if there's any published data on IRR for peer review; taht could be proxy for legibility - maybe average time to publication? publication history? rate of "sleeping beauties"?

        - another proxy that i hate: distribution of citations (more concentration = higher consensus). i gues, like it or not, if there is high concentration, then there is high consensus in a sense. even if htat consensus is off base. here they used [[Gini coefficient]]

            - both cc [[[[QUE]] - How can we measure quality legibility of a field?]]

    - pilot study in this paper: does citation obsolescence vary across english lit vs. science?

        - control for growth rate of field (number of pubs), which Zuckerman and Merton and Price do not

    - cole and cole 1980; cole and cole 1981; cole, rubin and cole 1978 - no evidence of systematic variation across fields in terms of IRR (roughly) for grant peer review

    - small 1975 - weak correlation between reviewer IRR and eventual citation rate

    - carter 1974 - NIH peer review, week correlation between priority scores and later citations of work (r = .40; hmmm actually not that weak; real signal, seems like, but far from perfectly predictive)

    - cole and cole 1973 -

        - similar to [[@morganPrestigeDrivesEpistemic2018]] - if equal quant / citations, person from higher prestige institution rated more highly than person from lower prestige institution

            - cc [[[[CLM]] - Prestige substantially controls how scientific ideas spread]]

    - these are the ones to dig into that have the actual evidence #[[➰ breadcrumbs]]
