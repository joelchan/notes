---
title: @leeExplanationBasedTuningOpaque2020
url: https://roamresearch.com/#/app/megacoglab/page/dxQSnPvld
author: Joel Chan
date: Wed Jan 13 2021 18:06:22 GMT-0500 (Eastern Standard Time)
---

- #[[references]]

    - Title: Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation

    - Meta:

        - Authored by:: [[Benjamin Charles Germain Lee]] [[Kyle Lo]] [[Doug Downey]] [[Daniel S. Weld]]

        - Year: [[2020]]

        - Publication: arXiv:2003.04315 [cs, stat]

        - Zotero link: [Zotero Link](zotero://select/items/1_3GT27MC2)

        - URL: [Lee et al. (2020). Explanation-Based Tuning of Opaque Machine Learners with Application to Paper Recommendation. arXiv:2003.04315 [cs, stat]](http://arxiv.org/abs/2003.04315)

    - Content

        - Abstract

            - Research in human-centered AI has shown the benefits of machine-learning systems that can explain their predictions. Methods that allow users to tune a model in response to the explanations are similarly useful. While both capabilities are well-developed for transparent learning models (e.g., linear models and GA2Ms), and recent techniques (e.g., LIME and SHAP) can generate explanations for opaque models, no method currently exists for tuning of opaque models in response to explanations. This paper introduces LIMEADE, a general framework for tuning an arbitrary machine learning model based on an explanation of the model's prediction. We apply our framework to Semantic Sanity, a neural recommender system for scientific papers, and report on a detailed user study, showing that our framework leads to significantly higher perceived user control, trust, and satisfaction.

###### Discourse Context


