---
title: @booteScholarsResearchersCentrality2005
url: https://roamresearch.com/#/app/megacoglab/page/lTiVFRHm-
author: Joel Chan
date: Wed Jan 29 2020 09:55:32 GMT-0500 (Eastern Standard Time)
---

- Title:: Scholars Before Researchers: On the Centrality of the Dissertation Literature Review in Research Preparation
- Author(s):: [[David N. Boote]], [[Penny Beile]]
- Type:: [[Article]]
- Publication:: Educational Researcher
- URL : http://journals.sagepub.com/doi/10.3102/0013189X034006003
- Date Added:: [[July 31st, 2021]]
- Zotero links:: [Local library](zotero://select/groups/2451508/items/A5R5AQPB), [Local library](https://www.zotero.org/groups/2451508/items/A5R5AQPB)
- PDF links : [Boote_Beile_2005_Scholars Before Researchers.pdf](zotero://open-pdf/groups/2451508/items/PDDGREPL)
- #[[references]]

    - Title: Scholars Before Researchers: On the Centrality of the Dissertation Literature Review in Research Preparation

    - Meta:

        - Tags: #[[references]] #[[D/Synthesis Infrastructure]]

        - Authored by:: [[David N. Boote]] [[Penny Beile]]

        - Year: [[2005]]

        - Publication: Educational Researcher

        - Zotero link: [Zotero Link](zotero://select/items/1_D4PXQ9UD)

        - URL: [Boote & Beile (2005). Scholars Before Researchers: On the Centrality of the Dissertation Literature Review in Research Preparation. Educational Researcher](http://journals.sagepub.com/doi/10.3102/0013189X034006003)

    - Content

        - Abstract

            - undefined

    - #lit-context

    - #[[üìù lit-notes]]

        - the level of synthesis in regular manuscripts frequently subpar (p.4)

            - #ClaimSecondary

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNbJN82GKeI.png?alt=media&token=94bb3cf6-b064-4fe9-a64b-7b03468bd692)

            - cites #[[@alton-leeTroubleshooterChecklistProspective1998]], who found that, [[[[EVD]] - approximately one-third of criticisms in reviews of submitted manuscripts for a teacher education journal were directly related to inadequacies in synthesis - [[@alton-leeTroubleshooterChecklistProspective1998]]]]

        - Developed a rubric for scoring literature reviews, including a category for [[synthesis]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyPK1uMfJyb?alt=media&token=4a2ebef2-85f1-48e7-b5a2-8a5ad89d5c4d)

###### Discourse Context

- **Informs::** [[CLM - Effective synthesis is rare]]
- **Informs::** [[QUE - What is synthesis]]
- **Informs::** [[QUE - How might we conceptualize and measure synthesis quality]]

###### References

[[P Synthesis requirements theory paper]]

- For example, Boote and Beile [[@booteScholarsResearchersCentrality2005]]

    - Developed a rubric for scoring literature reviews, including a category for [[synthesis]]
[[@alton-leeTroubleshooterChecklistProspective1998]]

- as cited in #[[@booteScholarsResearchersCentrality2005]]

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNbJN82GKeI.png?alt=media&token=94bb3cf6-b064-4fe9-a64b-7b03468bd692)

    - a bit of a chicken-and-egg here maybe.. if [[[[CLM]] - Effective synthesis is hard]] for everyone, then reviewers may not have adequate expertise to judge whether a [[synthesis]] is adequate. Probably doesn't change the numbers here that much though. and also remember that these aren't necessarily [[interdisciplinarity]] papers.
[[May 1st, 2020]]

- Resurfacing [[@booteScholarsResearchersCentrality2005]], which actually includes an intriguing data point - the level of synthesis in regular manuscripts frequently subpar (p.4) not sure how this answers my original question though: this gives p $$p(synthesisissue | issue)$$, not $$p (synthesisissue | paper)$$. Also, need to look into it: usually people give suggestions anyway, even if hte paper is overall ok. We want to know about the proportion of times a lack of synthesis happens that is serious enough that it undermines. Maybe $$p(reject|synthesisissue)$$ will give us that?

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FkvKYAl6XHt?alt=media&token=7ba5b416-a000-4bf1-9064-9d7f0316549e)

    - The citation tree for this turns out to be a goldmine. Some samplings:

        - [[R: alaviReviewKnowledgeManagement2001]] (cited by [[@roweWhatLiteratureReview2014]])

        - Distinguishing literature reviews from [[Synthesis Systems]]s, with a somewhat derogatory summary of "non-systematic reviews" [[R: robinsonLiteratureReviewsVs2015]] - could be an interesting explanation for why [[Z: Publishing review papers is hard]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FGKCQOPm_dK.png?alt=media&token=8e076a88-c840-42a8-8a68-8e1cc3384038)

    - In general, we really need to ground the whole [[D/Synthesis Infrastructure]] work in the [[MIS]] [[Knowledge Management]] world: it is a close cousin of what we're doing, and lots of the insights we are building on were in the context of these organizational [[Knowledge Management]] systems (cf. [[@ackermanSharingKnowledgeExpertise2013]])

    - Important paper here: [[@levySystemsApproachConduct2006]] (I think same person [[Timothy J. Ellis]] who did [[@ellisFrameworkProblemBasedResearch2008]])

        - #appraisal Has some really nice #example-of the __outputs__ of [[synthesis]], but I wish there was more about the __process__! HOW do you get to those outputs? Think harder? Work longer?
[[June 29th, 2021]]

- [[@booteScholarsResearchersCentrality2005]] has a rubric

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyPK1uMfJyb?alt=media&token=4a2ebef2-85f1-48e7-b5a2-8a5ad89d5c4d)
[[QUE - How might we conceptualize and measure synthesis quality]]

- In both cases, we need a firm conceptualization of synthesis quality. After previous work on synthesis in scholarly work [[@strikeTypesSynthesisTheir1983]][[@booteScholarsResearchersCentrality2005]], we can conceptualize synthesis quality along two major axes:

    - 2) The resulting synthesis contributes a **new, useful conceptual whole** that is not clearly seen in the individual or aggregated source materials. Examples of this might be a new understanding of a key concept that clarifies connections and differences across disciplinary boundaries; or a novel mechanism (and associated hypotheses to test) that might account for contradictory observations in a body of findings; or a novel relationship to examine between concepts not previously thought to be closely related; or a novel theory/model of a phenomenon.

    - 1) The resulting synthesis includes **critical engagement** with/across sources. Sources are not taken at face value, but are instead weighed and put in conversation with each other, bringing out key points of strength and weakness in individual studies, and articulating points of conflict and convergence across sources. Oten this entails articulating commonalities/differences in contextual details across sources that may not be commonly discussed. Examples of this might be an articulation of a key methodological gap in a body of evidence; or discussing how a few critical studies might outweigh a larger collection of weaker evidence, based on differences in internal and/or external validity; or identifying a possible mediator/moderator for a causal pathway based on systematic variations in effects across a contextual dimension.
[[CLM - Effective synthesis is rare]]

- #[[@booteScholarsResearchersCentrality2005]]

    - the level of synthesis in regular manuscripts frequently subpar (p.4)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNbJN82GKeI.png?alt=media&token=94bb3cf6-b064-4fe9-a64b-7b03468bd692)
