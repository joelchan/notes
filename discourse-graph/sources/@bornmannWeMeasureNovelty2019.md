---
title: @bornmannWeMeasureNovelty2019
url: https://roamresearch.com/#/app/megacoglab/page/JHob-j88v
author: Joel Chan
date: Mon Jul 19 2021 11:22:53 GMT-0400 (Eastern Daylight Time)
---

- #[[references]]

    - Title: Do we measure novelty when we analyze unusual combinations of cited references? A validation study of bibliometric novelty indicators based on F1000Prime data

    - Meta:

        - Authored by:: [[Lutz Bornmann]] [[Alexander Tekles]] [[Helena H. Zhang]] [[Fred Y. Ye]]

        - Year: [[2019]]

        - Publication: Journal of Informetrics

        - Zotero link: [Zotero Link](zotero://select/items/7_SMQPJDN2)

        - URL: [Bornmann et al. (2019). Do we measure novelty when we analyze unusual combinations of cited references? A validation study of bibliometric novelty indicators based on F1000Prime data. Journal of Informetrics](https://www.sciencedirect.com/science/article/pii/S1751157718304371)

    - Content

        - Abstract

            - Lee et al. (2015) – based on Uzzi et al. (2013) – and Wang et al. (2017) proposed scores based on cited references (cited journals) data which can be used to measure the novelty of papers (named as novelty scores U and W in this study). Although previous research has used novelty scores in various empirical analyses, no study has been published up to now – to the best of our knowledge – which quantitatively tested the convergent validity of novelty scores: do these scores measure what they propose to measure? Using novelty assessments by faculty members (FMs) at F1000Prime for comparison, we tested the convergent validity of the two novelty scores (U and W). FMs’ assessments do not only refer to the quality of biomedical papers, but also to their characteristics (by assigning certain tags to the papers): for example, are the presented findings or formulated hypotheses novel (tags “new findings” and “hypothesis”)? We used these and other tags to investigate the convergent validity of both novelty scores. Our study reveals different results for the novelty scores: the results for novelty score U are mostly in agreement with previously formulated expectations. We found, for instance, that for a standard deviation (one unit) increase in novelty score U, the expected number of assignments of the “new finding” tag increase by 7.47%. The results for novelty score W, however, do not reflect convergent validity with the FMs’ assessments: only the results for some tags are in agreement with the expectations. Thus, we propose – based on our results – the use of novelty score U for measuring novelty quantitatively, but question the use of novelty score W.

###### Discourse Context



###### References

[[July 19th, 2021]]

- some questioning of the novelty measure in a later paper ([[@bornmannWeMeasureNovelty2019]])  comparing it to [[@uzziAtypicalCombinationsScientific2013]]'s measure in terms of correlation to [[F1000 Prime]] ratings of biomed papers: depending on how you slice it, this is a plus or minus, since one key result in Wang is the asymmetry between home vs. foreign impact, and these F1000 tags are for papers that get recommended

    - binary

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FmGDz-fNPvY.png?alt=media&token=53d83368-6711-40dd-9e0c-10e90051784e)

    - poisson

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FzvlaLru7cu.png?alt=media&token=ac99a415-e143-4cc0-b985-f048f450c634)

    - tags for recommended papers

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FWzYiN04gkG.png?alt=media&token=453317bc-b5ca-45c6-8aba-e7eaca353583)
