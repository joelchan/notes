---
title: @suttonBitterLesson2019
url: https://roamresearch.com/#/app/megacoglab/page/srG1kG2YR
author: Joel Chan
date: Sat Aug 01 2020 13:59:06 GMT-0400 (Eastern Daylight Time)
---

- #references

    - Title: The Bitter Lesson

    - Meta:

        - Tags: #ref/Other

        - Authored by::  Rich Sutton

        - Year: [[2019]]

        - Publication: no_info

        - URL: http://www.incompleteideas.net/IncIdeas/BitterLesson.html

        - Citekey: suttonBitterLesson2019

    - Content

        - Placeholder

        - Abstract

        - #quotes

            - "general methods that leverage computation are ultimately the most effective, and by a large margin"

            - "The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. ^^We want AI agents that can discover like we can, not which contain what we have discovered^^. Building in our discoveries only makes it harder to see how the discovering process can be done"

###### Discourse Context



###### References

[[August 1st, 2020]]

- Lots of discussion of [[sys/GPT-3]] and what it means. One good take on this is that these large [[transformer language model]] might illustrate [[the bitter lesson of AI]] [[@suttonBitterLesson2019]] "general methods that leverage computation are ultimately the most effective, and by a large margin"

    - The deeper point here is less about computation per se, and more about the folly of trying to incorporate *our* (imperfect) understanding how minds work as priors for AI systems; far better to develop ways for the systems to learn these priors directly, than to rely on our incomplete, flattened understanding of the structure of the world and our mental toolkit

        - "The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity. Essential to these methods is that they can find good approximations, but the search for them should be by our methods, not by us. ^^We want AI agents that can discover like we can, not which contain what we have discovered^^. Building in our discoveries only makes it harder to see how the discovering process can be done"

            - I find this to be a very powerful connection to the lesson of [[Knowledge Management]] about the primacy of [[tacit knowledge]], and the eventual turn away from the [[repository model]].
