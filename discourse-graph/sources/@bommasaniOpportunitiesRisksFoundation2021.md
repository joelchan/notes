---
title: @bommasaniOpportunitiesRisksFoundation2021
url: https://roamresearch.com/#/app/megacoglab/page/nZ4yugCaS
author: Joel Chan
date: Fri Nov 05 2021 13:18:08 GMT-0400 (Eastern Daylight Time)
---

- Title:: On the Opportunities and Risks of Foundation Models
- Author(s):: [[Rishi Bommasani]], [[Drew A. Hudson]], [[Ehsan Adeli]], [[Russ Altman]], [[Simran Arora]], [[Sydney von Arx]], [[Michael S. Bernstein]], [[Jeannette Bohg]], [[Antoine Bosselut]], [[Emma Brunskill]], [[Erik Brynjolfsson]], [[Shyamal Buch]], [[Dallas Card]], [[Rodrigo Castellon]], [[Niladri Chatterji]], [[Annie Chen]], [[Kathleen Creel]], [[Jared Quincy Davis]], [[Dora Demszky]], [[Chris Donahue]], [[Moussa Doumbouya]], [[Esin Durmus]], [[Stefano Ermon]], [[John Etchemendy]], [[Kawin Ethayarajh]], [[Li Fei-Fei]], [[Chelsea Finn]], [[Trevor Gale]], [[Lauren Gillespie]], [[Karan Goel]], [[Noah Goodman]], [[Shelby Grossman]], [[Neel Guha]], [[Tatsunori Hashimoto]], [[Peter Henderson]], [[John Hewitt]], [[Daniel E. Ho]], [[Jenny Hong]], [[Kyle Hsu]], [[Jing Huang]], [[Thomas Icard]], [[Saahil Jain]], [[Dan Jurafsky]], [[Pratyusha Kalluri]], [[Siddharth Karamcheti]], [[Geoff Keeling]], [[Fereshte Khani]], [[Omar Khattab]], [[Pang Wei Koh]], [[Mark Krass]], [[Ranjay Krishna]], [[Rohith Kuditipudi]], [[Ananya Kumar]], [[Faisal Ladhak]], [[Mina Lee]], [[Tony Lee]], [[Jure Leskovec]], [[Isabelle Levent]], [[Xiang Lisa Li]], [[Xuechen Li]], [[Tengyu Ma]], [[Ali Malik]], [[Christopher D. Manning]], [[Suvir Mirchandani]], [[Eric Mitchell]], [[Zanele Munyikwa]], [[Suraj Nair]], [[Avanika Narayan]], [[Deepak Narayanan]], [[Ben Newman]], [[Allen Nie]], [[Juan Carlos Niebles]], [[Hamed Nilforoshan]], [[Julian Nyarko]], [[Giray Ogut]], [[Laurel Orr]], [[Isabel Papadimitriou]], [[Joon Sung Park]], [[Chris Piech]], [[Eva Portelance]], [[Christopher Potts]], [[Aditi Raghunathan]], [[Rob Reich]], [[Hongyu Ren]], [[Frieda Rong]], [[Yusuf Roohani]], [[Camilo Ruiz]], [[Jack Ryan]], [[Christopher Ré]], [[Dorsa Sadigh]], [[Shiori Sagawa]], [[Keshav Santhanam]], [[Andy Shih]], [[Krishnan Srinivasan]], [[Alex Tamkin]], [[Rohan Taori]], [[Armin W. Thomas]], [[Florian Tramèr]], [[Rose E. Wang]], [[William Wang]], [[Bohan Wu]], [[Jiajun Wu]], [[Yuhuai Wu]], [[Sang Michael Xie]], [[Michihiro Yasunaga]], [[Jiaxuan You]], [[Matei Zaharia]], [[Michael Zhang]], [[Tianyi Zhang]], [[Xikun Zhang]], [[Yuhui Zhang]], [[Lucia Zheng]], [[Kaitlyn Zhou]], [[Percy Liang]]
- Abstract:: AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.
- Type:: [[Article]]
- Publication:: arXiv:2108.07258 [cs]
- URL : http://arxiv.org/abs/2108.07258
- Date Added:: [[November 5th, 2021]]
- Zotero links:: [Local library](zotero://select/groups/2451508/items/IV2BB8Y4), [Local library](https://www.zotero.org/groups/2451508/items/IV2BB8Y4)
- Tags:: #[[Computer Science - Artificial Intelligence]], #[[Computer Science - Computers and Society]], #[[Computer Science - Machine Learning]]
- PDF links : [2108.07258.pdf](zotero://open-pdf/groups/2451508/items/NWPE4MWD)
- [[Notes]]

    - Comment: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI)

###### Discourse Context

- **Informs::** [[QUE - What do we know about transformer language models' natural language generation capabilities]]

###### References

[[NowReading]]

- [[@bommasaniOpportunitiesRisksFoundation2021]] for [[[[QUE]] - What do we know about [[transformer language model]]s' natural language generation capabilities?]] |

    - I like using the term [[foundation model]]

    - key aspects/implications: 1) emergence, and 2) homogenization

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FRhjyDD3UWc.png?alt=media&token=3d102583-a236-4c1d-b064-755bef1d386a) (p. 3)

    - claims to follow up on #[[➰ breadcrumbs]]

        - non-experts struggle to distinguish text generated by [[sys/GPT-3]] from human-generated text

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FTcMjslH-Ab.png?alt=media&token=4369585e-2913-464b-b8f4-87154538d43b) (p. 22)

        - previous SOTA for text-generation was via linguistic sub-tasks

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fox-g1DVufB.png?alt=media&token=9cec7ca1-d809-4a3b-814d-7e101f122c23)

        - joint-training of language models over high- and low-resource languages can support multilingual abilities

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FyIHyCPcSF7.png?alt=media&token=9d99b599-c6ff-4c1b-8fc6-c5e4000f915a)

            - but be careful: not clear how this works when we don't have English in the mix

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FBZ_s-S0Fkn.png?alt=media&token=093fca22-53a9-4462-9987-e5a90d71c706)

        - can be difficult to get [[transformer language model]] to consistently perform intended task, and we still don't really understand what [[transformer language model]] are capable of. here are some leads on solutions (e.g., [[prompt programming]])

    - questions that come to mind:

        - are there common "checklists" of things we need to do when exploring a new prompting-based task?
[[November 10th, 2021]]

- Our aims here are a bit more modest. We're not as interested in an autonomous agent, so much as we are interested in exploring the capabilities of AI systems like [[transformer language model]]s, which have high levels of adaptability and generativity [[@bommasaniOpportunitiesRisksFoundation2021]] via the paradigm of [[prompt programming]] (so they don't need to be expensively developed through [[supervised learning]] or even [[fine-tuning]])

    - MANY other examples from just a quick search

        - Deep Learning Trends: top 20 best uses of GPT-3 by OpenAI: https://www.educative.io/blog/top-uses-gpt-3-deep-learning

        - Calvin French-Owen on Twitter: "I've been experimenting with GPT-3 and wow! I haven't seen a technology this magical in a long time Yes, the model is shockingly good. More interestingly, it opens up programming to anyone who can write. Writing tools will become the new thinking tools. https://t.co/cCsCvRW3Hs" / Twitter: https://twitter.com/calvinfo/status/1286332337563684865

        - Siddharth Karamcheti on Twitter: "Since getting academic access, I’ve been thinking about GPT-3’s applications to grounded language understanding — e.g. for robotics and other embodied agents. In doing so, I came up with a new demo: Objects to Affordances: “what can I do with an object?” cc @gdb https://t.co/ptRXmy197P" / Twitter: https://twitter.com/siddkaramcheti/status/1286168606896603136

        - shreyashankar/gpt3-sandbox: The goal of this project is to enable users to create cool web demos using the newly released OpenAI GPT-3 API with just a few lines of Python.: https://github.com/shreyashankar/gpt3-sandbox

        - Learn Awesome on Twitter: "Using GPT-3 for automatic quiz generation on any topic and then evaluating the students' answers. This thing is fantastic! @sama https://t.co/qutUffWh7J" / Twitter: https://twitter.com/learn_awesome/status/1286189729826738176

        - Compose.ai | GPT-3 Demo: https://gpt3demo.com/apps/compose-ai

        - Adflow | GPT-3 Demo: https://gpt3demo.com/apps/adflow-ai

        - Anyword | GPT-3 Demo: https://gpt3demo.com/apps/anyword

        - Pencil | GPT-3 Demo: https://gpt3demo.com/apps/trypencil

        - Sudowrite | GPT-3 Demo: https://gpt3demo.com/apps/sudowrite

        - GPT-3 Recipe Builder | GPT-3 Demo: https://gpt3demo.com/apps/gpt-3-recipe-builder

        - Red Diaries Fellini Forward by Campari | GPT-3 Demo: https://gpt3demo.com/apps/red-diaries-fellini-forward-by-campari

        - GPT-Startup | GPT-3 Demo: https://gpt3demo.com/apps/gpt-startup

        - IdeasAI | GPT-3 Demo: https://gpt3demo.com/apps/ideasai

        - ideasby.ai | GPT-3 Demo: https://gpt3demo.com/apps/ideasby-ai

        - 500+ Openers for Tinder written by GPT-3 | GPT-3 Demo: https://gpt3demo.com/apps/500-openers-for-tinder-written-by-gpt-3

        - AI Weirdness | GPT-3 Demo: https://gpt3demo.com/apps/ai-weirdness

        - GPT-3 pickup lines | GPT-3 Demo: https://gpt3demo.com/apps/gpt-3-tries-pickup-lines

        - things are a little crazy rn | GPT-3 Demo: https://gpt3demo.com/apps/things-are-a-little-crazy-rn

        - Wisdom_by_GPT3 | GPT-3 Demo: https://gpt3demo.com/apps/wisdom-by-gpt3

        - FABLE - The Future of Storytelling: https://fable-studio.com/

    - One concrete example of this is [[sys/GPT-2]], which was used to generate the popular [[sys/AI Dungeon]]

    - There's also some emerging evidence that [[[[CLM]] - [[transformer language model]]s have some analogical reasoning ability]], contra [[[[QUE]] - Can deep learning discover analogical representations?]]

        - another line of evidence for this might be the insane performance of giant language models like [[sys/GPT-3]]
[[Week of November 15th, 2021]]

- Thread of stuff about consolidation of (AI) research (also covered in [[@bommasaniOpportunitiesRisksFoundation2021]] as a major risk of homogenization)

    - How the MFA swallowed literature - by Erik Hoel - The Intrinsic Perspective: https://erikhoel.substack.com/p/how-the-mfa-swallowed-literature

    - Nadia Eghbal | The creator economy: https://nadiaeghbal.com/creator-economy

    - Meredith Whittaker on Twitter: "📢New paper! In which I work through a lot of my uncomfortable observations since joining academia, examining the alarming-but-quiet capture of academic AI research by big tech, what this means for how and what we know about AI, and how we can resist. https://t.co/WYUv91zy9L https://t.co/cnSlM9sLt3" / Twitter:

        - https://twitter.com/mer__edith/status/1460652982572400649

    - Abeba Birhane on Twitter: "Our (@radical_ai_, @dallascard, @willie_agnew, @DotanRavit, @TheMichelleBao &amp; I) preprint is up on ArXiv (paper currently under review) Paper: The Values Encoded in Machine Learning Research https://t.co/rGjFJ5C83k Code and supplementary material: https://t.co/JJlPzuIEg8 1/ https://t.co/8jkhsmdUDq" / Twitter:

        - https://twitter.com/Abebab/status/1410267861130620928

        - Abeba Birhane on Twitter: "In 2008/09, 24% of the top cited papers had corporate affiliated authors &amp; in 2018/19 this statistic almost tripled to 71%. Furthermore, we find a concentration of a few large tech firms increasing more than fivefold, from 11% to 58%. 6/ https://t.co/LwCrXFsEU2" / Twitter:

            - https://twitter.com/Abebab/status/1410271484308770824

        - Abeba Birhane on Twitter: "Proportion of papers with corporate ties, by author affiliation or funding, dramatically increased from 43% in 2008/09 to 79% in 2018/19. We also found paramount domination of elite universities; of the total papers with uni affiliations, 82% were from elite universities. 7/ https://t.co/kghYMdWVyK" / Twitter:

            - https://twitter.com/Abebab/status/1410272146115420160
