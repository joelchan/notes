---
title: @clarkAllThatHuman2021
url: https://roamresearch.com/#/app/megacoglab/page/RPAPEO0TQ
author: Joel Chan
date: Wed Dec 01 2021 23:48:39 GMT+0800 (Malaysia Time)
---

- Title:: All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text
- Author(s):: [[Elizabeth Clark]]
- Abstract:: Human evaluations are typically considered the gold standard in natural language generation, but as models' fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts' ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators' accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.
- Type:: [[Article]]
- Publication:: arXiv:2107.00061 [cs]
- URL : http://arxiv.org/abs/2107.00061
- Date Added:: [[December 1st, 2021]]
- Zotero links:: [Local library](zotero://select/groups/2451508/items/6ZS8ZUHC), [Local library](https://www.zotero.org/groups/2451508/items/6ZS8ZUHC)
- Tags:: [[Computer Science - Computation and Language]]
- PDF links : [Clark et al_2021_All That's 'Human' Is Not Gold.pdf](zotero://open-pdf/groups/2451508/items/9QNEKI47)
- [[Notes]]

    - Comment: references added, corrected typo

###### Discourse Context

- **Informs::** [[QUE - What do we know about transformer language models' natural language generation capabilities]]
- **Informs::** [[QUE - How does the choice of what data we about scholarly papers we index influence performance for information retrieval systems]]
- **SourceFor::** [[EVD - crowd workers were at chance accuracy in distinguishing human-written from GPT-3-written stories, news stories, and recipes - @clarkAllThatHuman2021]]
- **SourceFor::** [[EVD - crowd workers mostly relied on form vs. content heuristics to make their judgments about human-likeness of generate text - @clarkAllThatHuman2021]]
