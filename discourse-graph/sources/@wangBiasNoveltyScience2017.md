---
title: @wangBiasNoveltyScience2017
url: https://roamresearch.com/#/app/megacoglab/page/d43-XaKGV
author: Joel Chan
date: Fri Jul 16 2021 10:09:22 GMT-0400 (Eastern Daylight Time)
---

- #[[references]]

    - Title: Bias against novelty in science: A cautionary tale for users of bibliometric indicators

    - Meta:

        - Authored by:: [[Jian Wang]] [[Reinhilde Veugelers]] [[Paula Stephan]]

        - Year: [[2017]]

        - Publication: Research Policy

        - Zotero link: [Zotero Link](zotero://select/items/7_73KW9W4A)

        - URL: [Wang et al. (2017). Bias against novelty in science: A cautionary tale for users of bibliometric indicators. Research Policy](https://www.sciencedirect.com/science/article/pii/S0048733317301038)

    - Content

        - Abstract

            - Research which explores unchartered waters has a high potential for major impact but also carries a higher uncertainty of having impact. Such explorative research is often described as taking a novel approach. This study examines the complex relationship between pursuing a novel approach and impact. Viewing scientific research as a combinatorial process, we measure novelty in science by examining whether a published paper makes first-time-ever combinations of referenced journals, taking into account the difficulty of making such combinations. We apply this newly developed measure of novelty to all Web of Science research articles published in 2001 across all scientific disciplines. We find that highly novel papers, defined to be those that make more (distant) new combinations, deliver high gains to science: they are more likely to be a top 1% highly cited paper in the long run, to inspire follow-on highly cited research, and to be cited in a broader set of disciplines and in disciplines that are more distant from their “home” field. At the same time, novel research is also more risky, reflected by a higher variance in its citation performance. We also find strong evidence of delayed recognition of novel papers as novel papers are less likely to be top cited when using short time-windows. In addition, we find that novel research is significantly more highly cited in “foreign” fields but not in their “home” field. Finally, novel papers are published in journals with a lower Impact Factor, compared with non-novel papers, ceteris paribus. These findings suggest that science policy, in particular funding decisions which rely on bibliometric indicators based on short-term citation counts and Journal Impact Factors, may be biased against “high risk/high gain” novel research. The findings also caution against a mono-disciplinary approach in peer review to assess the true value of novel research.

###### Discourse Context

- **Informs::** [[CLM - Prevailing incentives in academia are bad for science]]
- **Informs::** [[QUE - How might domain distance modulate the effects of analogies on creative output]]
- **SourceFor::** [[EVD - highly novel combinations of cited journals in a paper were almost always cross-disciplinary, but cross-disciplinary combinations were infrequently novel - @wangBiasNoveltyScience2017]]
- **SourceFor::** [[EVD - highly novel papers had higher variance in their citation outcomes over a 15-year window, biased towards the higher impact tail of the distribution - @wangBiasNoveltyScience2017]]
- **SourceFor::** [[EVD - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - @wangBiasNoveltyScience2017]]
- **SourceFor::** [[EVD - highly novel papers were more likely to be published in lower impact journals - @wangBiasNoveltyScience2017]]
- **SourceFor::** [[EVD - here is another result - i'm not sure i care about it yet - @wangBiasNoveltyScience2017]]

###### References

[[Playground ]]

- [[EVD]] - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - [[@wangBiasNoveltyScience2017]]

    - x

        - 826.4283299896191

    - y

        - 920.781742128128

    - color
[[Playground ]]

- [[EVD]] - highly novel papers had higher variance in their citation outcomes over a 15-year window, biased towards the higher impact tail of the distribution - [[@wangBiasNoveltyScience2017]]

    - x

        - 875.5

    - y

        - 468

    - color
[[July 19th, 2021]]

- novelty --> higher variance in impact: [[[[EVD]] - highly novel papers had higher variance in their citation outcomes over a 15-year window, biased towards the higher impact tail of the distribution - [[@wangBiasNoveltyScience2017]]]]

    - 5.1

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FpcUxyFZz14.png?alt=media&token=01302e1b-7c00-47ea-a764-cc3b4604e8b8)

    - GNB model Poisson regression
[[August 9th, 2021]]

- emphasizing a lot the so-called "reception side" of things (cf. [[[[EVD]] - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - [[@wangBiasNoveltyScience2017]]]])

    - Lo and Kennedy mentioned a couple times:

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FQTbvYLIHNM.png?alt=media&token=d36b60a5-ea5b-4fef-b234-f73a757c89de)

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FiFUJ2rIIc4.png?alt=media&token=5d099fce-44b6-46ac-8514-c9dccb48d6a6)

    - Zuckerman an important ref i think, probably theory, seminal claims/hypotheses (but probably not empirical evdience?) interesting: coming at it from sociology, and grounded in film industry. be careful!

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fh-s0tKiYni.png?alt=media&token=ac087ac4-e6c5-4030-b299-03aad89e2ba9)
[[Discussion with Protocol Labs Metaresearch Journal Club, about @strasserBusinessExtractingKnowledge2021]]

- counterpoint: lagging indicators, [[[[CLM]] - bibliometric measures are biased against novel breakthrough research - [[@wangBiasNoveltyScience2017]]]] what do we lose by continuing to rely on this "great man" heuristic? and increasingly so as the number of papers grows?

    - [[[[EVD]] - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - [[@wangBiasNoveltyScience2017]]]]

    - see also On (Not) Reading Papers - LessWrong: https://www.lesswrong.com/posts/72QA8qk9g6wZNDWeS/on-not-reading-papers
[[July 19th, 2021]]

- novelty --> lower impact factor journal, ceteris paribus: [[[[EVD]] - highly novel papers were more likely to be published in lower impact journals - [[@wangBiasNoveltyScience2017]]]]

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1m2QIGfuy6.png?alt=media&token=29c3bcc2-721f-4124-b4d1-47488037f6ce) (p. 7)

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjuzOTE3MWe.png?alt=media&token=41ed3e00-1078-4f5e-9722-c48ad57d1b51) (p. 11)
[[August 9th, 2021]]

- [[@wangBiasNoveltyScience2017]] write that their results are supportive of this novelty penalty

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FndScgBMWwF.png?alt=media&token=860ce195-1e72-4275-b79b-5d92d21429bd)
[[July 19th, 2021]]

- [[@wangBiasNoveltyScience2017]] for [[[[QUE]] - How might domain distance modulate the effects of analogies on creative output?]] | papers that make first-time difficulty-adjusted combinations of journals have higher (variance in) impact, mostly over long-run over short-run, frequently in lower-impact venues, and mostly in other fields

    - # results

        - novelty --> lower impact factor journal, ceteris paribus: [[[[EVD]] - highly novel papers were more likely to be published in lower impact journals - [[@wangBiasNoveltyScience2017]]]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1m2QIGfuy6.png?alt=media&token=29c3bcc2-721f-4124-b4d1-47488037f6ce) (p. 7)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FjuzOTE3MWe.png?alt=media&token=41ed3e00-1078-4f5e-9722-c48ad57d1b51) (p. 11)

        - novelty strongly implies cross-disciplinary, but not the complement; [[[[EVD]] - highly novel combinations of cited journals in a paper were almost always cross-disciplinary, but cross-disciplinary combinations were infrequently novel - [[@wangBiasNoveltyScience2017]]]]

        - novelty --> higher variance in impact: [[[[EVD]] - highly novel papers had higher variance in their citation outcomes over a 15-year window, biased towards the higher impact tail of the distribution - [[@wangBiasNoveltyScience2017]]]]

            - 5.1

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FpcUxyFZz14.png?alt=media&token=01302e1b-7c00-47ea-a764-cc3b4604e8b8)

            - GNB model Poisson regression

        - novelty --> big hit, esp. in long run, and outside home field: [[[[EVD]] - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - [[@wangBiasNoveltyScience2017]]]]

        - summary fig

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FMD9uL1xKBl.png?alt=media&token=ff1de385-c3e5-49de-b11a-42c225f0e74d)

        - [[[[EVD]] - here is another result - i'm not sure i care about it yet - [[@wangBiasNoveltyScience2017]]]]

    - # methods notes

        - novelty = difficulty-adjusted first-time combination of journals

            - in this sense, it could be related to [[far analogies]], but perhaps more closely to [[🧱 conceptual combination]]

            - why might journals be a reasonable proxy for conceptual chunks? domains?

                - better than keywords? (maybe too granular?)

                - or subject areas? (maybe too coarse?)

                    - some have used [[MeSH]] headings.

                        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fu-fHUaHEJU.png?alt=media&token=1d010c7d-505f-4af5-aea7-087e8877b43e)

                        - might depend on there being a reasonably complete and high-quality ontology or controlled vocabulary

                - [[@uzziAtypicalCombinationsScientific2013]] makes the same move (journals as proxies of bodies of knowledge)

            - difficulty is proxied by the cosine similarity between the co-citation vectors of the journals

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FHHLCWvv1L4.png?alt=media&token=784a0747-d383-4b08-bd51-903e6693dac3)

            - novelty per paper is sum over journal pairs binary novel/not multiplied by cosine distance between the co-citation vectors of the journal

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FkcP0aTxEQZ.png?alt=media&token=74d3a02c-c7e8-4368-bdf9-41671c3dee31)

            - filter out journals that don't really get citations (er.... whyyyyyy if more novel papers tend to get published in lower impact factor journals on average). pay close attention to Appendix III

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F0Gkaoda-lr.png?alt=media&token=e22d1308-f7c2-4e10-af0a-4cab485716f3)

        - sample: 661,643 papers published in 2001 from 251 subject categories, retrieved from  [[Web of Science]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FVYIDxSOBLg.png?alt=media&token=7c165565-4a9e-466c-af5d-5f14a16d78a9) (p. 4)

        - for analysis, novelty was binned into three categories (no novelty, >= 1 novel combination, but overall score < 99th percentile for subject category, >= 99th percentile for subject category) due to a highly skewed distribution

            - not sure about this; would like to see robustness checks with a different estimation strategy. but ok i guess.

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fyhp_0sx_40.png?alt=media&token=298a87ad-d5f2-4616-ae87-f5bc5ad942d3)

        - properties of novelty score: as expected, highly skewed (~89% of papers do not make novel combinations, and ones that do only score modestly after difficulty adjustment)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FrbtFk4kNEa.png?alt=media&token=a5a76e16-a2b8-475a-a74b-8610cbebfde5) ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fsc2n6w-OiT.png?alt=media&token=e56f301d-a632-4a5b-81e4-5e4e30adef60) (p. 4)

        - results robust to categorical conception of novelty, as well as filtering of sample (see 5.7)

        - analytic approach: [[Generalized Negative Binomial (GNB) model]] and [[logistic regression]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FqFqgUYQEo7.png?alt=media&token=cd38c864-3e1e-47a4-b182-208bda3f3883)(p. 13)

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FOzePEfQhsi.png?alt=media&token=e696b0da-202b-4212-9f29-481f50de31c5) (p. 15)

    - # discussion

        - idea of "sleeping beauty" seems interesting, also as a keyword related to [[[[CLM]] - True creative breakthroughs often take a long time to develop]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fej012XkYDp.png?alt=media&token=b6ab2014-c55c-4441-a6b6-0f9aeb97bd5b)

        - authors propose novel [[🧱 conceptual combination]]s (implicitly "inside a discipline"?? or outside??) as mechanisms for [[outsider innovation]]

            - would be really fun if we had the data to tease out differences between novel conceptual combinations that are close/far from the home/target discipline

                - if farther, then something new is being brought in (direct transfer)

                - if nearer, then maybe it's more about novel problem framing that allows for novel combination of existing pieces

                - for now, we know this connects well to [[far analogies]] because ~96% of the novel combinations are cross-disciplinary. which... makes sense based on how they define it.

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fc5_28pKgT1.png?alt=media&token=38a5eb7c-412d-4df6-bc32-4ae7299b27b4)

        - the measurement of impact in terms of the long run outcomes for a single paper seem... off to me. i want to know about field-wide changes. which i suppose is not really possible to glean from citation data, because they involve counterfactuals. maybe in some limited cases we can get some natural experiments to compare.

        - i keep thinking about relationships between this and [[literature-based discovery]]! which has a similar intuition about basically "undiscovered public knowledge" (undiscovered fruitful connections between things)

        - the results on delayed recognition and lower-impact factor REALLY throw a wrench in heuristics that focus on filtering papers based on "top-tier venues"

            - yeah you can recover by buzz making its way through the community post-hoc, but based on the dynamics laid out here, basically guarantees that you'll be like... 7 years behind the curve on new breakthroughs? in exchange for what? higher short-run throughput maybe?

            - how far does this extend? to workshop papers? "just books" (no peer-reviewed journal papers???)

                - see, e.g., this book on [[Naturalistic Decision-Making]]

            - my intuition is also that this *underestimates* the delayed recognition penalty, when accounting for other forces like active prestige differentials and so on

                - thinking of [[@morganPrestigeDrivesEpistemic2018]] and [[@cattaniDeconstructingOutsiderPuzzle2017]] and [[@gruberDarwinManPsychological1974]]

            - and i really wonder if things have changed now: this was for papers published in 2001; this was pre-Google Scholar, pre-BERT, and pre-Twitter - and throughout this time, my feeling is that the constricting forces of publish or perish have only gotten stronger

        - should really pay attention to the variance in these effects: how tight is the relationship? we know for sure that the variance in hits

            - it's hard/impossible to tease apart variance due to extraneous factors vs. variance due to intrinsic variation in the quality of hte outcome

            - woudl really want to dive into the highly novel combos that didn't pan out, and see what was going on

        - some questioning of the novelty measure in a later paper ([[@bornmannWeMeasureNovelty2019]])  comparing it to [[@uzziAtypicalCombinationsScientific2013]]'s measure in terms of correlation to [[F1000 Prime]] ratings of biomed papers: depending on how you slice it, this is a plus or minus, since one key result in Wang is the asymmetry between home vs. foreign impact, and these F1000 tags are for papers that get recommended

            - binary

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FmGDz-fNPvY.png?alt=media&token=53d83368-6711-40dd-9e0c-10e90051784e)

            - poisson

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FzvlaLru7cu.png?alt=media&token=ac99a415-e143-4cc0-b985-f048f450c634)

            - tags for recommended papers

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FWzYiN04gkG.png?alt=media&token=453317bc-b5ca-45c6-8aba-e7eaca353583)

        - main claim here is that [[[[CLM]] - bibliometric measures are biased against novel breakthrough research - [[@wangBiasNoveltyScience2017]]]]

        - basic result of [[[[EVD]] - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - [[@wangBiasNoveltyScience2017]]]] seems to be replicated by a later shorter paper focusing only on physicists: [[@mairesseNoveltyScienceImpact2018]] - noting that this was a sample of papers from 2005-2009, so it starts to get a little bit at my misgivings about generalizing from a sample from 2001 (although it also pushes in the opposite direction by focusing only on physics)
