---
title: @feldmanBuildingBetterSearch2020
url: https://roamresearch.com/#/app/megacoglab/page/BLSHaKuh_
author: Joel Chan
date: Sun Aug 02 2020 23:55:18 GMT-0400 (Eastern Daylight Time)
---



###### Discourse Context



###### References

[[August 2nd, 2020]]

- great #example-of [[context]] in ML, getting something to work well for an academic [[search engine]]: [[@feldmanBuildingBetterSearch2020]]

    - [Building a Better Search Engine for Semantic Scholar](https://medium.com/ai2-blog/building-a-better-search-engine-for-semantic-scholar-ea23a0b661e7)

        - #quotes

            - In this post, I’ll provide a “tell-all” account of why the above process was not as simple as we had hoped, and detail the following problems and their solutions:

            - The data is absolutely filthy and requires careful understanding and filtering.

            - Many features improve performance during model development but cause bizarre and unwanted behavior when used in practice.

            - Training a model is all well and good, but choosing the correct hyperparameters isn’t as simple as optimizing [nDCG](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) on a held-out test set.

            - The best-trained model still makes some bizarre mistakes, and posthoc correction is needed to fix them.

            - Elasticsearch is complex, and hard to get right.
