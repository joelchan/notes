---
title: @smaldinoNaturalSelectionBad2016
url: https://roamresearch.com/#/app/megacoglab/page/f5Qdl_BhB
author: Joel Chan
date: Tue Oct 27 2020 14:54:43 GMT-0400 (Eastern Daylight Time)
---

- #[[references]]

    - Title: The natural selection of bad science

    - Meta:

        - Authored by:: [[Paul E. Smaldino]] [[Richard McElreath]]

        - Year: [[2016]]

        - Publication: Royal Society Open Science

        - Zotero link: [Zotero Link](zotero://select/items/1_4UPCXNDH)

        - URL: [Smaldino & McElreath (2016). The natural selection of bad science. Royal Society Open Science](https://royalsocietypublishing.org/doi/full/10.1098/rsos.160384)

    - Content

        - Abstract

            - Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing‚Äîno deliberate cheating nor loafing‚Äîby scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‚Äòprogeny,‚Äô such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.

    - #[[üìù lit-notes]]

        - #[[observation-notes]]

            - The authors constructed a simple dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. In the model, successful labs produce more "progeny", and their preferred methods are more often copied and transmitted through their students being more likely to start their own labs.

                - In a similar fashion to what is seen in empirical observations of scientific practice, selecting for high output in [the model](The authors constructed a simple dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. In the model, successful labs produce more "progeny", and their preferred methods are more often copied and transmitted through their students being more likely to start their own labs.) led to the proliferation of poorer methods, such as low statistical power, and increasingly high false discovery rates

                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F2oY_cMp8TU.png?alt=media&token=9960298d-206e-484f-b30a-34d323390313)

                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FMO4IvpK0zN.png?alt=media&token=08130026-1f88-49d1-b248-d6fc7256413c) (p. 11)

                - Modifying the model to also allow for replication studies slowed but did not stop the natural selection of poorer methods over time.

                    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FreapnZb_bg.png?alt=media&token=0a4776de-7503-4e44-85d0-83a84b14c1c7) (p. 12)

###### Discourse Context

- **Informs::** [[CLM - Prevailing incentives in academia are bad for science]]

###### References

[[CLM - Prevailing incentives in academia are bad for science]]

- [[@smaldinoNaturalSelectionBad2016]]

    - In a similar fashion to what is seen in empirical observations of scientific practice, selecting for high output in [the model](((DRG86XRFt))) led to the proliferation of poorer methods, such as low statistical power, and increasingly high false discovery rates
[[January 31st, 2021]]

- wow! [[Paul E. Smaldino]] was also author on [that classic paper on natural selection of bad science]([[@smaldinoNaturalSelectionBad2016]])???

    - credibility++ then: this person has been thinking deeply about this problem (to the extent of creating a significant (recognized by community as such by publication and citations) model of the dynamics of methods selection/persistence) for a significant period of time (at minimum 5ish years)

    - [supporting cites](https://scite.ai/reports/the-natural-selection-of-bad-WL8RV5?contradicting=false&mentioning=false&page=1&utm_campaign=badge_generic&utm_medium=plugin&utm_source=generic) (via [[sys/scite.ai]]) for [[@smaldinoNaturalSelectionBad2016]] are a goldmine of evidence for thinking through [[[[CLM]] - Prevailing incentives in academia are bad for science]]

        - of particular interest is [[@brembsPrestigiousScienceJournals2018]], which reviews a few lines of unique evidence where some fields have fairly "objective" (quantifiable) indices of methods quality that can be computed and compared, and this is used to observe whether/how methods quality varies with journal "rank" (finding is that there is an *inverse* correlation: worse methods in higher-ranking journals)

            - addresses concerns about [[retraction]] as an index of methods quality
[[January 31st, 2021]]

- [supporting cites](https://scite.ai/reports/the-natural-selection-of-bad-WL8RV5?contradicting=false&mentioning=false&page=1&utm_campaign=badge_generic&utm_medium=plugin&utm_source=generic) (via [[sys/scite.ai]]) for [[@smaldinoNaturalSelectionBad2016]] are a goldmine of evidence for thinking through [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - of particular interest is [[@brembsPrestigiousScienceJournals2018]], which reviews a few lines of unique evidence where some fields have fairly "objective" (quantifiable) indices of methods quality that can be computed and compared, and this is used to observe whether/how methods quality varies with journal "rank" (finding is that there is an *inverse* correlation: worse methods in higher-ranking journals)

        - addresses concerns about [[retraction]] as an index of methods quality
[[May 20th, 2021]]

- Provocative counterpoint to [[@smaldinoNaturalSelectionBad2016]], focusing on the role of theory - very reminiscent of [[Iris van Rooij]]'s work

    - cc [[[[CLM]] - Scientific fields stall without adequate theoretical synthesis]]
