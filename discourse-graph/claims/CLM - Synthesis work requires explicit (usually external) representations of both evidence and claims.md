---
title: [[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims
url: https://roamresearch.com/#/app/megacoglab/page/1bdxRphK9
author: Joel Chan
date: Sun Feb 06 2022 09:39:08 GMT+0800 (Malaysia Time)
---

- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FmTZ7FdjdMs.jpeg?alt=media&token=4cc9b738-cfea-476f-a0d0-ad881ed40a38)
- Draw on [[dance between the theoretical and the evidential]]
- I want to think through a thoughtful question I got from someone today, roughly paraphrased, "why represent evidence explicitly in your notes / in the scholarly discourse, if claims are what we really care about?"
- Here's the question, modified slightly:

    - > Evidence can be revised and analyzed differently leading to diff results. However, when a claim is accepted by a community (academic or otherwise), implicitly, its evidence is also deemed acceptable (at least partially). We can expand on types/contexts of evidence to support a claim and eventually, an edge case might appear giving rise to a new claim, repeating the cycle. In this cycle claims are the proxy of entry that naturally leads to its evidence. What's the advantage of going the other way around?
- First, what I agree with.

    - Claims are indeed the most natural point of entry into a discourse.

        - Theories and arguments are built fundamentally from claims. And masses of individual results are hard to interpret without having a sense of what they "mean" (the claims).

    - The general cycle of evidence being refined and eventually sparking new claims or theories, is how I think it should work! And I think in aggregate it probably does work like this often, although I do not know how often.
- So why not just work with claims? A few reasons I care about.

    - It's tricky to know when a claim is "accepted", and even more tricky to know when this acceptance also implies good evidence. Both false positives and false negatives are common, and have serious consequences for both scientific progress, and for society.

        - Some examples:

            - droplets are primary mechanism of transmission for COVID (false positive: https://www.wired.com/story/the-teeny-tiny-scientific-screwup-that-helped-covid-kill/)

            - neural information retrieval mechanisms far outperform simpler methods

            - "[[scientific flip-flopping]]

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FI_rFi86zY_.png?alt=media&token=ce9bd08d-4e0f-44af-a065-964fa6c20ee3)≈ì

                - https://twitter.com/ArtirKel/status/1495169375891980288

        - This difficulty has consequences for peer review (reviewer says "not novel; we already know x", but we really don't, or at least only under condition A), and project planning and discovery ("oh no, xyz papers have already done A"), slowing or sometimes stifling progress.

        - Insiders can get a good sense of this, through thorough engagement with the literature. But IME this sense is only very reliable for a narrow band of your direct specialty. And we don't always have the luxury of building only on evidence and claims within our exact specialty

        - It helps then, to be able to go back through and interrogate the evidence. This is hard when only claims are explicitly represented and privileged, e.g., through citation statements, abstracts, titles, headlines, etc.

        - Claim with N citations can convey far more certainty and generality than is warranted. And our human brains and incentive structures seem to push us to do this often. I see this repeatedly in my own lived experience.

        - Let's look at some examples that others have documented of the [lossy claim+citation telephone game]([[CLM - Citation practices in science are far from optimal]]

            - [[@harzingPersistentMythHigh1995]]

            - [[EVD - A style guide example citation (not a real paper) was copied by more than 10 later papers to support claims of therapeutic effects of Rutin - @lengPhantomReferencePropagation2020]]

            - We report a method of estimating what percentage of people who cited a paper had actually read it. The method is based on a stochastic modeling of the citation process that explains empirical studies of misprint distributions in citations (which we show follows a Zipf law). Our estimate is only about 20% of citers read the original.

            - #example-of [[affirmative citation bias]]

            - Citations of a particular finding about an Alzheimer's relevant protein in the brain tended to be biased against refutation, and biased towards amplifications, leading to substantial distortions in judgments of the reliability of the finding [[@greenbergHowCitationDistortions2009]]

            - [[open access]]

            - Papers citing papers that refute the Hawthorne effect cite those papers **in support of** the Hawthorne effect ü§¶‚Äç‚ôÇÔ∏è [[@letrudAffirmativeCitationBias2019]]

            - One striking observation in [[@cohanStructuralScaffoldsCitation2019]]

            - https://twitter.com/scite/status/1286332504211894272

            - It's worth noting that as an aggregate signal of quality, number of citations (w/ various transformations) works ok, and seems to reveal much about the social and institituional structure of science, but as a means for systematically tracing lines of evidence, I worry a lot about the quality of the input data. This is why i find @scite indispensable. Though it is limited by the quality of the input data.

                - Recent review: [[@tahamtanWhatCitationCounts2019]]

                - And essay on responsible use of citation data: [[@clausetDatadrivenPredictionsScience2017]]

    - 2 - One might argue that the previous issue is constrained to our current suboptimal incentive structures and human error. What if we all acted in good faith? I think we would still want separable, accessible means of tracing claims back to their evidence, because [[CLM - Knowledge is fundamentally contextual]]

        - Claims are scoped. Conditions can change. Obvious example is what is possible vs. not, why X did not work. Crucial to retain info. This is sort of inherent to any sort of knowledge claim

        - For science at the frontier of knowledge, I think there's more to it. Significance of result changes over time, need to have it be independent somehow

            - See [[long search paths]]

            - And [[by-productive thinking]]

                - {{embed: A few main examples of these}}

            - Sleeping beauties (some pointers in this from [[@clausetDatadrivenPredictionsScience2017]]

###### Discourse Context

- **Informed By::** [[@greenbergHowCitationDistortions2009]]
- **Informed By::** [[@tahamtanWhatCitationCounts2019]]
- **Informed By::** [[@letrudAffirmativeCitationBias2019]]
- **Informed By::** [[@lengPhantomReferencePropagation2020]]
- **Informed By::** [[@clausetDatadrivenPredictionsScience2017]]
- **Informed By::** [[CLM - Knowledge is fundamentally contextual]]
- **Informed By::** [[CLM - Citation practices in science are far from optimal]]
