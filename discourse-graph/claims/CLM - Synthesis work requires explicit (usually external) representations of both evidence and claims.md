---
title: [[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims
url: https://roamresearch.com/#/app/megacoglab/page/1bdxRphK9
author: Joel Chan
date: Sat Feb 05 2022 20:39:08 GMT-0500 (Eastern Standard Time)
---

- ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FmTZ7FdjdMs.jpeg?alt=media&token=4cc9b738-cfea-476f-a0d0-ad881ed40a38)
- Draw on [[dance between the theoretical and the evidential]]
- I want to think through a thoughtful question I got from someone today, roughly paraphrased, "why represent evidence explicitly in your notes / in the scholarly discourse, if claims are what we really care about?"
- Here's the question, modified slightly:

    - > Evidence can be revised and analyzed differently leading to diff results. However, when a claim is accepted by a community (academic or otherwise), implicitly, its evidence is also deemed acceptable (at least partially). We can expand on types/contexts of evidence to support a claim and eventually, an edge case might appear giving rise to a new claim, repeating the cycle. In this cycle claims are the proxy of entry that naturally leads to its evidence. What's the advantage of going the other way around?
- First, what I agree with.

    - Claims are indeed the most natural point of entry into a discourse.

        - Theories and arguments are built fundamentally from claims. And masses of individual results are hard to interpret without having a sense of what they "mean" (the claims).

    - The general cycle of evidence being refined and eventually sparking new claims or theories, is how I think it should work! And I think in aggregate it probably does work like this often, although I do not know how often.
- So why not just work with claims? A few reasons I care about.

    - It's tricky to know when a claim is "accepted", and even more tricky to know when this acceptance also implies good evidence. Both false positives and false negatives are common, and have serious consequences for both scientific progress, and for society.

        - Some examples:

            - droplets are primary mechanism of transmission for COVID (false positive: https://www.wired.com/story/the-teeny-tiny-scientific-screwup-that-helped-covid-kill/)

            - neural information retrieval mechanisms far outperform simpler methods

            - "[[scientific flip-flopping]]" (h/t [[Jos√© Luis Ric√≥n Fern√°ndez de la Puente]]:

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FI_rFi86zY_.png?alt=media&token=ce9bd08d-4e0f-44af-a065-964fa6c20ee3)≈ì

                - https://twitter.com/ArtirKel/status/1495169375891980288

        - This difficulty has consequences for peer review (reviewer says "not novel; we already know x", but we really don't, or at least only under condition A), and project planning and discovery ("oh no, xyz papers have already done A"), slowing or sometimes stifling progress.

        - Insiders can get a good sense of this, through thorough engagement with the literature. But IME this sense is only very reliable for a narrow band of your direct specialty. And we don't always have the luxury of building only on evidence and claims within our exact specialty

        - It helps then, to be able to go back through and interrogate the evidence. This is hard when only claims are explicitly represented and privileged, e.g., through citation statements, abstracts, titles, headlines, etc.

        - Claim with N citations can convey far more certainty and generality than is warranted. And our human brains and incentive structures seem to push us to do this often. I see this repeatedly in my own lived experience.

        - Let's look at some examples that others have documented of the [lossy claim+citation telephone game]([[[[CLM]] - Citation practices in science are far from optimal]]):

            - [[@harzingPersistentMythHigh1995]] is another really striking example, arguing that "there is almost no empirical foundation for the existence of high failure rates when measured as premature re-entry. The persistent myth of high expatriate failure rates seems to have been created by massive (mis)quotations of three articles. Only one of these articles contained solid empirical evidence on expatriate failure rates and in fact showed them to be rather low"

            - [[[[EVD]] - A style guide example citation (not a real paper) was copied by more than 10 later papers to support claims of therapeutic effects of Rutin - [[@lengPhantomReferencePropagation2020]]]]

            - We report a method of estimating what percentage of people who cited a paper had actually read it. The method is based on a stochastic modeling of the citation process that explains empirical studies of misprint distributions in citations (which we show follows a Zipf law). Our estimate is only about 20% of citers read the original.

            - #example-of [[affirmative citation bias]] [[[[CLM]] - Citation practices in science are far from optimal]]

            - Citations of a particular finding about an Alzheimer's relevant protein in the brain tended to be biased against refutation, and biased towards amplifications, leading to substantial distortions in judgments of the reliability of the finding #[[@greenbergHowCitationDistortions2009]]

            - [[open access]] could help ameliorate some of the [bad citing practices in science]([[[[CLM]] - Citation practices in science are far from optimal]]): people cite less from superficial knowledge because they can actually read the article! [[@mccabeCiteUnseenTheory2020]]

            - Papers citing papers that refute the Hawthorne effect cite those papers **in support of** the Hawthorne effect ü§¶‚Äç‚ôÇÔ∏è [[@letrudAffirmativeCitationBias2019]]

            - One striking observation in [[@cohanStructuralScaffoldsCitation2019]]: ((G_fqQ_Wl9))((wfq4THZ2-))

            - https://twitter.com/scite/status/1286332504211894272

            - It's worth noting that as an aggregate signal of quality, number of citations (w/ various transformations) works ok, and seems to reveal much about the social and institituional structure of science, but as a means for systematically tracing lines of evidence, I worry a lot about the quality of the input data. This is why i find @scite indispensable. Though it is limited by the quality of the input data.

                - Recent review: [[@tahamtanWhatCitationCounts2019]]

                - And essay on responsible use of citation data: [[@clausetDatadrivenPredictionsScience2017]]

    - 2 - One might argue that the previous issue is constrained to our current suboptimal incentive structures and human error. What if we all acted in good faith? I think we would still want separable, accessible means of tracing claims back to their evidence, because [[[[CLM]] - Knowledge is fundamentally contextual]]

        - Claims are scoped. Conditions can change. Obvious example is what is possible vs. not, why X did not work. Crucial to retain info. This is sort of inherent to any sort of knowledge claim

        - For science at the frontier of knowledge, I think there's more to it. Significance of result changes over time, need to have it be independent somehow

            - See [[long search paths]]

            - And [[by-productive thinking]]

                - {{embed: A few main examples of these}}

            - Sleeping beauties (some pointers in this from [[@clausetDatadrivenPredictionsScience2017]])

###### Discourse Context

- **Informed By::** [[@greenbergHowCitationDistortions2009]]
- **Informed By::** [[@tahamtanWhatCitationCounts2019]]
- **Informed By::** [[@letrudAffirmativeCitationBias2019]]
- **Informed By::** [[@lengPhantomReferencePropagation2020]]
- **Informed By::** [[@clausetDatadrivenPredictionsScience2017]]

###### References

[[outline2022-03-14 INST802 synthesis]]

- From claims/statements only (all at same level), to separating "claims" from "evidence": [[[[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]]

    - Three conceptual handles for the core intuition of [[dance between the theoretical and the evidential]]

        - [[m/Grounded Theory]]: [[[[CLM]] - Scholars repurpose qualitative data analysis software to facilitate synthesis]]

        - design: [[[[CLM]] - Design proceeds by co-evolution of problem and solution space]]

        - [[üß± sensemaking]]: [[[[CLM]] - Sensemaking requires iterative loops of (re)interpreting data in light of evolving schemas]]
[[April 6th, 2022]]

- Thought dump for [[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]]

    - Grounding for the model in cognitive models of scientific reasoning: [[@klahrDualSpaceSearch1988]] extended the problem space model to account for scientific discovery as joint search in a space of hypotheses and a space of experiments.
[[February 18th, 2022]]

- Need grounding in evidence cc [[[[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]]

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FuAzgE2QrGg.png?alt=media&token=8aafe20c-4142-4b40-8b1d-b6bcf72fb167)
[[Teach INST 802 Week 8 (Mar 14) - Process - Synthesis 1]]

- ### From claims/statements only, to a [dialectic between claims and evidence]([[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]])

    - This is a corollary of the previous point, but very worth calling out on its own.

    - Why care about the difference between claims and evidence?

    - It's tricky to know when a claim is "accepted", and even more tricky to know when this acceptance also implies good evidence. Both false positives and false negatives are common, and have serious consequences.

        - This difficulty has consequences for peer review (reviewer says "not novel; we already know x", but we really don't, or at least only under condition A), and project planning and discovery ("oh no, xyz papers have already done A"), slowing or sometimes stifling progress.

        - Insiders can get a good sense of this, through thorough engagement with the literature. But IME this sense is only very reliable for a narrow band of your direct specialty. And we don't always have the luxury of building only on evidence and claims within our exact specialty

        - It helps then, to be able to go back through and interrogate the evidence. This is hard when only claims are explicitly represented and privileged, e.g., through citation statements, abstracts, titles, headlines, etc.

        - Examples

            - droplets are primary mechanism of transmission for COVID (false positive: https://www.wired.com/story/the-teeny-tiny-scientific-screwup-that-helped-covid-kill/)

            - neural information retrieval mechanisms far outperform simpler methods

            - "[[scientific flip-flopping]]" (h/t [this tweet](https://twitter.com/ArtirKel/status/1495169375891980288)):

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FI_rFi86zY_.png?alt=media&token=ce9bd08d-4e0f-44af-a065-964fa6c20ee3)

    - Claim with N citations can convey far more certainty and generality than is warranted. And our human brains and incentive structures seem to push us to do this often. I see this repeatedly in my own lived experience.

        - Let's look at some examples that others have documented of the lossy claim+citation telephone game:

            - [[@harzingPersistentMythHigh1995]] is a really striking example, arguing that "there is almost no empirical foundation for the existence of high failure rates when measured as premature re-entry. The persistent myth of high expatriate failure rates seems to have been created by massive (mis)quotations of three articles. Only one of these articles contained solid empirical evidence on expatriate failure rates and in fact showed them to be rather low"

            - A style guide example citation (not a real paper) was copied by more than 10 later papers to support claims of therapeutic effects of Rutin - [[@lengPhantomReferencePropagation2020]]

            - We report a method of estimating what percentage of people who cited a paper had actually read it. The method is based on a stochastic modeling of the citation process that explains empirical studies of misprint distributions in citations (which we show follows a Zipf law). Our estimate is only about 20% of citers read the original. - [[@simkinReadYouCite2002]]

            - Citations of a particular finding about an Alzheimer's relevant protein in the brain tended to be biased against refutation, and biased towards amplifications, leading to substantial distortions in judgments of the reliability of the finding [[@greenbergHowCitationDistortions2009]]

            - Papers citing papers that refute the Hawthorne effect cite those papers **in support of** the Hawthorne effect ü§¶‚Äç‚ôÇÔ∏è [[@letrudAffirmativeCitationBias2019]]

            - study with high number of disputing citations amongst retracted articles continues to be cited as if it were reliable:

                - https://twitter.com/scite/status/1287734606435942400

                    - ![[CleanShot 2022-03-14 at 17.18.41.png]]

                    - #example-of [[affirmative citation bias]] [[[[CLM]] - Citation practices in science are far from optimal]]

    - One might argue that the previous issue is constrained to our current suboptimal incentive structures and human error. What if we all acted in good faith? I think we would still want separable, accessible means of tracing claims back to their evidence, because **knowledge is fundamentally contextual**

        - Claims are scoped. Conditions can change. Obvious example is what is possible vs. not, why X did not work. Crucial to retain info. This is sort of inherent to any sort of knowledge claim

        - For science at the frontier of knowledge, I think there's more to it. Significance of result changes over time, need to have it be independent somehow. See, for example, idea of sleeping beauties (some pointers in this from [[@clausetDatadrivenPredictionsScience2017]])

        - Distinguishing claims and evidence is also part of the core dialectic in science.

        - A central insight from the philosophy and practice of science is that a priori theorizing with minimal to no reference to data is not a viable path to deep conceptual progress; such theorizing would stall as "armchair speculation" [[@andersonReviewTheoryBuilding1970]]. On the other hand, theory-free mining of data patterns cannot lead to the depth of understanding needed to explain and solve difficult real-world problems [[@newellYouCanPlay1973]], [[@katzHowPlay202018]], [[@jonasCouldNeuroscientistUnderstand2017]]. This is because observations are inherently driven by our theories, which can often preclude observing the very data needed to construct powerful new theories [[@popperLogicScientificDiscovery1959]]; further, the space of possible theories is typically severely underdetermined by data.

        - Here is an eloquent quote on this from this analysis of Darwin's process towards his theory of evolution [[@gruberDarwinmanPsychological1974]]:

            - > The two kinds of task, theoretical and evidential, entail different activities, and in the long run they may yield distinguishable products for our consideration. But __in vivo__, in the life of the thinking person, they are thoroughly intertwined. The most speculative "castle in the air" is triggered by a simple observation or a friend's remark about his dog. Hard work amassing the facts on a special point is guided by a long theoretical argument with which it may have only a tenuous logical relationship: the theory does not absolutely depend on the facts, nor could the facts ever guarantee the theory. The relation of theory and evidence is not simply logical but psychological.

        - This core **dialectic** shows up in other senses too:

            - [[Grounded theory]] (our familiar friend)

            - Design: reflective conversation with the design situation / materials [[@schonDesigningReflectiveConversation1992]]

            - I think of the dialectic like this:

                - ![[CleanShot 2022-03-14 at 17.28.46.png]]

        - It is for this reason that I strongly recommend clearly distinguishing "takeaways" from sources in terms of "claims" and "evidence". They are not the same!

        - Evidence is closer to 'data', and claims are closer to "theory". I need both, explicitly, to keep me honest, to discipline my imagination.

        - In my years of research on this, I see this distinction falling by the wayside the easiest, and I suspect it comes from the politics of where most of our writing is happening: a prose related work section in papers, where space is scarce, and the speech act of citation for legitimacy is ever-present. Without an explicit representation of evidence, it's too easy for my thinking to ossify into just the "received wisdom at time X", especially since details are the first to fade from memory!

    - *SCOPING NOTE**: This design pattern comes from my [[Objectivism|Objectivist]]/[[Constructionism|Constructionist]]-leaning [[Pragmatism|pragmatist]] epistemology. If you're squarely in formal theory, philosophy, or [[Subjectivism]]-land, then I don't think this applies as cleanly. But if you're at all interested in having "the world" talk back to your theories, then this applies.

    - #discuss What are the analogs of "claims" and "evidence" across YOUR domains and problems?
[[Week of March 14th, 2022]]

- a few #example-of [[[[CLM]] - Knowledge is fundamentally contextual]] (pain only studied in male mice!!!! turns out quite diff for females; also BIG individ. diff in visual imagery). cc also [[[[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]]

    - Dr Suzy J Styles on Twitter: "Now when we look at data like Rosch‚Äôs prototype model of category structure, we can look at the data from US in the 1970s, but we can also talk concretely about how ROBIN is not the birdie at bird where we are! https://t.co/NYtlPbD0x3" / Twitter

        - tweet

            - https://twitter.com/suzyjstyles/status/1505112286339670016

    - Dr Suzy J Styles on Twitter: "Turns out that the experience of pain is regulated by TWO completely separate pathways, and one pathway is dominant in the presence of female sex hormones‚Ä¶ This persistent bias means current pain meds were designed to handle a male-specific type of pain! https://t.co/b9G4O2Myb7" / Twitter

        - tweet

            - https://twitter.com/suzyjstyles/status/1505109885641719810

    - Dr Suzy J Styles on Twitter: "And even though I knew what to expect, it was still stunning for me (a person with visual imagery) to discover we have six people taking the class synchronously who have no visual images!!! Here they are discovering that ‚Äòminds eye‚Äô metaphors are not just metaphors for others ü•∞ https://t.co/fuJEJ9dpIk" / Twitter

        - tweet

            - https://twitter.com/suzyjstyles/status/1505132430428356610
[[February 5th, 2022]]

- #[[üöö outline]] for [[[[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]]

    - [[dance between the theoretical and the evidential]]
[[Week of April 18th, 2022]]

- https://depts.washington.edu/gs630/Winter/Wineberg.pdf [[[[CLM]] - Synthesis work requires explicit (usually external) representations of both evidence and claims]] works for history too! primary sources (and their context) and observations thereof, vs. claims about history

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FP852PsZF5-.png?alt=media&token=76ad399d-01a6-49e7-b870-5657ea92d9ec)
