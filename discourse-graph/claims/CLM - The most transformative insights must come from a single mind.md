---
title: [[CLM]] - The most transformative insights must come from a single "mind"
url: https://roamresearch.com/#/app/megacoglab/page/G4kxfFSgy
author: Joel Chan
date: Mon Jun 15 2020 13:04:35 GMT-0400 (Eastern Daylight Time)
---

- #[[ðŸŒ² zettels]] #[[Z]]

    - Description

        - Something brewing in my brain, connecting back to that convo with [[Benjamin Reinhardt]] where he noted [[Andy Matuschak]] and [[Michael Nielsen]]'s conjecture that significant insights can/have only come from a "single mind". This might be a connection back to [[ðŸ§± interdisciplinary research modes]], and knowledge requirements for that, and why [[SRS]] might be a fascinating paradigm for understanding how we might supercharge creativity (e.g., [[ðŸ§± conceptual combination]]) by making more conceptual building blocks available for recombination in a single mind.

        - I do wonder if this effect of ideas intermingling over time at various levels of specificity can be approximated by being collocated and engaging in various "watercooler" and hallway [[Informal]] discussions

        - Seeing lots of papers on obvious application of [[SRS]] to fact-based learning like medical vocab and language learning, but not seen anything yet making the connection to downstream creative work, particularly [[interdisciplinarity]]

        - See also: [[[[CLM]] - Where possible, design systems, not tools]]

###### Discourse Context



###### References

[[May 15th, 2022]]

- random analogy to programming: [[[[CLM]] - The most transformative insights must come from a single "mind"]] and [[that can't pin it down local optimum tradeoff thing]]

    - rn interface between world and mind is often like... nonexistent. and most efficient and effective way to generate insights is to load into memory (working memory), so ppl rationally lean into that at the expense of distributing memory into the world

        - but... this can place upper limit on complexity of thought. need externalization. see, e.g., [[[[EVD]] - Starbucks baristas used a paper cup to support complex coordination to reduce the cost structure of creating complex drinks in a tight, multi-tasking and interruption-intensive environment - [[@kirshMultitaskingCostStructure2005]]]]

    - one way tot hink about tools for thought, then, is improving the interface between external memory and "in-memory"

    - and this extends to collaboration too

    - really tough to kind of... push things through a straw (speaking), and really ineffective and inefficient to try to push through writing. can get alignment through shared sketching. but what about all that context?

    - analogies in dbs:

        - sharding?

        - streaming?

    - this kind of goes back to a diff. between an *ability* framing, and a *capacity* framing (which can evoke [[[[PTN]] - enabling environment]]s)

    - d/graphing is a way to structure your environment to make resources easier to access

        - not necessarily to build an exhaustive model of your knowledge

            - that is a fool's errand, in part bc of how much of our knowledge is still in flux, and even though it might seem stable, it will be subject to change

                - this is why a ledger is great, but incomplete (cc [[org/DeSci Labs]]): we still want something like [[TFTNet]], for improving interface between minds in the like... 80%? of the time that thought is not stable enough to put into a distributed immutable ledger

    - thread from:

        - why you shouldn't interrupt a programmer h/t [[Max Krieger]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FkASmDtGrfF.png?alt=media&token=370f9ac0-7d7e-4e51-ba1d-f0ca1daeb666)
[[idea zettelconversations]]

- Can this kind of design do a better job (w/ fixed cost) of "[merging the minds]([[[[CLM]] - The most transformative insights must come from a single "mind"]])" than other methods?

    - Fully async might not have enough shared [[context]], and also not enough "traction" to build shared [[context]] (need to riff, etc.)

    - Fully sync w/o access to deep notes might be too shallow (not enough balance of rigor/depth to novelty/breadth)
[[January 14th, 2021]]

- Connecting back to [discussions with Niki](Review article w/ Lingfei Wu, who worked with [[James A. Evans]], first author on [[@wuLargeTeamsDevelop2019]]: small teams disrupt, large teams develop) yesterday about team size and a stray comment about an upper limit on the ability to act "as a single mind" (cf. [[[[CLM]] - The most transformative insights must come from a single "mind"]])

    - Which connected with [discussions with Brian and Mark](#[[Q]] how does scale play a role in conversational health?) about the adaptivity/fitness of different discussion "strategies/moves" at different scales, and maybe there's an upper limit in terms of scale for useful discussions?

        - This sensitizes me to notice comments like this:

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FuSEG1PBK9V.png?alt=media&token=915d5692-71d2-4ad9-b1e9-74764392d3f5)

    - There is another angle on scale now that I'm thinking about, sparked by this paper on "memoization" as a strategy for more efficient computation [[@dasguptaMemoryComputationalResource2021]] Title: Memory as a Computational Resource

        - Why might efficiency matter? I think there is something to the fact that the human brain operates on temporal dynamics (rising and falling of spikes, oscillation, synchronization), such that it might really matter whether or not we can get ideas "on the table" at the same timescale

            - Something about [[Hebbian learning]] here, [[cells that fire together, wire together]]

                - And back to [[[[CL]] - Complex compositionality can arise from surprisingly simple units and connection dynamics]]

                    - This could be a reason to be bullish on the answer "yes" to [[[[QUE]] - Can deep learning discover analogical representations?]], to the extent that connectivity could give rise to meaningful forms of [[compositionality]]

        - This is separate from ability to iterate: faster cycles = more iteration = more learning, better ideas
[[CLM - Where possible, design systems, not tools]]

- This might be one reason that [[Andy Matuschak]] and [[Michael Nielsen]] think that [[[[CLM]] - The most transformative insights must come from a single "mind"]]. Because it's really challenging to (or we haven't yet figured out how to) enable a medium with data model properties of [[compression]], [[context]], and [[compositionality]] that spans multiple minds; a distributed system that enfolds or spans more than one mind and their workflow.

    - Our current workaround is to have people spend LOTS of time together in an intense collaboration to get to the point where this is possible, or to have people who are very similar already get together.

    - But there is a very [significant barrier]([[[[CLM]] - Interdisciplinary teamwork is hard]]) to team-based [[Convergence]] because of this bottleneck of not being able to have a shared "dataset" of ideas that satisfy the properties. Our external representations that are **shareable** lack one or more of those properties. We've designed models for some that could have these properties, but nobody will use them.

        - Also: [[Charles Darwin]] did the theory of evolution with a paper notebook! Ditto Einstein, [[Richard Feynman]], etc. [[@americaninstituteofphysicsInterviewRichardFeyman2015]]
[[June 7th, 2021]]

- just came across this paper by [[R. Keith Sawyer]] generally applicable to thinking about [[collective creativity]] with a bit of an analogy to [[distributed cognition]] - makes me also think it's useful for thinking about [[[[QUE]] - How can we best bridge private vs. public knowledge?]] and contra [[Andy Matuschak]]'s ideas about [[[[CLM]] - The most transformative insights must come from a single "mind"]]

    - [[@sawyerDistributedCreativityHow2009]]
[[January 4th, 2021]]

- connecting back to [[[[CLM]] - The most transformative insights must come from a single "mind"]] - questions can be a way to organize and form in open-ended knowledge work

    - act as the navigational guides, motivators, to explore someone else's graph
[[January 8th, 2022]]

- [[[[CLM]] - The most transformative insights must come from a single "mind"]] and [[[[CLM]] - Interdisciplinary teamwork is hard]]

    - This might be one reason that [[Andy Matuschak]] and [[Michael Nielsen]] think that [[[[CLM]] - The most transformative insights must come from a single "mind"]]. Because it's really challenging to (or we haven't yet figured out how to) enable a medium with data model properties of [[compression]], [[context]], and [[compositionality]] that spans multiple minds; a distributed system that enfolds or spans more than one mind and their workflow.

        - Our current workaround is to have people spend LOTS of time together in an intense collaboration to get to the point where this is possible, or to have people who are very similar already get together.

        - But there is a very [significant barrier]([[[[CLM]] - Interdisciplinary teamwork is hard]]) to team-based [[Convergence]] because of this bottleneck of not being able to have a shared "dataset" of ideas that satisfy the properties. Our external representations that are **shareable** lack one or more of those properties. We've designed models for some that could have these properties, but nobody will use them.
