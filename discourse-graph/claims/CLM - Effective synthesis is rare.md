---
title: [[CLM]] - Effective synthesis is rare
url: https://roamresearch.com/#/app/megacoglab/page/A4eO7QPKR
author: Joel Chan
date: Wed May 06 2020 09:44:57 GMT-0400 (Eastern Daylight Time)
---

- #[[ðŸŒ² zettels]]

    - Tags: #synthesis #[[D/Synthesis Infrastructure]]

    - Description

        - Amongst doctoral students: [[[[CLM]] - Doctoral students struggle to effectively synthesize literature]]

        - Amongst review papers

            - There is definitely a sense in the scholarly air that review papers (though immensely valuable to a field) are very hard to write, and somewhat of a "fool's errand" for [[ECRs]]. But I'm wondering now about the numbers and precision behind this sense. __Are__ review papers really all that rare?

            - Chewing again on this thread: ((aIg1IBAPE)) on [[[[CLM]] - Effective synthesis is rare]] / [[[[CLM]] - Effective synthesis is hard]]

        - In regular manuscripts

            - ((-25jBwY60)) (from [[@greifenederApplesVsOranges2017]])

        - Possibly because #[[[[CLM]] - Effective synthesis is hard]]

        - Also #[[Z: Publishing review papers is hard]]

    - R-Sources

        - #[[@flemingCochraneNonCochraneSystematic2013]]

            - #Claim Only about 20% of published [[systematic review]]s in orthondotics are "good" by [[AMSTAR standards of review quality]]; 20% are considered "poor"! (Table 2, p.246) #synthesis

        - #[[@booteScholarsResearchersCentrality2005]]

            - the level of synthesis in regular manuscripts frequently subpar (p.4)

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNbJN82GKeI.png?alt=media&token=94bb3cf6-b064-4fe9-a64b-7b03468bd692)

        - #@alton-leeTroubleshooterChecklistProspective1998

            - [[[[EVD]] - approximately one-third of criticisms in reviews of submitted manuscripts for a teacher education journal were directly related to inadequacies in synthesis - [[@alton-leeTroubleshooterChecklistProspective1998]]]]

###### Discourse Context

- **Informs::** [[QUE - What is synthesis]]
- **Informed By::** [[@booteScholarsResearchersCentrality2005]]
- **Informed By::** [[@flemingCochraneNonCochraneSystematic2013]]
- **Informed By::** [[@alton-leeTroubleshooterChecklistProspective1998]]

###### References

[[May 6th, 2020]]

- Start new stub zettel, bc starting to collect enough structure to refine/test claim that [[[[CLM]] - Effective synthesis is rare]]

    - This [GS search](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C21&q=bibliometric+article+types&btnG=) seems useful: turned up this paper on the first page [[R: belterBibliometricAnalysisClimate2013]], which finds Ballpark around 10% of less of papers in climate engineering published are review articles (p.420-421)
[[December 6th, 2020]]

- Quick idea: to make motivation for [[D/Synthesis Infrastructure]], and work out consequences of [[[[CLM]] - Effective synthesis is hard]] and [[[[CLM]] - Effective synthesis is rare]]...

    - It's hard to measure these, they're like hidden costs

    - But we can make some analogies here, maybe, to software reuse: imagine that everytime you started a new project, you either had to a) start from scratch, or b) run through a terrible gauntlet to get to reusable code (e.g., everyone else's code is poorly documented)

        - What is the estimated force multiplier of having access to clean reusable code in software engineering?

        - Could extrapolate a bit then to our setting
[[May 19th, 2020]]

- Is [[[[CLM]] - Effective synthesis is rare]], even for [[interdisciplinarity]]??

    - #anecdote about Ben S getting disillusioned about [[Convergence]], and suspicions that he's far from alone

        - Reached out to ARHU, and tech/media/participation efforts
[[April 14th, 2021]]

- contra [[[[CLM]] - Effective synthesis is rare]] (maybe?), we have ~1400 [[systematic review]]s published per year!

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fj_YXrU1Ueg.png?alt=media&token=36ede926-af51-4faf-b5af-0c220674a14e)

    - dangerous for exploration!! seems more similar to what we're interested in re: [[synthesis]]

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Ftubdnr--gG.png?alt=media&token=53998bf1-79fc-44e6-8336-1a8b50499ec5)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FVboBrUpdW3.png?alt=media&token=023403b2-6892-4cad-8098-fdd9fc8c2201)

            - products of [[systematic mapping]] are siloed atm - lack [[interoperability]] - big wastage!

            - [[@wolffeSystematicEvidenceMaps2019]]

        - cc [[[[CLM]] - [[systematic review]]s are typically narrowly focused, and provide less insight]]
[[January 3rd, 2021]]

- Some tweets to add to [[Z: Theories are core examples of synthesis]] and [[[[CLM]] - Scientific fields stall without adequate theoretical synthesis]], maybe also [[[[CLM]] - Effective synthesis is rare]]

    - Luiz Pessoa on Twitter: "Back to "causation is overrated". @_fernando_rosas Neuroscience is obsessed with manipulating bits and pieces and seeing what happens. Makes a lot of sense and needs to be done. Brain parts are highly interdependent and not surprisingly every week there's a new finding...ðŸ§¶" / Twitter: https://twitter.com/PessoaBrain/status/1340314232622100481

    - Rationally Speaking | Official Podcast of New York City Skeptics - Current Episodes - RS 193 - Eric Jonas on "Could a neuroscientist understand aÂ microprocessor?": http://rationallyspeakingpodcast.org/show/rs-193-eric-jonas-on-could-a-neuroscientist-understand-a-mic.html

        - Discussing [[@jonasCouldNeuroscientistUnderstand2017]]

    - Tons from [[Iris van Rooij]] (should really look more deeply into her work)

        - Now Reading threads â€“ metatheorist â€“ thinking about thinking about thinking: https://metatheorist.com/Now-Reading/

        - Iris van Rooij: https://irisvanrooijcogsci.com/

        - Rooij, Iris van | NIAS: https://nias.knaw.nl/fellow/rooij-iris-van/ (esp. her fellowship work which is focusing on developing tools and processes for theory development, reminiscent of vision of [[D/Synthesis Infrastructure]])

            - Beautiful summary here:

                - [[@rooijPsychologicalScienceNeeds2019]]

        - Psychological science needs theory development before preregistration â€“ Psychonomic Society Featured Content: https://featuredcontent.psychonomic.org/psychological-science-needs-theory-development-before-preregistration/
[[May 8th, 2020]]

- More on [[[[CLM]] - Effective synthesis is rare]] and/or [[[[CLM]] - Effective synthesis is hard]]

    - Important new paper to read, updates on [[@petrosino1999lead]] to give insight into the process and challenges of producing [[systematic review]]s (particularly [[Cochrane systematic reviews]]): [[@turnerProducingCochraneSystematic2017]]

    - lots more insight into the challenges of [[systematic review]] process in [[@ervinMotivatingAuthorsUpdate2008]]

    - Spending more time on the upfront stuff because I want to crystallize what the requirements are **for** (for [[P: Synthesis requirements theory paper]])

        - To reiterate, it's about requirements for data structures, and then ideas about authoring tools that can get us there (diagnosing as UX problem). Mostly individual, not focused on tools to support __collaborative__ sensemaking and synthesis.

            - The collaborative piece comes into play as we think about the possible mechanisms for constructing and managing the [[infrastructure]], not the target activity per se

    - [[I wonder]] if [[systematic review]]s [tend to be way more narrow]( [[@greenhalghTimeChallengeSpurious2018]] then cites [[@thorneRediscoveringNarrativeReview2018]] to critique the fetishizing of systematic reviews, noting that (similar ot my point), ((3tX8yzDeS)).) precisely because [[[[CLM]] - Effective synthesis is hard]]?

    - Struck by how many of these threads are in specialized journals. Not in infosci. WHY?? Too niche?

    - This paper [[@reyndersContactingAuthorsModified2019]] is super interesting: it's kind of about [[context]]

        - [[systematic review]] teams very frequently (on the order of 70% of the time) need to contact authors for additional details (context) for reported findings #synthesis

        - When extra information is obtained from authors, this frequently changes the review in substantial ways

    - #[[@flemingCochraneNonCochraneSystematic2013]]
[[May 1st, 2020]]

- Chewing again on this thread: There is definitely a sense in the scholarly air that review papers (though immensely valuable to a field) are very hard to write, and somewhat of a "fool's errand" for [[ECRs]]. But I'm wondering now about the numbers and precision behind this sense. __Are__ review papers really all that rare?  on [[[[CLM]] - Effective synthesis is rare]] / [[[[CLM]] - Effective synthesis is hard]]

    - Resurfacing [[@booteScholarsResearchersCentrality2005]], which actually includes an intriguing data point - the level of synthesis in regular manuscripts frequently subpar (p.4) not sure how this answers my original question though: this gives p $$p(synthesisissue | issue)$$, not $$p (synthesisissue | paper)$$. Also, need to look into it: usually people give suggestions anyway, even if hte paper is overall ok. We want to know about the proportion of times a lack of synthesis happens that is serious enough that it undermines. Maybe $$p(reject|synthesisissue)$$ will give us that?

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FkvKYAl6XHt?alt=media&token=7ba5b416-a000-4bf1-9064-9d7f0316549e)

        - The citation tree for this turns out to be a goldmine. Some samplings:

            - [[R: alaviReviewKnowledgeManagement2001]] (cited by [[@roweWhatLiteratureReview2014]])

            - Distinguishing literature reviews from [[Synthesis Systems]]s, with a somewhat derogatory summary of "non-systematic reviews" [[R: robinsonLiteratureReviewsVs2015]] - could be an interesting explanation for why [[Z: Publishing review papers is hard]]

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FGKCQOPm_dK.png?alt=media&token=8e076a88-c840-42a8-8a68-8e1cc3384038)

        - In general, we really need to ground the whole [[D/Synthesis Infrastructure]] work in the [[MIS]] [[Knowledge Management]] world: it is a close cousin of what we're doing, and lots of the insights we are building on were in the context of these organizational [[Knowledge Management]] systems (cf. [[@ackermanSharingKnowledgeExpertise2013]])

        - Important paper here: [[@levySystemsApproachConduct2006]] (I think same person [[Timothy J. Ellis]] who did [[@ellisFrameworkProblemBasedResearch2008]])

            - #appraisal Has some really nice #example-of the __outputs__ of [[synthesis]], but I wish there was more about the __process__! HOW do you get to those outputs? Think harder? Work longer?
[[CL - Science is getting less bang for its buck]]

- Possibly because #[[[[CLM]] - Effective synthesis is hard]] and also [rare]([[[[CLM]] - Effective synthesis is rare]]), since #[[[[CLM]] - Scientific fields stall without adequate theoretical synthesis]]

    - Our [[synthesis]] apparatus was probably ok for a world in which there was less to synthesize, and possibly less [[Scatter]]. But the world has changed (e.g., [[interdisciplinarity]] is way more of a thing, there is way more to synthesize now, way more [[Scatter]]), and our synthesis apparatus has hardly changed at all. So it's quite plausible that our effective synthesis rate would fall quite a bit behind where we need it to be.

    - It's hard to study the level of [[synthesis]] directly (and in particular draw a causal link between that and "progress", which is in itself really tricky to measure, since we rely a lot on scientometrics and bibliometrics, and [[[[CLM]] - Citation practices in science are far from optimal]]), but we have lots of anecdotal evidence at least.
[[January 6th, 2021]]

- Adding these papers in discussion with [[Matt Clancy]] about how to get at sense that [[[[CLM]] - Effective synthesis is rare]]

    - [[@uzziAtypicalCombinationsScientific2013]]

    - [[@mukherjeeNearlyUniversalLink2017]]

    - [[@aroraWhyInnovationEcosystem2019]]
[[February 11th, 2021]]

- Really want to flesh out [[[[CLM]] - Effective synthesis is rare]]

    - Reminded of [[@watsonBeingSystematicLiterature2015]]

        - proposes authorsourcing version of [[scholar-powered model of semantic publishing]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2F1pNfC4md6o.png?alt=media&token=e25c49d2-8b9d-4b94-94a8-3cb984b7c896)

            - similar to [[@grozaSALTWeavingClaim2007]] and [[@kuhnGenuineSemanticPublishing2017]]
[[February 4th, 2021]]

- New potential perspective on whether [[[[CLM]] - Effective synthesis is rare]]: https://blogs.lse.ac.uk/impactofsocialsciences/2021/02/04/against-research-waste-how-the-evidence-based-research-paradigm-promotes-more-ethical-and-innovative-research/ (h/t [tweet](https://twitter.com/natematias/status/1357332348166094849) by [[J. Nathan Matias]])

    - This has slightly more to do with the lack of **use** (reuse) of [[synthesis]], though, and is likely to be driven quite strongly by [incentive issues]([[[[CLM]] - Prevailing incentives in academia are bad for science]])

    - Nevertheless, an additional data point to consider when drawing the whole picture, since we don't have definite studies just yet

    - There is an idea of "[[research waste]]" in evidence-based research, with the intuition that we are often going down paths that are not worth going down

    - Paradigmatic example of this is continuing to run large [[RCT]]s for questions that have presumably been concluded to be answered by a [[systematic review]]

        - This judgment makes me feel slightly uncomfortable: I don't like the idea of closing down lines of inquiry. But maybe we don't need to do the exact same kinds of studies and designs long past we have reached sufficient certainty. We want to explore boundary conditions, for example, or have a good reason to revisit past work (even if it's just, "this was done two decades ago, let's see if it's still operating, since things have changed in the world", like [[@bjornDoesDistanceStill2014]])

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FhkWwmP1fxU.png?alt=media&token=4b3818e7-25a1-4b98-b36d-6fe00a0cf49c)

    - But can also be seen in the lack of integration of synthesis with design and results of a study
[[September 20th, 2020]]

- Another #example-of conditions for [[[[CLM]] - Effective synthesis is rare]], in part because of lack of [[context]] (necessary information about context of intervention to facilitate its reuse in real-world settings)

    - https://twitter.com/Psyched_Psyche/status/1306653513456459780
[[October 26th, 2020]]

- intriguing related work from this group on [[standards]] for measuring the degree to which a given article appropriately cites prior work - could maybe help quantify some sense that [[[[CLM]] - Effective synthesis is rare]]?

    - [[@craigFormulationFAIRMetrics2018]]
[[May 7th, 2020]]

- Thinking through again #[[[[CLM]] - Effective synthesis is rare]], we infer from the low base rate of effective *published/public* synthesis that "actual" synthesis is rare?

    - The world we want isn't necessarily one where we have tons more published reviews, but maybe that each individual paper is well-situated in and arising from an effective synthesis.

    - More fodder though:

        - this paper finds that majority of reviews that are published are "mere" narrative reviews, as opposed to systematic reviews [[@faggionSurveyPrevalenceNarrative2017]]

        - later [[Sally Thorne]] follows up on this to say "let's rediscover the narrative review! [[@thorneRediscoveringNarrativeReview2018]]

            - reminds me of [[@suriAdvancementsResearchSynthesis2009]]'s point about the need to deal with heterogeneity in the evidence types and assumptions, not just integrate over RCTs or something.

            - [[@greenhalghTimeChallengeSpurious2018]] then cites [[@thorneRediscoveringNarrativeReview2018]] to critique the fetishizing of systematic reviews, noting that (similar ot my point), #CLlaim [[[[CLM]] - [[systematic review]]s are typically narrowly focused, and provide less insight]].

        - separately, this paper from an editors' perspective (similar to [[@websterAnalyzingPrepareFuture2002]]), complains about the lack of synthesis in.... review papers! falling prey to the dreaded "list" [[@dowdWhyPublishSystematic2020]]

            - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2Fdbe44UMNhZ.png?alt=media&token=0f710e2d-db1a-48d1-85e6-175994b99c93)

            - much less emphasis on synthesis than on rigorous collection ([[ðŸ§± foraging]])!

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FouY4-FfDKn.png?alt=media&token=a16d7cc5-6826-4eff-b6ab-30bccc608cfb)

            - Not a huge fan of hte empashis on "story", but..

            - often see a lack of critical appraisal, insight, and discussion of heterogeneity, in submitted manuscripts (contra [[@strikeTypesSynthesisTheir1983]] Three intellectual standards for (good) synthesis (p356 onwards), and the [importance (and lack of) of critical appraisal](((-WYQv2Cnn))) laid out in [[@holbrookLevelsSuccessUse2008]])

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FcNK4ymP6Si.png?alt=media&token=fa1fd8d4-cfb8-4ba9-89d8-56e396198385)
