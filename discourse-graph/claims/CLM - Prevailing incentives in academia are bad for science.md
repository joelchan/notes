---
title: [[CLM]] - Prevailing incentives in academia are bad for science
url: https://roamresearch.com/#/app/megacoglab/page/Di6-UFQ5t
author: Joel Chan
date: Sun Mar 01 2020 21:53:43 GMT-0500 (Eastern Standard Time)
---

- #[[üå≤ zettels]]

    - Tags: #[[D/Synthesis Infrastructure]] #[[V: Open and Sustainable Innovation Systems]]

    - Description:

        - [[[[CLM]] - Prestige substantially controls how scientific ideas spread]]

        - Current [[incentives]] bias favor [[short-termism]]

            - For example, [[[[CL]] - Scientists prefer to pursue safer incremental advances rather than risky breakthroughs]]

            - This is bad for science because [[[[CLM]] - True creative breakthroughs often take a long time to develop]]

        - [[[[CLM]] - Scientific peer review is deeply flawed]]

        - [[[[CLM]] - Citation practices in science are far from optimal]]

        - [[[[CLM]] - Error detection work in science is systematically undervalued]]

        - [[[[CLM]] - bibliometric measures are biased against novel breakthrough research - [[@wangBiasNoveltyScience2017]]]]

    - R-Sources:

        - [[@heesen2018reward]]

            - #Claim Imperfections in [[peer review]] and the way credit is awarded systematically favor lower levels of reproducibility

        - [[@smaldinoNaturalSelectionBad2016]]

            - In a similar fashion to what is seen in empirical observations of scientific practice, selecting for high output in [the model](((DRG86XRFt))) led to the proliferation of poorer methods, such as low statistical power, and increasingly high false discovery rates

        - [[@nosekScientificUtopiaII2012]]

        - [[@fortnowViewpointTimeComputer2009]]

            - Discusses (mostly from personal experience) the issues with a conference-driven publication culture, including deadline-driven, [[least-publishable unit]] stuff, possibly [[short-termism]] too

        - [[@bengioTimeRethinkPublication2020]]

        - [[@johnMeasuringPrevalenceQuestionable2012]]

            - Approximately 50% of psychologists have engaged in [[Questionable Research Practices]] (short of falsifying data) at least once (p. 527)

                - ((7ia6XJRBw))

        - [[@everettTragedyAcademicCommons2015]]

        - [[@britzAIResearchReplicability2020]]

            - [[SOTA]] from old models are actually not really improved upon that much

        - [[@dedeoWhenScienceGame2020]]

            - science can be a "game", which helps produce behaviors such as failing to prosecute [[Questionable Research Practices]] and barriers to open science or [[semantic publishing]]

###### Discourse Context

- **Informed By::** [[@bengioTimeRethinkPublication2020]]
- **Informed By::** [[@dedeoWhenScienceGame2020]]
- **Informed By::** [[@heesen2018reward]]
- **Informed By::** [[@britzAIResearchReplicability2020]]
- **Informed By::** [[@wangBiasNoveltyScience2017]]
- **Informed By::** [[@fortnowViewpointTimeComputer2009]]
- **Informed By::** [[@johnMeasuringPrevalenceQuestionable2012]]
- **Informed By::** [[@everettTragedyAcademicCommons2015]]
- **Informed By::** [[@smaldinoNaturalSelectionBad2016]]
- **Informed By::** [[@nosekScientificUtopiaII2012]]

###### References

[[March 24th, 2021]]

- Brain is really really full after reading [this post](https://www.openphilanthropy.org/blog/science-policy-and-infrastructure) by [[Open Philanthrophy]] about science policy and infrastructure (h/t [[Evan Miyazono]]), and connecting it to the [[org/Protocol Labs]] roadmapping project for metascience: so many roads seem to run through the *system* as a core issue (e.g., [[[[CLM]] - Prevailing incentives in academia are bad for science]]) - ^^**the biggest, most transformative advance might be at the level of creating a flourishing ecosystem for amateur (in the positive, technical sense) science. **^^

    - There are already hints of this going on with [[citizen science]], and the growing ecosystem of "[[independent researchers]]" (see, e.g., [[Andy Matuschak]] and [[@thegeneralistIndieResearcher2020]] discussion amongst [indie researchers]([[independent researchers]]) like [[Andy Matuschak]] and [[Adam Wiggins]] (of [[org/Ink and Switch]], who made [[sys/Muse]]) that gives insight into the [problems with academic science and research]([[[[CLM]] - Prevailing incentives in academia are bad for science]]))

        - And now [[sys/Experiment]], with [[Cindy Wu]] et al

        - And of course [[org/Protocol Labs]] and [[org/Ought]] and many others

    - What if this "alt-academia" / independent research thing were the norm, and not the exception? Is that possible? #[[Question]] #idea [[[[QUE]] - How might we reimagine the way science is done at the whole system level?]]

        - It's possible that no path exists to a better ecosystem for science through the current system. But in any case, it would be highly transformative to construct a roadmap and focused effort to try to get there, whether it's through burning bridges and siphoning off the many people who have left academia because [[[[CLM]] - Prevailing incentives in academia are bad for science]] (e.g., Blogpost by [[James Heathers]] about leaving academia [[@heathersQuit2020]] )

        - A vision for this might also come from [[tools for conviviality]]: instead of thinking about constructing an industrialized system, envision a system that is convivial, that empowers each person with maximal freedom to contribute to the advancement of scientific knowledge.

    - A big motivation for me is how difficult it is to think through traction for adoption of [[metascience]] interventions.

        - Side note: it would be interesting to analyze the history of issues in science, such as reproducibility, focus on quantity over quality, [[@smaldinoNaturalSelectionBad2016]], etc.
[[November 10th, 2020]]

- [[@thegeneralistIndieResearcher2020]] discussion amongst [indie researchers]([[independent researchers]]) like [[Andy Matuschak]] and [[Adam Wiggins]] (of [[org/Ink and Switch]], who made [[sys/Muse]]) that gives insight into the [problems with academic science and research]([[[[CLM]] - Prevailing incentives in academia are bad for science]])

    - I can articulate now one reason I'm fascinated with that scene: in some ways, their work represents research and the pursuit of insight in its purest form, unadulterated by careerism (how am I going to get more pubs, how am I going to get promoted, get more grants, rewards, etc.), because they're not in a system that links their work to those incentives. That is how it should be in academia, but my experience is that I'm swimming **against** these currents to do the work I feel needs to be done.

    - see also this tweet by [[Michael Nielsen]]

        - https://twitter.com/michael_nielsen/status/1326231551844126720
[[May 9th, 2020]]

- Related: if we had effective [[infrastructure]] for [[synthesis]], such that everyone had a pretty good [[synthesis]] in their heads, would "shoddy work" and prestige flourish as it is now? (cf. [[[[CLM]] - Prevailing incentives in academia are bad for science]])

    - Thinking again of this #Claim Imperfections in [[peer review]] and the way credit is awarded systematically favor lower levels of reproducibility from [[@heesen2018reward]]
[[November 12th, 2020]]

- cc possible solution for [[[[CLM]] - Prevailing incentives in academia are bad for science]], via [[interdisciplinarity]]

    - [[@smaldinoInterdisciplinarityCanAid2020]]
[[October 17th, 2020]]

- makes me think of how [[[[CLM]] - Prevailing incentives in academia are bad for science]] re: error-checking

    - See also: [[Z: Tagging is essential but unrewarded work in online knowledge sharing communities]]

    - And more generally the idea that [[[[CLM]] - Infrastructure is sociotechnical, not just technical]]
[[June 21st, 2020]]

- Lots of concrete stuff in here relevant for [[[[CLM]] - Prevailing incentives in academia are bad for science]], specifically about the fact that error detection work isn't rewarded. The generalization of this is that [[[[CLM]] - Error detection work in science is systematically undervalued]]

    - Related piece by [[Josh Nicholson]] (who started [[sys/scite.ai]]): https://www.statnews.com/2015/12/10/phd-academic-culture-must-change/

    - And more by [[Simon DeDeo]] analyzing how science can be a "game", which helps produce behaviors such as failing to prosecute [[Questionable Research Practices]] and barriers to open science or [[semantic publishing]]? [[@dedeoWhenScienceGame2020]]

        - Reminds me very much of stuff that [[David Chapman]] talks about with [[@chapmanUpgradeYourCargo2016]] on [[Cargo Cult Science]]
[[üå±üåæ Research Garden]]

- [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - [[[[CLM]] - Error detection work in science is systematically undervalued]]

    - [[[[CL]] - Scientists prefer to pursue safer incremental advances rather than risky breakthroughs]]
[[June 18th, 2020]]

- OUTSTANDING blogpost that covers how [[[[CLM]] - Prevailing incentives in academia are bad for science]] in [[Machine Learning]] and [[AI]] research

    - [AI Research, Replicability and Incentives](https://dennybritz.com/blog/ai-replication-incentives/) [[@britzAIResearchReplicability2020]]

        - One would think that implementing the same model in two different frameworks would lead to identical results. But that's not the case. Subtle differences in ^^framework implementations, insufficient documentation, hidden hyperparameters, and bugs^^ can cascade and lead to different outcomes. If you go through the Github issues and forums of popular Deep Learning frameworks, you can find many examples of researchers obtaining unexpected results (like [here](https://l7.curtisnorthcutt.com/towards-reproducibility-benchmarking-keras-pytorch) or [here](https://github.com/keras-team/keras/pull/9965) or [here](https://github.com/keras-team/keras/issues/4444) or [here](https://github.com/keras-team/keras/issues/8672) or [here](https://github.com/kuangliu/pytorch-cifar/issues/45) or [here](https://github.com/Microsoft/MMdnn/issues/595)). From what I have seen, high-level frameworks like Keras that hide low-level implementation details and come with implicit hyperparameter choices already made for you, are the most common source of confusion.
[[August 1st, 2020]]

- Good #illustration-of [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - http://nautil.us/blog/you-want-to-see-my-data-i-thought-we-were-friends

        - summarizes ideas from [[@ritchieScienceFictionsHow2020]]
[[Week of December 13th, 2021]]

- [[[[CL]] - Tenure in the present system does not protect or increase risky boundary-pushing research]] cc [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - Do Economists Swing for the Fences after Tenure?: https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.32.1.179

    - [[Michael Nielsen]] on Twitter: "Fascinating, though note the year and many confounders https://t.co/fP7wohwVPz" / Twitter:

        - https://twitter.com/michael_nielsen/status/1471688962754318338
[[March 1st, 2020]]

- Some encouraging signs that we are reaching a breaking point in science in terms relevant to [[D/Synthesis Infrastructure]], especially regarding the point that [[[[CLM]] - Prevailing incentives in academia are bad for science]] - good blog post by [[Yoshua Bengio]] [[@bengioTimeRethinkPublication2020]], with some spicy commentary on Twitter by other ML/NLP luminaries, including [[Zachary Lipton]]. Good [pointer](https://twitter.com/mark_riedl/status/1233958607080902656) by [[Mark Riedl]] back to the [[CRA Best Practices Memo for Top-N Evaluation in Promotion and Hiring]]

    - Spicy commentary

        - https://twitter.com/nlpnoah/status/1234195280368586753

        - https://twitter.com/zacharylipton/status/1233953952762339330
[[April 27th, 2020]]

- [[Benjamin Reinhardt]] [writes](https://benjaminreinhardt.com/startup-constraints/) about the many constraints on startups, which honestly reminds me a lot of constraints on scientific research and how [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - The need for a great story is one particularly that resonates (plus the noise in evaluation) (cc #Claim Imperfections in [[peer review]] and the way credit is awarded systematically favor lower levels of reproducibility)

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FNoahkexjcF?alt=media&token=1836e676-9b23-47d6-8394-77c9205999d1)
[[Week of April 18th, 2022]]

- cc [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - narrow_path_talk https://rosanneliu.com/pres/narrow_path.pdf

        - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FX82LM9FY1m.png?alt=media&token=50c03e35-fc07-4eb0-ac26-710bfe7502ac)

        - story of how [[org/ML Collective]] was born.

        - sort of [[academic diaspora]] - but very similar to [[org/invisible college]] in a lot of ways, and maybe also [[org/Arcadia]]

    - Juan Mateos Garcia on Twitter: ""Racing to the bottom" Fantastic new paper about the impact of publication races on the quality of (structural biology) research. Tl;dr: it is negative. https://t.co/a9k8gYVWXb #econai adjacent https://t.co/6kj8Puj35v" / Twitter

        - tweet

            - https://twitter.com/JMateosGarcia/status/1517395924032626688?s=20&t=pHyx78wWGlPRVpxjdYxWdA

            - https://twitter.com/JMateosGarcia/status/1517395906718539776
[[January 31st, 2021]]

- [supporting cites](https://scite.ai/reports/the-natural-selection-of-bad-WL8RV5?contradicting=false&mentioning=false&page=1&utm_campaign=badge_generic&utm_medium=plugin&utm_source=generic) (via [[sys/scite.ai]]) for [[@smaldinoNaturalSelectionBad2016]] are a goldmine of evidence for thinking through [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - of particular interest is [[@brembsPrestigiousScienceJournals2018]], which reviews a few lines of unique evidence where some fields have fairly "objective" (quantifiable) indices of methods quality that can be computed and compared, and this is used to observe whether/how methods quality varies with journal "rank" (finding is that there is an *inverse* correlation: worse methods in higher-ranking journals)

        - addresses concerns about [[retraction]] as an index of methods quality
[[May 24th, 2020]]

- cc [[[[CLM]] - Prevailing incentives in academia are bad for science]] and [[[[CL]] - Science is getting less bang for its buck]]

    - developed into this paper [[@aroraChangingStructureAmerican2020]]

        - precursor was this NBER #workingpaper [[@aroraChangingStructureAmerican2019]]

        - summarized in [[@aroraChangingStructureAmerican2020]]
[[Week of February 7th, 2022]]

- more stuff on solutions for [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - The Scientific Virtues ‚Äì SLIME MOLD TIME MOLD: https://slimemoldtimemold.com/2022/02/10/the-scientific-virtues/

        - this is MAGNIFICENT. h/t [[Karola Kirsanow]]

        - loving this framing of virtues

            - seems related to [[Mental Health and Well-Being for PhD students]], but also [[[[QUE]] - How can we measure everyday progress in open-ended creative work?]] maybe?

    - Manifesto ‚Äî SoS: https://www.theseedsofscience.org/manifesto

        - another [[micropublishing platforms]]

    - Amateur hour: Improving knowledge diversity in psychological and behavioral science by harnessing contributions from amateurs - ScienceDirect: https://www.sciencedirect.com/science/article/pii/S0732118X21000714

    - Interesting Science: https://cohomology.group/Interesting-Science-226ac5f69d2946ecbb4c827a6b940bc9

    - Exegesis - by [[Roger's Bacon]] - Secretum Secretorum: https://rogersbacon.substack.com/p/exegesis
[[May 19th, 2020]]

- More to file under [[[[CLM]] - Prevailing incentives in academia are bad for science]] - specific finding in [[@bryanInnovationCrisisEvidence2020]]: huge payoffs --> MANY entrants --> race towards lower-value but easier to pull off (read: incremental) therapies being explored. #short-termism

    - SOUNDS LIKE SCIENCE??? HCI??? NLP??? etc.
[[June 14th, 2020]]

- New fact relevant to [[[[CLM]] - Prevailing incentives in academia are bad for science]]: there has long been an "inside track" to publishing in [[PNAS]] that sidesteps the regular [[peer review]] process.

    - This paper documents it: https://www.nature.com/news/scientific-publishing-the-inside-track-1.15424 - nuanced by the fact that most people don't use it, and it's used heavily by only a small number of people. But importantly, the mechanism was used and justified because there was a sense that [[[[CLM]] - Scientific peer review is deeply flawed]]

        - [Scientific publishing: The inside track](https://www.nature.com/news/scientific-publishing-the-inside-track-1.15424)

            - #quotes

                - These scientists say that the main motivator for using the contributed track is an intense frustration with the peer-review process at other high-profile journals, which they argue has become excessive and laborious

                - Complaints about nitpicking reviews at Nature and Science go hand-in-hand with the charge that the editors at these journals are in thrall to trendy areas of research. ‚ÄúVery often what seems to be fashionable is not very good science,‚Äù says Croce.

        - See also: https://scholarlykitchen.sspnet.org/2016/01/14/pnas-tighter-editorial-policy-improves-nas-papers/

            - Original source was this Twitter post:

                - https://twitter.com/nataliexdean/status/1271777475798892544/photo/1
[[March 1st, 2020]]

- Great discussion of how Twitter changes the landscape of #SciComm and gives hope for [[D/Synthesis Infrastructure]] and contra [[[[CLM]] - Prevailing incentives in academia are bad for science]] if the pressure to publish in only the "top journals" lessens - includes good examples that might be helpful for #[[P/Teach INST 888]]

    - https://danco.substack.com/p/can-twitter-save-science

    - Somewhat related is [[Venkatesh Rao]]'s long piece on how there is a text renaissance underway for online media: https://www.ribbonfarm.com/2020/02/24/a-text-renaissance/

    - Also related is this wonderful writeup of how scientists are responding to  [[R: kupferschmidtCompletelyNewCulture2020]]. I tweeted about this [here](https://twitter.com/JoelChan86/status/1233404766497247233).

        - It's a wonderful #example-of a positive change to the [[D/Synthesis Infrastructure]]
[[Week of December 20th, 2021]]

- Seems related to: [[[[QUE]] - What are powerful funding models for maximizing breakthrough research progress?]] and [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - This Scientist Created a Rapid Test Just Weeks Into the Pandemic. Here‚Äôs Why You Still Can‚Äôt Get It. ‚Äî ProPublica: https://www.propublica.org/article/this-scientist-created-a-rapid-test-just-weeks-into-the-pandemic-heres-why-you-still-cant-get-it

    - Alexander Berger on Twitter: "Devastating reporting from @lydiadepillis: https://t.co/ctGphGgv0A" / Twitter:

        - https://twitter.com/albrgr/status/1473418387694915589

    - Kelsey Piper on Twitter: ""That‚Äôs partly on the FDA for making poor decisions such that optimal treatment required virtue on the part of individual doctors. But mostly it‚Äôs on me, for not having it.": https://t.co/WrsRD7gWW0" / Twitter:

        - https://twitter.com/KelseyTuoc/status/1473508770026115073
[[@acmsigchiUISTCSCWCelebration2020]]

- major reiteration of the theme about [incentives]([[[[CLM]] - Prevailing incentives in academia are bad for science]]), and connecting that to our collective failure to anticipate harms (h/t [[Aniket Kittur]]): we just don't incentivize enough the kind of work that could uncover and deal with these harms (not enough scale, not enough real users and fidelity of systems to be used with enough reality to uncover bad uses, harms, etc.)

    - unfortunately this part of the Q&A was cutoff in the recording :(
[[August 31st, 2021]]

- #[[Twitter thread]] about science funding and quants, cc [[[[CLM]] - Prevailing incentives in academia are bad for science]]

    - {{tweet}}

        - Do research funding "quants" exist? If not, why not? If yes, who/where are they? And do they work well? #ThinkingInPublic
![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FnXJLrNEdpJ.png?alt=media&token=f17c0237-2445-4253-9f58-2afc7aa78a21)

        - Possible barrier: we haven't figured out good ways to measure the "returns" on research that are as informative as financial returns. But is this an intractable problem? Are parts of it tractable?

        - Possible barrier: the quant model doesn't fit. Research investment isn't that kind of thing, even though it looks similar (risks, etc.). But could the model be adapted to overcome misfits to still be useful?

        - Source: https://www.albertbridgecapital.com/post/was-ben-graham-a-quant h/t [[@ejames_c]]

        - cc [[@Ben_Reinhardt]]

        - Closest I can think of is [[@davidtlang]]'s https://www.scibetter.com/angels

    - Responses:

        - [[Benjamin Reinhardt]] makes a connection back to [[Knightian uncertainty]]: basically quant won't work bc we don't really have risk, just uncertainty in the Knightian sense: unknowable, unmeasurable.

            - https://twitter.com/Ben_Reinhardt/status/1432783609442840588

            - links

                - WP_92-Frydman-et-al-KUH.pdf: https://www.ineteconomics.org/uploads/papers/WP_92-Frydman-et-al-KUH.pdf

                - Knightian uncertainty - Wikipedia: https://en.wikipedia.org/wiki/Knightian_uncertainty

                - Explained: Knightian uncertainty | MIT News | Massachusetts Institute of Technology: https://news.mit.edu/2010/explained-knightian-0602

                - I feel like I maybe cited this in [[@chanAnalogyStrategySupporting2012]]? Or maybe [[Christian Schunn]] did in his chapter on uncertainty?

        - [[Trent Fowler]] points back to Alvaro de Menard https://fantasticanachronism.com/2020/09/11/whats-wrong-with-social-science-and-how-to-fix-it/
[[Discussion with Protocol Labs Metaresearch Journal Club, about @strasserBusinessExtractingKnowledge2021]]

- most papers are terrible because [[[[CLM]] - Prevailing incentives in academia are bad for science]] (e.g., [[[[CLM]] - Scientific peer review is deeply flawed]], [[[[CLM]] - Citation practices in science are far from optimal]])

    - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FhaDF07AxE4.png?alt=media&token=2698866b-eccd-4f73-b1a9-23df2d8425fc)

    - counterpoint: lagging indicators, [[[[CLM]] - bibliometric measures are biased against novel breakthrough research - [[@wangBiasNoveltyScience2017]]]] what do we lose by continuing to rely on this "great man" heuristic? and increasingly so as the number of papers grows?

        - [[[[EVD]] - highly novel papers were more likely to be in the top 1% of citations in the long run, but not in the short run, and particularly in other fields - [[@wangBiasNoveltyScience2017]]]]

        - see also On (Not) Reading Papers - LessWrong: https://www.lesswrong.com/posts/72QA8qk9g6wZNDWeS/on-not-reading-papers
