---
title: [[CLM]] - Interdisciplinary synthesis is hard because of scatter in the literature
url: https://roamresearch.com/#/app/megacoglab/page/0SJlzLm2v
author: Joel Chan
date: Thu May 07 2020 11:01:56 GMT-0400 (Eastern Daylight Time)
---

- #[[ðŸŒ² zettels]]

    - Tags: [[synthesis]] #interdisciplinarity #[[D/Synthesis Infrastructure]]

    - Description

        - [[interdisciplinarity]] is hard because of [[Scatter]] ([[@moteReasonsVariationInformation1962]]

            - Ah! As I suspected: some are arguing ([[@bates1996learning]]; [[@palmerAligningStudiesInformation1999]]) that [[interdisciplinarity]] influences scholars' information practices. SHould dig into that to get into what they're really saying about __how__ precisely the practices are impacted by degree of [[interdisciplinarity]]

            - [[@bates1996learning]] claims [[interdisciplinarity]] typically characterized by high degree of [[Scatter]]

###### Discourse Context

- **Informed By::** [[@moteReasonsVariationInformation1962]]

###### References

[[December 15th, 2020]]

- [[@packerImportanceSDICurrent1979]] in particular is instructive as a cautionary tale that a decontextualized recommender service can really help in boundary-crossing / [navigating]([[[[CLM]] - Interdisciplinary synthesis is hard because of scatter in the literature]]) high-[[Scatter]] / [[interdisciplinarity]]

    - seems like a supporting observation for [[[[QUE]] - What are the most efficient routes to useful cross-boundary knowledge?]]

    - this is really making me wonder about what we have discovered since [[SDI]]s

        - I'm worried that most [[recommender system]]s work seems to be evaluated with offline evaluations. I recall that the [[sys/DocEar]] team has done some excellent work on evaluations

            - Found this good paper comparing offline, online, and user study data

                - Really low absolute performance (in the single digits at best)

                - Thoughtful criticisms of offline datasets in light of [[[[CLM]] - Citation practices in science are far from optimal]] and other criticisms

            - This paper quantifies the balance of eval methods: ~70% of approaches as of [[2013]] were with offline eval, 34% had a user study (only 10% in a detailed usage setting) - should prioritize looking up those papers to triangulate against [[@packerImportanceSDICurrent1979]]

                - ![](https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fmegacoglab%2FrmKRL1R7Mr.png?alt=media&token=d6666784-fc10-45b1-9738-c585fd761281)
